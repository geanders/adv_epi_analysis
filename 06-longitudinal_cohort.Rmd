# Longitudinal cohort study designs

## Readings

The required readings for this chapter are:

- @andersson201970 Provides background on the study we'll use in this chapter for our example data

- @wong1989risk An epidemiological study using the study data.

There are also some supplemental readings you may find useful. The following are a series of instructional papers on survival analysis, that are 
meant as general background on how to fit survival analysis models:  

- @clark2003survival

- @bradburn2003survival

- @bradburn2003survival2

This article is a good summary for the limitations of Hazards Ratios and offers an alternative survival analysis approach enabling the plotting of adjusted cumulative incidence (and similarly survival rate) curves:

- @hernan2010hazards (Note that there is an online erratum for this article: "On page 15, column 1, second full paragraph: '... and the average HR is ensured to reach the value 1.' should instead say, '... and the risk ratio is ensured to reach the value 1.'")

These articles provide more background on the Framingham Heart Study: 

- @dawber1951epidemiological A paper from the 1950s, this presents the rationale behind the design of the Framingham Heart Study
- @dawber2015ii A reprint of the paper describing the first results to come out of the Framingham Heart Study

These articles provide some general reviews on our current understanding of the epidemiology and etiology of heart disease and its risk factors: 

- @wong2014epidemiological Review of large epidemiological studies on coronary heart disease and key findings
- @ziaeian2016epidemiology Review on the epidemiology and etiology of heart failure
- @valenzuela2021lifestyle Review of risk factors for hypertension
- @zhou2021global Review on global epidemiology of hypertension

## Longitudinal cohort data

Our example data for this chapter comes from the Framingham Heart Study. This is a cohort study that began in 1948 [@andersson201970]. At the time (and continuing today), heart disease was the most common cause of death in the US---a notable shift from earlier times, when infectious diseases played a larger role in mortality. While heart disease was an important cause of death, however, very little was known about risk factors for heart disease, outside of a little concerning the role of some infectious and nutritional diseases that affected the heart [@dawber1951epidemiological;@andersson201970]. This study was designed to collect a group of people without evident cardiovascular disease (although this restriction was eased in practice) and track them over many years, to try to identify risk factors for developing cardiovascular disease. The study subjects were tracked over time, with data regularly collected on both risk factors and cardiovascular outcomes. The study was revolutionary in identifying some key risk factors for coronary heart disease, including elevated blood pressure, high cholesterol levels, being overweight, and smoking [@andersson201970]. It was also revolutionary in identifying high blood pressure as a risk for other cardiovascular outcomes, including stroke and congestive heart failure [@andersson201970].

The original cohort included about 5,200 people from the town of Framingham, MA. The
example data is a subset of data from this original cohort. You can download the example dataset for this class by clicking [here](https://github.com/geanders/adv_epi_analysis/raw/master/data/frmgham2.csv) and then saving the content of the page as a csv file (we recommend using the original filename of "frmgham2.csv"). There is also a codebook file that comes with these data, which you can download for this class by clicking [here](https://github.com/geanders/adv_epi_analysis/raw/master/data/Framingham%20Longitudinal%20Data%20Documentation.pdf). This codebook includes some explanations about the columns in the data, as well as how multiple measurements from a single study subject are included in the data. 

The data are saved in a csv format, and so they can be read into R using the 
`read_csv` function from the `readr` package (part of the tidyverse). You can use the following code to read in these data, assuming you have saved them in a "data" subdirectory of your current
working directory: 

```{r message = FALSE}
library(tidyverse) # Loads all the tidyverse packages, including readr
fhs <- read_csv("data/frmgham2.csv")
fhs
```

You can find full details on the structure of this data in the codebook. At a broad scale, note that it includes several health outcomes related to heart disease, which the codebook calls "events" (`DEATH`: indicator of death from any cause; `ANYCHD`: indicator of one of several types of events related to coronary heart disease; `HOSPMI`: Hopitalization for myocardial infarction [heart attack], etc.). The data also includes a number of risk factors that the study researchers hypothesized might be linked to cardiovascular disease (`CURSMOKE`: if the study subject is currently a smoker; `TOTCHOL`: serum total cholesterol; `SYSBP`, `DIABP`: measures of the systolic and diastolic blood pressure, respectively; `BMI`: Body Mass Index; etc.). Finally, there are some characteristics of the study subject, like age (`AGE`) and sex (`SEX`), as well as some variables that are connected to either the time of the record or the time of certain events. 

As you look through the data, pay attention to some features that are more characteristic of cohort studies, compared to features of the time series data we worked with in earlier chapters: 

- One important difference compared to a time-series dataset is the `RANDID` variable. This is the unique identifier for unit for which we have repeated observations for over time.
In this case the `RANDID` variable represents a unique identifier for each study participant, with multiple observations (rows) per participant over time. 
- The `TIME` variable indicates the number of days that have ellapsed since beginning of follow-up of each observation. `TIME` s always 0 for the first observation of each participant, and then for following measurements will track the time since follow-up started for that study participant.
- The number of observations varies between participants (typical of many cohort studies)
- The time spacing between observations is not constant. This is because the repeated observations in the Framingham Heart Study are the result of follow-up exams happening 3 to 5 years apart. Many longitudinal cohorts will instead have observations over a fixed time interval (monthly, annual, biannual etc), resulting in a more balanced dataset.
- Observations are given for various risk factors, covariates and cardiovascular outcomes. Some will be invariant for each participant over time (`SEX`, `educ`), while others will vary with each exam.

From a data management perspective, we might want to change all the column names
to be in lowercase, rather than uppercase. This will save our pinkies some 
work as we code with the data! You can make that change with the following 
code, using the `str_to_lower` function from the `stringr` package (part of 
the `tidyverse`): 

```{r}
fhs <- fhs %>% 
  rename_all(.funs = str_to_lower)
fhs
```


*Applied exercise: Exploring longitudinal cohort data*

Read the example cohort data in R and explore it to answer the following
questions: 

1. What is the number of participants and number of observations in the `fhs` dataset?
2. Is there any missingness in the data?
3. How many participants died during the observation period? What is the distribution of age at time of death?
4. What is the distribution of age at time of incident MI? Are there differences between males and females? Are there differences in smoking between males and females? 
5. What is the distribution of BMI among MI cases and non-cases? How about between smokers and non-smokers

Based on this exploratory exercise, talk about the potential
for confounding when these data are analyzed to estimate the association between
smoking and risk of incident MI. 

*Applied exercise: Example code*

1. **What is the number of participants and the number of observations in the `fhs` dataset? (i.e what is the sample size and number of person-time observations)**

In the `fhs` dataset, the number of participants will be equal to the number of unique ID's (The `RANDID` variable which takes a unique value for each participant). We can extract this using the `unique` function nested within the `length` function

```{r}
length(unique(fhs$randid))
```

If you'd like to use `tidyverse` tools to answer this question, you can do 
that, as well. The pipe operator (`%>%`) works on any type of object---it will 
take your current output and include it as the first parameter value for the
function call you pipe into. If you want to perform operations on a column of 
a dataframe, you can use `pull` to extract it from the dataframe as a vector, and
then pipe that into vector operations: 

```{r}
fhs %>% 
  pull(randid) %>% 
  unique() %>% 
  length()
```

It's entirely a personal choice whether you use the `$` operator and "nesting"
of function calls, versus `pull` and piping to do a series of function calls.
You can see you get the same result, so it just comes down to the style that 
you will find easiest to understand when you look at your code later.

The number of person-time observations will be equal to the length of the dataset, since there's a row for every observation taken.
The `dim` function gives us the length (number of rows) and width (number of columns) for a dataframe or any matrix like object in R.

```{r}
dim(fhs)
```

We see that there are 11,626 observations, which is an average of approximately 2 to 3 observations per participant (11,626 / 4,434 = 2.6). 
 
When you know there are repeated measurements, it can be helpful to explore
how much variation there is in the number of observations per study subject. 
You could do that in this dataset with the following code: 

```{r}
fhs %>% 
  # Group by the study subject identifier and then count the rows for each
  group_by(randid) %>% 
  count() %>% 
  # Reorder the dataset so the subjects with the most observations come first
  arrange(desc(n)) %>% 
  head()
```

You can visualize this, as well. A histogram is one good choice: 

```{r message = FALSE, warning = FALSE}
fhs %>% 
  # Group by the study subject identifier and then count the rows for each
  group_by(randid) %>% 
  count() %>% 
  ggplot(aes(x = n)) + 
  geom_histogram()
```

All study subjects have between one and three measurements. Most of the study 
subjects (over 3,000) have three measurements recorded in the dataset. 

2. **Is there any missingness in the data?**

We can check for missingness in a number of ways. There are a couple of great
packages, `visdat` and `naniar`, that include functions for investigating
missingness in a dataset. If you don't have these installed, you can install
them using `install.packages("naniar")` and `install.packages("visdat")`. The
`naniar` package has [a vignette with
examples](https://cran.r-project.org/web/packages/naniar/vignettes/getting-started-w-naniar.html)
that is a nice starting point for working with both packages.

The `vis_miss` function from the `visdat` package shows missingness in a dataset in a way that lets you 
get a top-level snapshot:

```{r}
library(visdat)
vis_miss(fhs)
```

This shows you how much data is missing for each column in the data. For a smaller dataset, the design of the plot would also let you see how often missing data line up across several columns for the same observation (in other words, if an observation that's missing one measurement tends to also be missing several other measurements). In this case, however, there are so many rows, it's a bit hard to visually line up missingness by row.

Another was to visualize this is with `gg_miss_var` from the `naniar` package: 

```{r}
library(naniar)
gg_miss_var(fhs)
```

This output again focuses on missingness by column in the data, helping you identify columns where we might not have many non-missing observations.
In this case, many of the columns have measurements that are available for all observations, with no missingness,
including records of the subject's ID, measures of death, stroke, CVD and other
events, age, sex, and BMI. Some of the measured values from visits are missing
occasionally, like the total cholesterol, and glucose. Other measures asked of
the participants (number of cigarettes per day, education) are occasionally
missing. Two of the variables---`hdlc` and `ldlc` (High Density Lipoprotein Cholesterol and Low Density Lipoprotein Cholesterol, respectively)---are missing more often than 
they are available. If you read the codebook for the data, you'll see that this is because these measurements are only available at time period 3. 

You can also do faceting with the `gg_miss_var` function. For
example, you could see if missingness varies by the period of the observation: 

```{r}
gg_miss_var(fhs, facet = period)
```

You may also want to check if missingness varies with whether an observation
was associated with death of the study subject: 

```{r}
gg_miss_var(fhs, facet = death)
```

There are also functions in these packages that allow you to look at how 
missingness is related across variables. For example, both `glucose` and 
`totchol` are continuous variables, and both are occasionally missing. You
can use the geom function `geom_miss_point` from the `naniar` package
with a ggplot object to explore patterns of missingness among these two 
variables: 

```{r}
fhs %>% 
  ggplot(aes(x = glucose, y = totchol)) + 
  geom_miss_point()
```

The lower left corner shows the observations where both values are missing---it
looks like there aren't too many. For observations with one missing but not the 
other (the points in red along the x- and y-axes), it looks like the distribution
across the non-missing variable is pretty similar to that for observations
with both measurements available. In other words, `totchol` has a similar 
distribution among observations where `glucose` is available as observations
where `glucose` is missing, and the same for `glucose` for observations with and without missingess for `totchol`.

Since this function interfaces with `ggplot`, you can use any usual tricks with ggplot in association with it. For example, you can do things like facet by sex to explore patterns of missingness and codistribution at a finer level:

```{r}
fhs %>% 
  ggplot(aes(x = glucose, y = totchol)) + 
  geom_miss_point() + 
  facet_wrap(~ sex)
```

3. **How many participants died during the observation period? What is the distribution of age at time of death?**

The `death` variable in the `fhs` data is an indicator for mortality if a participant died at any point during follow-up. It is time-invariant: that is, it takes the value 1 if a participant died at any point during the follow-up period or 0 if they were alive at their end of follow-up, so we have to be careful on how to extract the actual number of deaths.

If you arrange by the random ID and look at `period` and `death` for each subject,
you can see that the `death` variable is the same for all periods for each
subject:

```{r}
fhs %>% 
  arrange(randid) %>% 
  select(randid, period, death)
```

We need to think some about this convention of recording the data when we count
the deaths.

It is often useful to extract the first (and sometimes last) observation, in order to assess certain covariate statistics on the individual level. We can create a dataset including only the first (or last) observation per participant from the `fhs` data using  `tidyverse` tools. The `group_by` functions groups data by unique values of designated variables (here `randid`) and the `slice` function selects rows as designated.

```{r}
fhs_first <- fhs %>% 
  group_by(randid) %>% 
  slice(1L) %>% # The L after the one clarifies that this is an integer
  ungroup()
fhs_first
```

Alternatively you can use the `slice_head` function, which allows us to slice a designated number of rows beginning from the first observation. Because we are piping this in the `group_by` function, we will be slicing rows beginning from the first observation for each `randid`:

```{r}
fhs_first <- fhs %>% 
  group_by(randid) %>% 
  slice_head(n = 1) %>% 
  ungroup()
```

We can similarly select the last observation for each participant: 

```{r}
fhs_last <- fhs %>% 
  group_by(randid) %>% 
  slice(n()) %>% # The `n()` function gives the count of rows in a group 
  ungroup()
fhs_last
```

or using the `slice_tail` function:

```{r}
fhs_last <- fhs %>% 
  group_by(randid) %>% 
  slice_tail(n=1) %>% 
  ungroup()
```

In this dataset we can extract statistics on baseline covariates on the individual level, but also assess the number of participants with specific values, including `death = 1`. For example, we can use the `sum` function in base R, which generates the sum of all values for a given vector. In this case since each death has the value of 1, the `sum` function will give us the number of deaths in the sample. 

```{r}
sum(fhs_first$death) 
```

Conversely using `tidyverse` tools we can extract the number of observations with `death = 1` using the `count` function

```{r}
fhs_first %>% 
  count(death) 
```

Note that survival or time-to-event outcomes in longitudinal cohort data will often be time-varying. For example, a variable for mortality will take the value of zero until the person-time observation that represents the time interval that the outcome actually happens in. For outcomes such as mortality this will typically be the last observation. We will construct a variable like this in `fhs` below, although with the conventions of this particular dataset, it doesn't matter if we take the first or last observation for each study subject (since they give a time-invariant value for death).

In order to estimate the distribution of age at death among those participants who died during follow-up we need to create a new age at death variable. The `age` variable in `fhs` represents the participants age at each visit. Typically a death would happen between visits so the last recorded value for `age` would be less than the age at death. We will use the `timedth` variable to help us determine the actual age at death. The value of `timedth` is the number of days from beginning of follow-up until death for those with `death=1`, while it is a fixed value of `timedth=8766` (the maximum duration of follow-up) for those with `death=0`.

We can create a new age at death variable for those with `death=1` using the `age` at baseline and `timedth` values

```{r}
fhs_first<-fhs_first %>% 
mutate(agedth=age+timedth/365.25,
       timedthy=timedth/365.25) ###also creating a year time-to-death variable
```

We can then get summary statistics on this new variable 

```{r}
fhs_first %>% 
summarize(min_agedth = min(agedth),
mean_agedth = mean(agedth),
max_agedth = max(agedth))
```

We can also check on these values by groups of interest such as sex

```{r}
fhs_first %>% 
group_by(sex) %>%
summarize(min_agedth = min(agedth),
mean_agedth = mean(agedth),
max_agedth = max(agedth))
```

4. *What is the distribution of age at time of incident MI? Are there differences between males and females? Are there differences in smoking between males and females?*

Similar to the question about death (all-cause mortality) we can look at disease incidence, for example myocardial infarction (MI). The `fhs` dataset has the `hospmi` variable as an indicator for any participant who had a hospitalization due to MI and `timemi` gives the number of days from beginning of follow up to the hospitalization due to MI. We can create an age at incident MI hospitalizaton in a similar fashion as the example for age at death.

```{r}
fhs_first<-fhs_first %>% 
mutate(agemi=age+timemi/365.25)
```

We can then get summary statistics on this new `agemi` variable 

```{r}
fhs_first %>% 
summarize(min_agemi = min(agemi),
mean_agemi = mean(agemi),
max_agemi = max(agemi))
```

And by sex
```{r}
fhs_first %>% 
group_by(sex)  %>% 
summarize(min_agemi = min(agemi),
mean_agemi = mean(agemi),
max_agemi = max(agemi))
```
We can see that the mean age at incident MI hospitalization among males and females is similar, but with males being somewhat younger on average at the time of incident MI. We can take a closer look at the distibution using boxplots:

```{r message = FALSE, warning = FALSE}
fhs_first %>% 
  # define the axes for the boxplot
  ggplot(aes(x = sex, y=agemi)) + 
  geom_boxplot()
```
We see that R didn't return two separate boxplots by sex, but rather one centered between the two values of `sex=1` and `sex=2` which are the values for males and females respectively. This is an indicator that the sex variable is of class `numeric` and is treated as a continuous values rather than categorical. We can verify that this is in fact the case:

```{r}
class(fhs_first$sex)
```

We can trasform the variable to one of class `factor` in order for it to be trated as a cateogrical variable

```{r}
fhs_first<-fhs_first %>% 
mutate(sex=as.factor(sex))
```

If we repeat the function for the boxplot now we get separate boxplots by sex

```{r message = FALSE, warning = FALSE}
fhs_first %>% 
  # define the axes for the boxplot
  ggplot(aes(x = sex, y=agemi)) + 
  geom_boxplot()
```

We can once again see from the the boxplots that females tend to be a little older at incidence of MI.

5. *What is the distribution of BMI among MI cases and non-cases? How about between smokers and non-smokers*

Similar to the exercise above we can compare BMI distributions by MI case status. 

## Coding a survival analysis

[R package `survival`]

In the context of survival analysis what is modelled is time to an event (also referred to as survival time or failure time). This is a bit different than the models in the linear or `glm` family that model an outcome that may follow a gaussian (linear regression),  binomial (logistic model) or Poisson distribution. Another difference is that the outcome (time to event) will not be determined in some participants, as they will not have experienced the event of interest during their follow-up. These participants are considered  'censored'. Censoring can occur in three ways:

- the participant does not experience the event of interest before the study end
- the participant is lost to follow-up before experiencing the event of interest
- the participant experiences a difference event that makes the event of interest impossible (for example if the event of interest is acute MI a participant that dies from a different cause is considered censored)

These are all types of right censoring and in simple survival analysis they are considered to be uninformative (typically not related to exposure). If the censoring is related to the exposure and the outcome then adjustment for censoring has to happen.

Let's assume that we are interested in all cause mortality as the event of interest let's denote  $T$ is time to death and $T\geq 0$. We define the survival function as 
$S(t)=Pr[T>t]=1-F(t)$, where the survival function $S(t)$ is the probability that a participant survives past time $t$ ($Pr[T>t]=1$). $F(t)$ is the Probability Density Function, (sometimes also denoted as the the Cumulative Incidence Function, $R(t)$) or the probability that that an individual will have a survival time less than or equal to t ($[Pr(T≤t)]$)

Time to event $t$ is bounded by $[0,\infty)$ and $S(t)$ is non-increasing as $t$ becomes greater. At $t=0$, $S(t)=1$ and conversely as $t$ approaches $\infty$, $S(t)=0$. A property of the survival and probabilty density function is $S(t) = 1 – F(t)$: the survival function and the probability density function (or cumulative incidence function ($R(t)$) sum to 1.

Another useful function is the hazard Function, $h(t)$, which is the instantaneous potential of experiencing an event at time $t$, conditional on having survived to that time ($h(t)=\frac{Pr[t<T\leq t+\Delta t|T>t]}{\Delta t}=\frac{f(t)}{S(t)}$). The cumulative Hazard Function, $H(t)$ is defined as the integral of the hazard function from time $0$ to time $t$, which equals the area under the curve $h(t)$ between time $0$ and time $t$ ($H(t)=\int_{0}^{t}h(u)du$).
If we know any of $S(t)$, $H(t)$ or $h(t)$, we can derive the rest based on the following relationships:

$h(t)=\frac{\partial log(S(t))}{\partial t}$

$H(t)=-log(S(t))$ and conversely $S(t)=exp(-H(t))$


The `survival` package in R allows us to fit these types of models, including a very popular model in survival analysis, the Cox proportional hazards model that was also applied in @wong1989risk. 
*Applied exercise: Survival curves and simple survival analysis*
1. What does the survival curve for mortality look like with follow-up time as the time scale of interest? How about with age?
2. How do (survival) curves for mortality compare between males and females? How about for MI?
3. What is the Hazard Ratio for smoking and the risk of MI, from a Cox Proportional Hazards model? 

**1. What does the survival curve for mortality look like with follow-up time as the time scale of interest? How about with age?**

A very simple way to estimate survival is the non-parametric Kaplan-Meier estimator.

In R we would estimate Survival $S(t)$ with all-cause mortality representing failure as follows:
```{r}
library(survival)
 S1=Surv(fhs_first$timedth,fhs_first$death)
 head(S1)
```

The numbers assigned to each individual represent their censoring times, with each number with a plus sign indicating that the participant was censored at that times without developing the outcome (haven't failed/died), while those without the plus sign are the times at which participants developed the outcome (failure/death).


```{r}
library(survminer) ##for plotting survival plots with ggplot2
fit_time<-  survfit(Surv(timedthy, death) ~ 1, data=fhs_first)
fit_time %>%
ggsurvplot(xlab="Time to death (years)",ylab=expression(paste('Overall Survival Probablity ', hat(S)*"(t)")))
```
We can see that as follow-up time increases survival decreases rather monotonically over time, or in other words the number of people who have died increases. Survival $\hat{S}(t)$ drops to about 0.65 at the end of follow-up, or in other words about 35% of participants have died, which is what is expected as we already know that 1550 of 4434 participants have died.

We can repeat this estimation with a different time-scale of interest. Other that follow-up times we may also be interested in Survival and failure (mortality) with respect to age. We repeat the same code only changing the first argument in the `Surv` function, substituting time of death with respect to follow-up time with age at death.

```{r}

fit_age<- survfit(Surv(agedth, death) ~ 1, data=fhs_first)
fit_age %>%
ggsurvplot(xlab="Age (years)",ylab=expression(paste('Overall Survival Probablity ', hat(S)*"(t)")))
```
We see that the shape of this survival curve is different, with virtually no one dying until they reach their 40s, and then a sharper drop in survival as age increases. 

**2.How do (survival) curves for mortality compare between males and females? How about for MI?**

Kaplan-Meir curves like the above are useful in comparing the survival rate between two groups. For example if we wanted to compare the survival rates between males and females we would fit the same model as above with sex as an independet variable. For all cause mortality:

```{r}
fit_bysex<-survfit(Surv(timedthy, death) ~ sex, data=fhs_first)
fit_age_bysex<- survfit(Surv(agedth, death) ~ sex, data=fhs_first)

fit_bysex%>%
ggsurvplot(xlab="Time to death (years)",ylab=expression(paste('Overall Survival Probablity ', hat(S)*"(t)")), legend.labs=c("Male","Female"),
  legend.title="Sex")

fit_age_bysex%>%
ggsurvplot(xlab="Age (years)",ylab=expression(paste('Overall Survival Probablity ', hat(S)*"(t)")), legend.labs=c("Male","Female"),
  legend.title="Sex")

```
You can now see that the survival rate for males drops quicker (at a younger age) than for females.
Similarly if we look at MI as the outcome:
```{r}
fit_age_MIbysex<-survfit(Surv(agemi, mi_fchd) ~ sex, data=fhs_first )

fit_age_MIbysex %>%
ggsurvplot(xlab="Age (years)",ylab=expression(paste('MI Survival Probablity ', hat(S)*"(t)")), legend.labs=c("Male","Female"),
  legend.title="Sex")

```
Once again we see a difference in the survival rates with age by sex, which is in line with what we already know from the literature.

We can actually approach survival rates by smoking status in the same manner:
```{r}
fit_age_MIsmoking<-survfit(Surv(agemi, mi_fchd) ~ cursmoke, data=fhs_first )

fit_age_MIsmoking %>%
ggsurvplot(xlab="Age (years)",ylab=expression(paste('MI Survival Probablity ', hat(S)*"(t)")), legend.labs=c("Non-smokers", "Smokers"),
  legend.title="Smoking status at baseline")

```
Once again we can observe that there is a difference in survival rates for MI, by smoking status at baseline.

The advantages of the Kaplan-Meier estimator for the survival function are its simplicity and the fact that it is a non-paramteric estimator. One limitation of Kaplan-Meier curves is that in this simple form of visualizing a survival rate, we cannot adjust for confounding by other variables, as the survival rates we are plotting here are marginal with respect to everything else. For example we can compare survival rates among smokers and non-smokers, but we can't really simply plot a sex adjusted survival rate for each, as the baseline rate for males and females will differ. What we can estimate while adjusting for other covariates is a survival time ratio, which is actually estimated using the same model we've been fitting. The `survreg` function in the `survival` package will fit a failure time model, with time to event as the outcome of interest. Unlike the Kaplan-Meier estimator this will require us to make an assumption about the distribution of time-to-event. Usually time-to-event outcomes are assumed to belong to the *exponential*, *Weibull*, *log-normal (log(T) is normally distributed)* or *log-logistic* distributions.

The majority of survival analyses for longitudinal cohort data, however has been dominated by the Cox proportional hazards model over the past few decades, and this is the type of model we will focus on here. The main advantage of the Cox proportional hazards model is that we don't have to make any distributional assumptions about the outcome or residuals. We simply model the instantaneous hazard of the outcome at specific time intervals as a function of covariates of interest, and the assumptions we have to make is that of 'proportional hazards'. This assumptions stipulates that the hazards across levels of covariates of interest are proportional over time. In other words the ratio of the hazards across levels covariates should be constant over time. 

The Cox proportional hazards model in a simple form has this form

$log(\lambda(t|X))=log(\lambda_{0}(t))+\beta_{1}\times X$

where $\lambda(t)$ represents the hazard at time $t$, $\lambda_{0}(t)$ is the baseline hazard at time $t$, and $\beta_{1}$ is the log hazard for those with $X=1$ compared to $X=0$. The baseline hazard $\lambda_{0}(t)$ is similar to the intercept term in a linear model or glm and is the value of the hazard when all covariates equal 0. However, unlike the intercept term in a linear model or glm, $\lambda_{0}(t)$ is not estimated by the model. 
The above model can also be writen as 

$\lambda(t|X)=\lambda_{0}(t)\times e^{\beta_{1}\times X}$

$e^{\beta_{1}}$ is the hazard ratio comparing those hose with $X=1$ and $X=0$

Using the `fhs` data we will fit a simple Cox proportional hazard for the effect of smoking on the hazard for MI. 

```{r}
coxph_mod1<-coxph(Surv(timedth, death) ~ cursmoke, data=fhs_first)
```

If we see the estimated parameters of the model, there isn't an intercept term as we've commented above:

```{r}
library(broom)
coxph_mod1 %>%
tidy()
```

The parameter for the covariate of interest is equivalent to the log of the hazard ratio comparing current smokers at baseline to non-smokers. We can extract the hazard ratio by exponentiating that parameter.

```{r}
coxph_mod1 %>% 
  tidy() %>% 
  filter(term == "cursmoke") %>% 
  mutate(hr = exp(estimate),
         low_ci = estimate - 1.96 * std.error, 
         high_ci = estimate + 1.96 * std.error, 
         low_hr = exp(low_ci), 
         high_hr = exp(high_ci)) %>% 
  select(term, hr, low_hr, high_hr)
```
We see that there is modest suggestive HR elevating the hazard for mortality, but the confidence intervals include the null.

We have said that the main assumption we need to make here is that of proportional hazards. The `survival` package actually allows us to check this with the `cox.zph` function

```{r}
phtest<-cox.zph(coxph_mod1)
```

The output of this function is the result of a $\chi^2$ test for the proportional hazards assumption. If the p-value here was below 0.05 we would have to reject the null hypothesis that the proportional hazards assumption holds. 
We can also plot the parameter(s) of interest across time from this output. If the proportional hazards assumption holds (constant HR) then the parameter should resemble a horizontal line with respect to time. 

```{r}
plot(phtest)
```

Here we see that the line is besically horizontal. Now let's repeat the model adjusting for some covariates, specifically sex and age.

```{r}
coxph_mod2<-coxph(Surv(timedth, death) ~ cursmoke + sex + age, data=fhs_first)
coxph_mod2 %>%
tidy()
```
We can already see that the parameter for smoking is now quite higher, but let's estimate the HR and 95% CI:

```{r}
coxph_mod2 %>% 
  tidy() %>% 
  filter(term == "cursmoke") %>% 
  mutate(hr = exp(estimate),
         low_ci = estimate - 1.96 * std.error, 
         high_ci = estimate + 1.96 * std.error, 
         low_hr = exp(low_ci), 
         high_hr = exp(high_ci)) %>% 
  select(term, hr, low_hr, high_hr)
```
The estimated Hazard Ratio is now 1.40 and the 95% CI does not include the null. We can determine that there was some confounding by these variables leading the estimate from the previous model to be biased towards the null. Let's test the ph assumption for this model.
```{r}
phtest2<-cox.zph(coxph_mod2)
phtest2
```

We see that the assumption holds, though the results for the test for both sex and age is close to rejecting the assumption.

Now let's see what happens if repeat the above model using age as the time-scale of interest in the survival function of the model:

```{r}
coxph_modage1<-coxph(Surv(agedth, death) ~ cursmoke + sex, data=fhs_first)
coxph_modage1 %>%
tidy()
```
Notice that we did not also include age as an additional parameter in the model, since using it as the time-scale inherently adjusts for it.

```{r}
coxph_modage1 %>% 
  tidy() %>% 
  filter(term == "cursmoke") %>% 
  mutate(hr = exp(estimate),
         low_ci = estimate - 1.96 * std.error, 
         high_ci = estimate + 1.96 * std.error, 
         low_hr = exp(low_ci), 
         high_hr = exp(high_ci)) %>% 
  select(term, hr, low_hr, high_hr)
```

We also see that the HR is similar as above (slightly higher).

Testing for the proportional hazards assumption in this model:

```{r}
phtestage<-cox.zph(coxph_modage1)
phtestage
```

Here we see that the assumption fails. If we plot the results of the models we can also see that we no longer have that horizontal line and in the case of smoking, it deviates from the that line significantly. 
```{r}
par(mfrow=c(1,2)) ###graphical parameters to plot two plots side by side (1 row, 2 columns)
plot(phtestage)
```
*Note: Variables of interest to continue with: 
for mixed models, `sysbp`, `diabp`, `totchol` compared to `cigpday`, `bmi` smoking or not
for long. analysis, `timemi` and `timestrk` and `hyperten`, exposure: `cigpday`, `sysbp`, `diabp`, `bmi`*



## Handling complexity

### Multi-level exposure

### Recurrent outcome

### Time-varying coeffificents

### Using survey results

[e.g., NHANES]

