# Longitudinal cohort study designs

The readings for this chapter are

- @andersson201970

- @wong1989risk

The following are a series of instructional papers on survival analysis, that are 
meant as general background on how to fit survival analysis models.  

- @clark2003survival

- @bradburn2003survival

- @bradburn2003survival2


## Longitudinal cohort data
Example datasets are available online, but also made available to you on the course 
website. For the Framingham Heart Study the example data are available as the file 
"frmgham2.csv". It is saved in a csv format, and so they can be read into R using the 
`read_csv` function from the `readr` package (part of the tidyverse). You can use the following code to read in these data, assuming you have saved them in a "data" subdirectory of your current
working directory: 

```{r message = FALSE}
library(tidyverse) # Loads all the tidyverse packages, including readr
fhs <- read_csv("data/frmgham2.csv")
fhs
```

- One important difference compared to a time-series dataset is the `RANDID` variable. This is the unique identifier for unit for which we have repeated observations for over time.
In this case the `RANDID` variable represents a unique identifier for each study participant, with multiple observations (rows) per participant over time. 
- The `TIME` variable indicates the number of days that have ellapsed since beginning of follow-up of each observation. (`TIME=0` for the first observation of each participant).
- Number of observations varies between participants (typical)
- The time spacing between observations is not constant. This is because the repeated observations in the Framingham Heart Study are the result of follow-up exams happening 3 to 5 years apart. Many longitudinal cohorts will instead have observations over a fixed time interval (monthly, annual, biannual etc), resulting in a more balanced dataset.
- Observations are given for various risk factors, covariates and cardiovascular outcomes. Some will be invariant for each participant over time (`SEX`, `educ`), while others will vary with each exam.

From a data management perspective, we might want to change all the column names
to be in lowercase, rather than uppercase. This will save our pinkies some 
work as we code with the data! You can make that change with the following 
code, using the `str_to_lower` function from the `stringr` package (part of 
the `tidyverse`): 

```{r}
fhs <- fhs %>% 
  rename_all(.funs = str_to_lower)
fhs
```


*Applied exercise: Exploring longitudinal cohort data*
Read the example cohort data in R and explore it to answer the following
questions: 

1. What is the number of participants and number of observations in the `fhs` dataset?
2. Is there any missingness in the data?
3. How many participants die? What is the distribution of age at time of death?
4. What is the distribution of age at time of incident MI? Are there differences between males and females? Are there differences in smoking between males and females? 
5. What is the distribution of BMI among MI cases and non-cases? How about between smokers and non-smokers

Based on this exploratory exercise in this section, talk about the potential
for confounding when these data are analyzed to estimate the association between
smoking and risk of incident MI. 

*Applied exercise: Example code*

1. **What is the number of participants and the number of observations in the `fhs` dataset? (i.e what is the sample size and number of person-time observations)**

In the `fhs` dataset, the number of participants will be equal to the number of unique ID's (The `RANDID` variable which takes a unique value for each participant). We can extract this using the `unique` function nested within the `length` function

```{r}
length(unique(fhs$randid))
```

If you'd like to use `tidyverse` tools to answer this question, you can do 
that, as well. The pipe operator (`%>%`) works on any type of object---it will 
take your current output and include it as the first parameter value for the
function call you pipe into. If you want to perform operations on a column of 
a dataframe, you can use `pull` to extract it from the dataframe as a vector, and
then pipe that into vector operations: 

```{r}
fhs %>% 
  pull(randid) %>% 
  unique() %>% 
  length()
```

It's entirely a personal choice whether you use the `$` operator and "nesting"
of function calls, versus `pull` and piping to do a series of function calls.
You can see you get the same result, so it just comes down to the style that 
you will find easiest to understand when you look at your code later.

The number of person-time observations will actually be equal to the length of the dataset.
The `dim` function gives us the length (number of rows) and width (number of columns) for a dataframe or any matrix like object in R.

```{r}
dim(fhs)
```
We see that there is approximately an average of 2 to 3 observations per participants. 
 
When you know there are repeated measurements, it can be helpful to explore
how much variation there is in the number of observations per study subject. 
You could do that in this dataset with the following code: 

```{r}
fhs %>% 
  # Group by the study subject identifier and then count the rows for each
  group_by(randid) %>% 
  count() %>% 
  # Reorder the dataset so the subjects with the most observations come first
  arrange(desc(n)) %>% 
  head()
```
You can visualize this, as well. A histogram is one good choice: 

```{r message = FALSE, warning = FALSE}
fhs %>% 
  # Group by the study subject identifier and then count the rows for each
  group_by(randid) %>% 
  count() %>% 
  ggplot(aes(x = n)) + 
  geom_histogram()
```
All study subjects have between one and three measurements. Most of the study 
subjects (over 3,000) have three measurements recorded in the dataset. 

2. *Is there any missingness in the data?*

We can check for missingness in a number of ways. There are a couple of great
packages, `visdat` and `naniar`, that include functions for investigating
missingness in a dataset. If you don't have these installed, you can install
them using `install.packages("naniar")` and `install.packages("visdat")`. The
`naniar` package has [a vignette with
examples](https://cran.r-project.org/web/packages/naniar/vignettes/getting-started-w-naniar.html)
that is a nice starting point for working with both packages.

The `vis_miss` function shows missingness in a dataset in a way that lets you 
get a top-level snapshot:

```{r}
library(visdat)
vis_miss(fhs)
```
Another was to visualize this is with `gg_miss_var`: 

```{r}
library(naniar)
gg_miss_var(fhs)
```

Many of the variables are available for all observations, with no missingness,
including records of the subject's ID, measures of death, stroke, CVD, and other
events, age, sex, and BMI. Some of the measured values from visits are missing
occasionally, like the total cholesterol, and glucose. Other measures asked of
the participants (number of cigarettes per day, education) are occasionally
missing. Two of the variables---`hdlc` and `ldlc`---are missing more often than 
they are available. 

You can also do faceting with the `gg_miss_var` function. For
example, you could see if missingness varies by the period of the observation: 

```{r}
gg_miss_var(fhs, facet = period)
```

You may also want to check if missingness varies with whether an observation
was associated with death of the study subject: 

```{r}
gg_miss_var(fhs, facet = death)
```

There are also functions in these packages that allow you to look at how 
missingness is related across variables. For example, both `glucose` and 
`totchol` are continuous variables, and both are occasionally missing. You
can use the geom function `geom_miss_point` from the `nanair` package
with a ggplot object to explore patterns of missingness among these two 
variables: 

```{r}
fhs %>% 
  ggplot(aes(x = glucose, y = totchol)) + 
  geom_miss_point()
```

The lower left corner shows the observations where both values are missing---it
looks like there aren't too many. For observations with one missing but not the 
other (the points in red along the x- and y-axes), it looks like the distribution
across the non-missing variable is pretty similar to that for observations
with both measurements avaiable. In other words, `totchol` has a similar 
distribution among observations where `glucose` is available as observations
where `glucose` is missing.

You can also do things like facet by sex to explore patterns at a finer level:

```{r}
fhs %>% 
  ggplot(aes(x = glucose, y = totchol)) + 
  geom_miss_point() + 
  facet_wrap(~ sex)
```

3. *How many participants die? What is the distribution of age at time of death?*

The `death` variable in the `fhs` data is an indicator for mortality if a participant died at any point during follow-up. It is time-invariant taking the value 1 if a participant died at any point or 0 if they were alive at their end of follow-up, so we have to be careful on how to extract the actual number of deaths.

If you arrange by the random ID and look at `period` and `death` for each subject,
you can see that the `death` variable is the same for all periods for each
subject:

```{r}
fhs %>% 
  arrange(randid) %>% 
  select(randid, period, death)
```
We need to think some about this convention of recording the data when we count
the deaths.

It is often useful to extract the first (and sometimes last) observation, in order to assess certain covariate statistics on the individual level. We can create a dataset including only the first (or last) observation per participant from the `fhs` data using  `tidyverse` tools. The `group_by` functions groups data by unique values of designated variables (here `randid`) and the `slice` function selects rows as designated.

```{r}
fhs_first <- fhs %>% 
  group_by(randid) %>% 
  slice(1L)%>%
  ungroup()
```

Alternatively you can use the `slice_head` function, which allows us to slice a designated number of rows beginning from the first observation. Because we are piping this in the `group_by` function, we will be slicing rows beginning from the first observation for each `randid`

```{r}
fhs_first <- fhs %>% 
  group_by(randid) %>% 
  slice_head(n=1)%>% 
ungroup()
```

We can similarly select the last observation for each participant 

```{r}
fhs_last <- fhs %>% 
  group_by(randid) %>% 
  slice(n())%>% 
ungroup()
```

or using the `slice_tail` function

```{r}
fhs_last <- fhs %>% 
  group_by(randid) %>% 
  slice_tail(n=1)%>% 
ungroup()
```

In this dataset we can extract statistics on baseline covariates on the individual level, but also assess the number of participants with specific values, including `death=1`. For example, we can use the `sum` function in base R, which generates the sum of all values for a given vector. In this case since each death has the value of 1 the `sum` function will give as the number of deaths in the sample. 

```{r}
sum(fhs_first$death) 
```

Conversely using `tidyverse` tools we can extract the number of observations with `death=1` using the `count` function

```{r}
fhs_first %>% 
count(death) 
```


Note that survival or time-to-event outcomes in longitudinal cohort data will often be time-varying. For example, a variable for mortality will take the value of zero until the person-time observation that represents the time interval that the outcome actually happens in. For outcomes such as mortality this will typically be the last observation. We will construct a variable like this in `fhs` below.

In order to estimate the distribution of age at death among those participants who died during follow-up we need to create a new age at death variable. The `age` variable in `fhs` represents the participants age at each visit. Typically a death would happen between visits so the last recorded value for `age` would be less than the age at death. We will use the `timedth` variable to help us determine the actual age at death. The value of `timedth` is the number of days from beginning of follow-up until death for those with `death=1`, while it is a fixed value of `timedth=8766` (the maximum duration of follow-up) for those with `death=0`.

We can create a new age at death variable for those with `death=1` using the `age` at baseline and `timedth` values

```{r}
fhs_first<-fhs_first %>% 
mutate(agedth=age+timedth/365.25)
```

We can then get summary statistics on this new variable 

```{r}
fhs_first %>% 
summarize(min_agedth = min(agedth),
mean_agedth = mean(agedth),
max_agedth = max(agedth))
```

We can also check on these values by groups of interest such as sex

```{r}
fhs_first %>% 
group_by(sex) %>%
summarize(min_agedth = min(agedth),
mean_agedth = mean(agedth),
max_agedth = max(agedth))
```

4. *What is the distribution of age at time of incident MI? Are there differences between males and females? Are there differences in smoking between males and females?*

Similar to the question about death (all-cause mortality) we can look at disease incidence, for example myocardial infarction (MI). The `fhs` dataset has the `hospmi` variable as an indicator for any participant who had a hospitalization due to MI and `timemi` gives the number of days from beginning of follow up to the hospitalization due to MI. We can create an age at incident MI hospitalizaton in a similar fashion as the example for age at death.

```{r}
fhs_first<-fhs_first %>% 
mutate(agemi=age+timemi/365.25)
```

We can then get summary statistics on this new `agemi` variable 

```{r}
fhs_first %>% 
summarize(min_agemi = min(agemi),
mean_agemi = mean(agemi),
max_agemi = max(agemi))
```

And by sex
```{r}
fhs_first %>% 
group_by(sex)  %>% 
summarize(min_agemi = min(agemi),
mean_agemi = mean(agemi),
max_agemi = max(agemi))
```
We can see that the mean age at incident MI hospitalization among males and females is similar, but with males being somewhat younger on average at the time of incident MI. We can take a closer look at the distibution using boxplots:

```{r message = FALSE, warning = FALSE}
fhs_first %>% 
  # define the axes for the boxplot
  ggplot(aes(x = sex, y=agemi)) + 
  geom_boxplot()
```
We see that R didn't return two separate boxplots by sex, but rather one centered between the two values of `sex=1` and `sex=2` which are the values for males and females respectively. This is an indicator that the sex variable is of class `numeric` and is treated as a continuous values rather than categorical. We can verify that this is in fact the case:

```{r}
class(fhs_first$sex)
```

We can trasform the variable to one of class `factor` in order for it to be trated as a cateogrical variable

```{r}
fhs_first<-fhs_first %>% 
mutate(sex=as.factor(sex))
```

If we repeat the function for the boxplot now we get separate boxplots by sex

```{r message = FALSE, warning = FALSE}
fhs_first %>% 
  # define the axes for the boxplot
  ggplot(aes(x = sex, y=agemi)) + 
  geom_boxplot()
```

We can once again see from the the boxplots that females tend to be a little older at incidence of MI.

5. *What is the distribution of BMI among MI cases and non-cases? How about between smokers and non-smokers*

Similar to the exercise above we can compare BMI distributions by MI case status. 

## Coding a survival analysis
In the context of survival analysis what is modelled is time to an event (also referred to as survival time or failure time). This is a bit different than the models in the linear or `glm` family that model an outcome that may follow a gaussian (linear regression),  binomial (logistic model) or Poisson distribution. Another difference is that the outcome (time to event) will not be determined in some participants, as they will not have experienced the event of interest during their follow-up. These participants are considered  'censored'. Censoring can occur in three ways:
\begin{itemize}
\item the participant does not experience the event of interest before the study end
\item the participant is lossed to follow-up before experiencing the event of interest
\item the participant experiences a difference event that makes the event of interest impossible (for example if the event of interest is acute MI a participants that dies from a different cause is considered censored)
\end{itemize}

These are all types of right censoring and in simple survival analysis they are considered to be uninformative (typically not related to exposure). If the censoring is related to the exposure and the outcome then adjustment for censoring has to happen.

Let's assume that we are interested in all cause mortality as the event of interest let's denote  $T$ is time to death and $T\geq 0$. We define the survival function as 
$S(t)=Pr[T>t]=1-F(t)$, where the survival function $S(t)$ is the probability that a participant survives past time $t$ ($Pr[T>t]=1$). $F(t)$ is the Probability Density Function, (sometimes also denoted as the the Cumulative Incidence Function, $R(t)$) or the probability that that an individual will have a survival time less than or equal to t ($[Pr(T≤t)]$)

Time to event $t$ is bounded by $[0,\infty)$ and $S(t)$ is non-increasing as $t$ becomes greater. At $t=0$, $S(t)=1$ and conversely as $t$ approaches $\infty$, $S(t)=0$. A property of the survival and probabilty density function is $S(t) = 1 – F(t)$: the survival function and the probability density function (or cumulative incidence function ($R(t)$) sum to 1.

Another useful function is the hazard Function, $h(t)$, which is the instantaneous potential of experiencing an event at time $t$, conditional on having survived to that time ($h(t)=\frac{Pr[t<T\leq t+\Delta t|T>t]}{\Delta t}=\frac{f(t)}{S(t)}$). The cumulative Hazard Function, $H(t)$ is defined as the integral of the hazard function from time $0$ to time $t$, which equals the area under the curve $h(t)$ between time $0$ and time $t$ ($H(t)=\int_{0}^{t}h(u)du$).
If we know any of $S(t)$, $H(t)$ or $h(t)$, we can derive the rest based on the following relationships:

$h(t)=\frac{\partial log(S(t))}{\partial t}$

$H(t)=-log(S(t))$ and conversely $S(t)=exp(-H(t))$


The `survival` package in R allows us to fit these types of models, including a very popular model in survival analysis, the Cox proportional hazards model that was also applied in @wong1989risk. A very simple way to estimate survival is the non-parametric Kaplan-Meier estimator.

In R we would estimate Survival $S(t)$ with all-cause mortality representing failure as follows:
```{r}
library(survival)
 S1=Surv(fhs_first$timedth,fhs_first$death)
 S1
```

The numbers assigned to each individual is their censoring times, with each number with a plus sign indicating that the participant was censored at that times without developing the outcome (haven't failed/died), while those without the plus sign are the times at which participants developed the outcome (failure/death).


```{r}
library(survminer) ##for plotting survival plots with ggplot2
fit1<-survfit(Surv(timedth, death) ~ 1, data = fhs_first)
summary(fit1)
fit1 %>%
ggsurvplot(xlab="Time to death (days)",ylab=expression(paste('Overall Survival Probablity ', hat(S)*"(t)")))
```
We can see that as follow-up time increases survival decreases or in other words the number of people who have died increases. Survival $\hat{S}(t)$ drops to about 0.65 at the end of follow-up, or in other words about 35% of participants have died, which is what is expected as we already know that 1550 of 4434 participants have died.

We can repeat this estimation with a different time-scale of interest. Other that follow-up times we may also be interested in Survival and failure (mortality) with respect to age. We repeat the same code only changing the first argument in the `Surv` function, substituting time of death with respect to follow-up time with age at death.

```{r}
fit2<-survfit(Surv(agedth, death) ~ 1, data = fhs_first)
summary(fit2)
fit2 %>%
ggsurvplot(xlab="Age (years)",ylab=expression(paste('Overall Survival Probablity ', hat(S)*"(t)")))
```
We see that the shape of this survival curve is different, with virtually noone dying until they reach their 40s, and then a sharper drop in survival as age increases. 




The Cox proportional hazards model in a simple form has this form

$log(\lambda(t|X))=log(\lambda_{0}(t))+\beta_{1}\times X$

where $\lambda(t)$ represent the hazard at time $t$, $\lambda_{0}(t)$ is the baseline hazard at time $t$, and $\beta_{1}$ is the log hazard for those with $X=1$ compared to $X=0$. The baseline hazard $\lambda_{0}(t)$ is similar to the intercept term in a linear model or glm and is the value of the hazard when all covariates equal 0. However, unlike the intercept term in a linear model or glm, $\lambda_{0}(t)$ is not estimated by the model. 
The above model can also be writen as 

$\lambda(t|X)=\lambda_{0}(t)\times e^{\beta_{1}\times X}$

$e^{\beta_{1}}$ is the hazard ratio comparing those hose with $X=1$ and $X=0$

Using the `fhs` data we will fit a simple Cox proportianal hazard for the effect of smoking on the hazard for MI. 

*Note: Variables of interest to continue with: 
for mixed models, `sysbp`, `diabp`, `totchol` compared to `cigpday`, `bmi` smoking or not
for long. analysis, `timemi` and `timestrk` and `hyperten`, exposure: `cigpday`, `sysbp`, `diabp`, `bmi`*



## Handling complexity

### Multi-level exposure

### Recurrent outcome

### Time-varying coeffificents

### Using survey results

[e.g., NHANES]

