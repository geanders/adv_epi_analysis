# Generalized linear models 

## Readings

The readings for this chapter are: 

- @bhaskaran2013time Provides an overview of time series regression
in environmental epidemiology.
- @vicedo2019hands Provides a tutorial of all the steps for a 
projecting of health impacts of temperature extremes under climate change. 
One of the steps is to fit the exposure-response association using present-day data
(the section on "Estimation of Exposure-Response Associations" in the paper). 
In this chapter, we will go into details on that step, and that section of the paper
is the only required reading for this chapter. Later in the class, we'll
look at other steps covered in this paper. Supplemental material for this paper is 
available to download by 
clicking http://links.lww.com/EDE/B504. You will need the data in this supplement
for the exercises for class.
- @armstrong2014conditional This paper describes different data structures for
case-crossover data, as well as how conditional Poisson regression can be used 
in some cases to fit a statistical model to these data.
Supplemental material for this paper is available at
https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/1471-2288-14-122#Sec13.

The following are supplemental readings (i.e., not required, but may be of
interest) associated with the material in this chapter: 

- @armstrong2006models Covers similar material as @bhaskaran2013time, but with 
more focus on the statistical modeling framework (highly recommended!)
- @gasparrini2010time Describes some of the advances made to time series study
designs and statistical analysis, specifically in the context of temperature
- @basu2005temperature Compares time series and case-crossover study designs in 
the context of exploring temperature and health. Includes a nice illustration 
of different referent periods, including time-stratified.
- @imai2015time Typically, the time series study design covered in this 
chapter is used to study non-communicable health outcomes. This paper discusses
opportunities and limitations in applying a similar framework for infectious
disease.
- @lu2007equivalence Heavier on statistics. This paper shows how, under
conditions often common for environmental epidemiology studies, case-crossover
and time series methods are equivalent.
- @gasparrini2014modeling Heavier on statistics. This provides the statistical 
framework for the distributed lag model for environmental epidemiology time 
series studies.
- @dunn2018generalized1 Introduction to statistical models, moving into regression models and 
generalized linear models. Chapter in a book that is available online through the CSU library.
- @james2013introduction3 General overview of linear regression, with an R coding 
"lab" at the end to provide coding examples. Covers model fit, continuous, binary,
and categorical covariates, and interaction terms. Chapter in a book that is
available online through the CSU library.

## Splines in GLMs

We saw from the last model, with a linear term for mean daily temperature, that the 
suggested effect on mortality is a decrease in daily mortality counts with 
increasing temperature. However, as you've probably guessed that's likely not 
entirely accurate. A linear term for the effect of exposure restricts us to an 
effect that can be fitted with a straight line (either a null effect or a 
monotonically increasing or decreasing effect with increasing exposure). 

This clearly is problematic in some cases. One example is when exploring the
association between temperature and health risk. Based on human physiology, 
we would expect many health risks to be elevated at temperature extremes, 
whether those are extreme cold or extreme heat. A linear term would be 
inadequate to describe this kind of U-shaped association. Other effects might
have a threshold---for example, heat stroke might have a very low risk at 
most temperatures, only increasing with temperature above a certain threshold.

We can 
capture non-linear patterns in effects, by using different functions of X. 
Examples are $\sqrt{X}$, $X^{2}$, or more complex smoothing functions, such as 
polynomials or splines. Polynomials might at first make a lot of sense, 
especially since you've likely come across polynomial terms in mathematics
classes since grade school. However, it turns out that they have some undesirable
properties. A key one is that they can have extreme behavior, particularly
when using a high-order polynomial, and particularly outside the range of 
data that are available to fit the model.

An alternative that is generally preferred for environmental epidemiology 
studies is the regression spline. The word "spline" originally comes from 
drafting and engineering (ship building, in particular), where it described
a flexible piece of wood or metal that you could use to draw a curved line---it created
a curve that was flexible enough---but just flexible enough---to fit a space
(see Wikipedia's very interesting article on [flat splines](https://en.wikipedia.org/wiki/Flat_spline) for more).

Splines follow a similar idea in mathematics, making them helpful tools when 
a line won't fit your data well. In general, a spline fits together a few 
simpler functions to create something with a curve or non-linearity. Each 
simpler function operates within an interval of the data, and then they join 
together at "knots" along the range of the data. 
Regression splines are therefore simple parametric smoothing function, 
which fit separate polynomial in each interval of the range of the predictor; these 
can be linear, quadratic, and cubic. 

The simplest example is a linear spline (also called a piecewise linear function). 
This type of spline creates non-linearity by having 
a breakpoint at the knot, allowing the slope of the line to be different in 
the intervals of data on either side of the knot. The following plot shows an 
example. Say you want to explore how mean temperature varies by the day in the
year (Jan 1 = 1, Jan 2 = 2, and so on) in the London example dataset from the
last chapter. Temperature tends to increase with day of 
year for a while, but then it changes around the start of August, after that 
decreasing with day of year. This patterns means that a line will give a bad fit
for how temperature changes with day of year, since it smooths right through that 
change. On the other hand, you can get a very reasonable fit using a linear spline
with a knot around August 1 (day 213 in the year). These two examples are shown in 
the following plot, with the linear function fit on the left and the linear spline
on the right:

```{r echo = FALSE, message = FALSE, warning = FALSE}
# Load some packages that will likely be useful
library(tidyverse)
library(lubridate)
library(broom)
library(gridExtra)

# Load and clean the data
obs <- read_csv("data/lndn_obs.csv") %>% 
  mutate(dow = wday(date, label = TRUE))

mod_1 <- lm(tmean ~ doy, data = obs)

mod_1_plot <- mod_1 %>% 
  augment() %>% 
  ggplot() + 
  geom_point(aes(x = doy, y = tmean), 
             size = 0.5, alpha = 0.3) + 
  geom_line(aes(x = doy, y = .fitted), 
            color = "red", size = 1.5) + 
  ggtitle("Line")

mod_2 <- lm(tmean ~ doy + I((doy - 213) * (doy >= 213)), data = obs)
mod_2_plot <- mod_2 %>% 
  augment() %>% 
  ggplot() + 
  geom_vline(xintercept = 213, linetype = 2, color = "blue") + 
  geom_point(aes(x = doy, y = tmean), 
             size = 0.5, alpha = 0.3) + 
  geom_line(aes(x = doy, y = .fitted), 
            color = "red", size = 1.5) + 
  ggtitle("Linear spline")

grid.arrange(mod_1_plot, mod_2_plot, nrow = 1)
```

If you were to write out these regression models in mathematical notation, the 
linear one is very simple: 

$$
Y_t = \alpha + \beta X_t
$$
where $Y_t$ is the temperature on day $t$, $\alpha$ is the model intercept, 
$X_t$ is the day of the year on day $t$, and $\beta$ is the estimated coefficient
for $X_t$. 

The notation for the model with the linear spline is a bit more complex: 

$$
Y_t = \alpha + \beta_1 X_t + \beta_2 (X_t - k)_+
$$

Here, $Y_t$ is again the temperature on day $t$, $X_t$ is again the day of the year 
on day $t$, and $\alpha$ is again the intercept. The term $k$ is the "knot"---the 
value of $X$ where we're letting the slope change. In this example, we're using
$k = 213$. The term $(X_t - k)_+$ has a special meaning---it takes the value 
0 if $X_t$ is in the interval to the left of the knot, while if $X_t$ is in the 
interval to the right of the knot, it takes the value of $X_t$ minus the knot
value: 

$$ 
(X_t - k)_+ =
\begin{cases}
0, & \mbox{if } X_t < k \\
X_t - k, & \mbox{if } X_t \ge k 
\end{cases}
$$
In this model, the coefficient $\beta_1$ estimates the slope of the line to the
left of the knot, while $\beta_2$ estimates how that slope will change to the 
right of the knot.

Fortunately, we usually won't have to get this complex in the model notation, 
especially when we use more complex splines (where the notation would get even
more complex). Instead, we'll often write out the
regression equation in a simpler way, just indicating that we're using a function
of the covariate, rather than the covariate directly: 

$$
Y_t = \alpha + f(X_t | \mathbf{\beta})
$$

where we can note that $f(X_t)$ is a function of day of the year ($X_t$), fit in 
this case using a linear spline, and with a set of estimated coefficients $\mathbf{\beta}$ for that
function (see @armstrong2006models for an example of using this model notation).

While a linear spline is the simplest conceptually (and mathematically), it often 
isn't very satisfying, because it fits a function with a sharp breakpoint, which 
often isn't realistic. For example, the linear spline fit above suggests that the
relationship between day of the year and temperature changes abruptly and dramatically
on August 1 of the year. In reality, we know that this change in the relationship 
between day of year and temperature is probably a lot smoother. 

To fit smoother shapes, we can move to higher level splines. Cubic splines
("cubic" because they include terms of the covariate up to the third power) are
very popular.
An example of a cubic spline 
function is $X+X^{2}+X^{3}+I((X>X_{0})*(X-X_{0})^3)$. This particular function is 
a cubic spline with four degrees of freedom ($df=4$) and one knot ($X_{0}$). 
A special type of cubic spline called a natural cubic spline is particularly popular.
Unlike a polynomial function, a natural cubic spline "behaves" better outside
the range of the data used to fit the model---they are constrained to continue
on a linear trajectory once they pass beyond the range of the data. 

Regression splines can be fit in a GLM via the package `splines`. Two commonly 
used examples of regression splines are b-splines and natural cubic 
splines. @vicedo2019hands uses natural cubic splines, which can be fit with the
`ns` (for "natural spline") function from the `splines` package.

While splines are great for fitting non-linear relationships, they do create some
challenges in interpreting the results. When you fit a linear relationship for 
a covariate, you will get a single estimate that helps describe the fitted relationship
between that covariate and the outcome variable. However, when you use a non-linear
function, you'll end up with a mix of coefficients associated with that function.
Sometimes, you will use splines to control for a potential confounder (as we 
will in the exercises for this first part of the chapter). In this case, you don't 
need to worry about interpreting the estimated coefficients---you're just trying to 
control for the variable, rather than inferring anything about how it's associated
with the outcome.
In later parts of this chapter, we'll talk about how to interpret these coefficients
if you're using a spline for the exposure that you're interested in, when we talk 
more broadly about basis functions.

*Applied: Including a spline in a GLM*

For this exercise, you will continue to build up the model that you began in 
the examples in the previous chapter. The example uses the data provided with 
one of this chapter's readings, @vicedo2019hands.

1. Start by fitting a somewhat simple model---how are daily mortality counts
associated with (a) a linear and (b) a non-linear function of time? Is a linear
term appropriate to describe this association? What types of patterns are 
captured by a non-linear function that are missed by a linear function?
2. In the last chapter, the final version of the model used a GLM with an 
overdispersed Poisson distribution, including control for day of week. 
Start from this model and add control for long-term and seasonal trends 
over the study period. 
3. Refine your model to fit for a non-linear, rather than linear, function 
of temperature in the model. Does a non-linear term seem to be more 
appropriate than a linear term?

*Applied exercise: Example code*

1. **Start by fitting a somewhat simple model---how are daily mortality counts
associated with (a) a linear and (b) a non-linear function of time?**

It is helpful to start by loading the R packages you are likely to need, as
well as the example dataset. You may also need to re-load the example data
and perform the steps taken to clean it in the last chapter: 

```{r message = FALSE, warning = FALSE}
# Load some packages that will likely be useful
library(tidyverse)
library(viridis)
library(lubridate)
library(broom)

# Load and clean the data
obs <- read_csv("data/lndn_obs.csv") %>% 
  mutate(dow = wday(date, label = TRUE))
```

For this first question, the aim is to model the association between time and
daily mortality counts within the example data. This approach is often used
to explore and, if needed, adjust for temporal factors in the data. 

There are a number of factors that can act over time to create patterns in both
environmental exposures and health outcomes. For example, there may be changes
in air pollution exposures over the years of a study because of changes in
regulations or growth or decline of factories and automobile traffic in an area.
Changes in health care and in population demographics can cause patterns in
health outcomes over the study period. At a shorter, seasonal term, there are
also factors that could influence both exposures and outcomes, including
seasonal changes in climate, seasonal changes in emissions, and seasonal
patterns in health outcomes. 

It can be difficult to pinpoint and measure these temporal factors, and so
instead a common practice is to include model control based on the time in the
study. This can be measured, for example, as the day since the start of the 
study period. 

You can easily add a column for day in study for a dataset that
includes date. R saves dates in a special format, which we're using the in 
`obs` dataset: 

```{r}
class(obs$date)
```

However, this is just a fancy overlay on a value that's ultimately saved as 
a number. Like most Unix programs, the date is saved as the number of days 
since the Unix "epoch", January 1, 1970. You can take advantage of this 
convention---if you use `as.numeric` around a date in R, it will give you a 
number that gets one unit higher for every new date. Here's the example for
the first date in our example data:

```{r}
obs$date[1]
as.numeric(obs$date[1]) 
```

And here's the example for the next date:

```{r}
obs$date[2]
as.numeric(obs$date[2])
```

You can use this convention to add a column that gives days since the first
study date. While you could also use the `1:n()` call to get a number for 
each row that goes from 1 to the number of rows, that approach would not 
catch any "skips" in dates in the data (e.g., missing dates if only warm-season
data are included). The use of the dates is more robust:

```{r}
obs <- obs %>% 
  mutate(time = as.numeric(date) - first(as.numeric(date)))

obs %>% 
  select(date, time)
```

As a next step, it is always useful to use exploratory data analysis to look 
at the patterns that might exist for an association, before you start designing
and fitting the regression model. 

```{r}
ggplot(obs, 
       aes(x = time, y = all)) +
  geom_point(size = 0.5, alpha = 0.5)
```

There are clear patterns between time and daily mortality counts in these data. 
First, there is a clear long-term pattern, with mortality rates declining on 
average over time. Second, there are clear seasonal patterns, with higher 
mortality generally in the winter and lower rates in the summer. 

To model this, we can start with fitting a linear term. In the last chapter, 
we determined that the mortality outcome data can be fit using a GLM with a
Poisson family, allowing for overdispersion as it is common in real-life
count data like these. To include time as a linear term, we can just include
that column name to the right of the `~` in the model formula:

```{r}
mod_time <- glm(all ~ time, 
                data = obs, family = "quasipoisson")
```

You can use the `augment` function from the `broom` package to pull out the
fitted estimate for each of the original observations and plot that, along
with the observed data, to get an idea of what this model has captured:

```{r}
mod_time %>% 
  augment() %>% 
  ggplot(aes(x = time)) + 
  geom_point(aes(y = all), alpha = 0.4, size = 0.5) + 
  geom_line(aes(y = exp(.fitted)), color = "red") + 
  labs(x = "Date in study", y = "Expected mortality count") 
```


This linear trend captures the long-term trend in mortality rates fairly well in
this case. This won't always be the case, as there may be some health
outcomes---or some study populations---where the long-term pattern over the
study period might be less linear than in this example. Further, the linear 
term is completely unsuccessful in capturing the shorter-term trends in mortality 
rate. These oscillate, and so would be impossible to capture over multiple 
years with a linear trend. 

Instead, it's helpful to use a non-linear term for time in the model. We can 
use a natural cubic spline for this, using the `ns` function from the `splines`
package. You will need to clarify how flexible the spline function should be, 
and this can be specified through the degrees of freedom for the spline. A 
spline with more degrees of freedom will be "wigglier" over a given data range
compared to a spline with fewer degrees of freedom. Let's start by using 
158 degrees of freedom, which translates to about 7 degrees of freedom per year:

```{r}
library(splines)
mod_time_nonlin <- glm(all ~ ns(time, df = 158), 
                       data = obs, family = "quasipoisson")
```

You can visualize the model results in a similar way to how we visualized the
last model. However, there is one extra step. The `augment` function only 
carries through columns in the original data (`obs`) that were directly used
in fitting the model. Now that we're using a transformation of the `time`
column, by wrapping it in `ns`, the `time` column is no longer included in the 
`augment` output. However, we can easily add it back in using `mutate`, 
pulling it from the original `obs` dataset, and then proceed as before.

```{r}
mod_time_nonlin %>% 
  augment() %>% 
  mutate(time = obs$time) %>% 
  ggplot(aes(x = time)) + 
  geom_point(aes(y = all), alpha = 0.4, size = 0.5) + 
  geom_line(aes(y = exp(.fitted)), color = "red") + 
  labs(x = "Date in study", y = "Expected mortality count") 
```

The non-linear term for time has allowed enough flexibility that the model now
captures both long-term and seasonal trends in the data. 

You might wonder how many degrees of freedom you should use for this time spline. In practice,
researchers often using about 6--8 degrees of freedom per year of the study, in
the case of year-round data. You can explore how changing the degrees of freedom
changes the way the model fits to the observed data. As you use more degrees of
freedom, the line will capture very short-term effects, and may start to
interfere with the shorter-term associations between environmental exposures and
health risk that you are trying to capture. Even in the example model we just
fit, for example, it looks like the control for time may be capturing some
patterns that were likely caused by heatwaves (the rare summer peaks, including
one from the 1995 heatwave). Conversely, if too few degrees of freedom are used, 
the model will shift to look much more like the linear model, with inadequate
control for seasonal patterns.

```{r}
# A model with many less d.f. for the time spline
mod_time_nonlin_lowdf <- glm(all ~ ns(time, df = 10), 
                             data = obs, family = "quasipoisson")
mod_time_nonlin_lowdf %>% 
  augment() %>% 
  mutate(time = obs$time) %>% 
  ggplot(aes(x = time)) + 
  geom_point(aes(y = all), alpha = 0.4, size = 0.5) + 
  geom_line(aes(y = exp(.fitted)), color = "red") + 
  labs(x = "Date in study", y = "Expected mortality count") 
```

```{r}
# A model with many more d.f. for the time spline
# (Takes a little while to run)
mod_time_nonlin_highdf <- glm(all ~ ns(time, df = 400), 
                             data = obs, family = "quasipoisson")
mod_time_nonlin_highdf %>% 
  augment() %>% 
  mutate(time = obs$time) %>% 
  ggplot(aes(x = time)) + 
  geom_point(aes(y = all), alpha = 0.4, size = 0.5) + 
  geom_line(aes(y = exp(.fitted)), color = "red") + 
  labs(x = "Date in study", y = "Expected mortality count") 
```

In all cases, when you fit a non-linear function of an explanatory variable, 
it will make the model summary results look much more complicated, e.g.: 

```{r}
mod_time_nonlin_lowdf %>% 
  tidy()
```

You can see that there are multiple model coefficients for the variable fit 
using a spline function, one less than the number of degrees of freedom. These
model coefficients are very hard to interpret on their own. When we are using 
the spline to *control* for a factor that might serve as a confounder of the
association of interest, we typically won't need to try to interpret these
model coefficients---instead, we are interested in accounting for how this
factor explains variability in the outcome, without needing to quantify the
association as a key result. However, there are also cases where we want to 
use a spline to fit the association with the exposure that we are interested
in. In this case, we will want to be able to interpret model coefficients from
the spline. Later in this chapter, we will introduce the `dlnm` package, which 
includes functions to both fit and interpret natural cubic splines within 
GLMs for environmental epidemiology.

2. **Start from the last model created in the last chapter and add control for 
long-term and seasonal trends over the study period.**

The last model fit in the last chapter was the following, which fits for the 
association between a linear term of temperature and mortality risk, with control
for day of week: 

```{r}
mod_ctrl_dow <- glm(all ~ tmean + factor(dow, ordered = FALSE), 
                    data = obs, family = "quasipoisson")
```

To add control for long-term and seasonal trends, you can take the natural cubic
spline function of temperature that you just fit and include it among the
explanatory / independent variables from the model in the last chapter. If you 
want to control for only long-term trends, a linear term of the `time` column 
could work, as we discovered in the first part of this chapter's exercise. 
However, seasonal trends could certainly confound the association of interest. 
Mortality rates have a clear seasonal pattern, and temperature does as well, 
and these patterns create the potential for confounding when we look at how
temperature and mortality risk are associated, beyond any seasonally-driven 
pathways. 

```{r}
mod_ctrl_dow_time <- glm(all ~ tmean + factor(dow, ordered = FALSE) +
                           ns(time, df = 158), 
                         data = obs, family = "quasipoisson")
```

You can see the influence of this seasonal confounding if you look at the model
results. When we look at the results from the model that did not control for
long-term and seasonal trends, we get an estimate that mortality rates tend to 
be lower on days with higher temperature, with a negative term for `tmean`: 

```{r}
mod_ctrl_dow %>% 
  tidy() %>% 
  filter(term == "tmean")
```
Conversely, when we include control for long-term and seasonal trends, the 
estimated association between mortality rates and temperature is reversed, 
estimating increased mortality rates on days with higher temperature, *controlling
for long-term and seasonal trends*:

```{r}
mod_ctrl_dow_time %>% 
  tidy() %>% 
  filter(term == "tmean")
```

3. **Refine your model to fit for a non-linear, rather than linear, function 
of temperature in the model.**

You can use a spline in the same way to fit a non-linear function for the 
exposure of interest in the model (temperature). We'll start there. However, 
as mentioned earlier, it's a bit tricky to interpret the coefficients from the
fit model---you no longer generate a single coefficient for the exposure of
interest, but instead several related to the spline. Therefore, once we show
how to fit using `ns` directly, we'll show how you can do the same thing using
specialized functions in the `dlnm` package. This package includes a lot of 
nice functions for not only fitting an association using a non-linear term, 
but also for interpreting the results after the model is fit.

First, here is code that can be used to fit the model using `ns` directly, 
similarly to the approach we used to control for temporal patterns with a
flexible function:

```{r}
mod_ctrl_nl_temp <- glm(all ~ ns(tmean, 4) + factor(dow, ordered = FALSE) +
                          ns(time, df = 158), 
                        data = obs, family = "quasipoisson")
```

We can plot the predicted values from this fitted model (red points in the plot below) 
compared to the observed data (black dots) using our usual method of using `augment` to 
extract the predicted values: 

```{r}
mod_ctrl_nl_temp %>% 
  augment() %>% 
  mutate(tmean = obs$tmean) %>% 
  ggplot(aes(x = tmean)) + 
  geom_point(aes(y = all), alpha = 0.4, size = 0.5) + 
  geom_point(aes(y = exp(.fitted)), color = "red",  size = 0.4) + 
  labs(x = "Daily mean temperature", y = "Expected mortality count") 
```

However, these predictions are very variable at any given temperature. This reflects how 
other independent variables, like long-term and seasonal trends, explain variability in 
mortality. This makes sense, but it makes it a bit hard to investigate the role of temperature
specifically. Let's look, then, at some other options for viewing the results that will 
help us focus on the association of the exposure we care about (temperature) with the outcome.

The next part has a lot of steps, so it might at first seem confusing. However, fortunately
we won't always have to do all the steps ourselves---there is a nice R package that 
will help us. We'll look at the process first, though, and then the easier way to use a 
package to help with some of the steps. Also, this process gives us a first look at how
the idea of basis functions work. We'll later expand on these to look at non-linear 
relationships with the exposure at different lag times, using cross-basis functions, so this
forms a starting point for moving into the idea of a cross-basis.

First, we need to think about how temperature gets included in the regression
model---specifically, as a function of *basis variables* rather than as a single variable. 
In other words, to include temperature as a nonlinear function, we're going to create 
a structure in the regression equation that includes the variable of temperature in 
several different terms, in different transformations in each term. I simple example of this
is a second-degree polynomial. If we wanted to include a second-order polynomial function
of temperature in the regression, then we'd use the following basis: 

$$
\beta_1 T + \beta_2 T^2
$$

Notice that here we're using the same independent variable ($T$, which stands for daily 
temperature), but we're including both untransformed $T$ and also $T$ squared. This function
of $T$ might go into the regression equation as something like: 

$$
E(Y) = \alpha + \beta_1 T + \beta_2 T^2 + ns(time) + \mathbf{\gamma}W
$$
where $\alpha$ is the intercept, $ns(time)$ is a natural cubic spline that controls for 
time (long-term and seasonal trends) and $W$ is day of week, with $\mathbf{\gamma}$ as 
the set of coefficients associated with day of week. (Here, I've included the outcome, 
$Y$, untransformed, as you would for a linear regression, but of course the same idea
works with Poisson regression, when you'd instead have $log(E(Y))$ on the left of the 
equation.) 

When you fit a regression model in a program like R, it sets up the observed data into 
something called a *model matrix*, where it has the values for each observation for each of
the independent variables you want to include, plus a column of "1"s for the intercept, if you
model structure includes one. The regression model will fit a coefficient
for each of these columns in the model matrix. In a simple case, this model matrix will just
repeat each of your independent variables. For example, the model matrix for the model
`mod_overdisp_reg`, which we fit in the last chapter and which included only an intercept
and a linear term for `tmean`, looks like this (the `model.matrix` function will give you 
the model matrix of any `glm` object that you get by running the `glm` function in R):  

```{r echo = FALSE}
mod_ovdisp_reg <- glm(all ~ tmean, data = obs, family = "quasipoisson")
mod_ctrl_dow <- glm(all ~ tmean + factor(dow, ordered = FALSE), 
                    data = obs, family = "quasipoisson")
```

```{r}
mod_ovdisp_reg %>% 
  model.matrix() %>% 
  head()
```

We already start using the idea of basis variables when we include categorical variables, like
day of week. Here is the model matrix for the `mod_ctrl_dow` model that we fit in the last
chapter:

```{r}
mod_ctrl_dow %>% 
  model.matrix() %>% 
  head()
```

You can see that the regression call broke the day-of-week variable up into a set of indicator variables, which 
equal either 1 or 0. There will be one less of these than the number of categories for the
variable; in otherwords, if the categorical variable took two values (Weekend / Weekday), then
this would be covered by a single column; since there are seven days in the week, the 
day-of-week variable breaks into six (7 - 1) columns. The first level of the categories 
(Sunday in this case) serves as a baseline and doesn't get a value. The others levels
(Monday, Tuesday, etc.) each get their own indicator variable---so, their own column in 
the model matrix---which equals "1" on that day (i.e., "1" for the Monday column if the date
of the observation is a Monday) and "0" on all other days. 

Now let's go back and look at the example of including temperature in the model as a nonlinear
function, starting with the simple example of using a second-degree polynomial. What would 
the basis for that look like? We can find out by fitting the model and then looking at the
model matrix (the `I()` function lets us specify a transformation of a column from the 
data when we set up the regression equation structure in `glm`): 

```{r}
mod_polynomial_temp <- glm(all ~ tmean + I(tmean ^ 2), 
                           data = obs, family = "quasipoisson")
mod_polynomial_temp %>% 
  model.matrix() %>% 
  head()
```

You can see that this has created two columns based on the temperature variable, one with 
it untransformed and one with it squared. The regression will estimate coefficients for each 
of these basis variables of temperature, and then that allows you to fit a model with a 
function of temperature, rather than solely temperature. Here are the coefficients from 
this model: 

```{r}
mod_polynomial_temp %>% 
  tidy()
```

Therefore, it's fit the following model to describe the association between temperature and
mortality: 

$$
log(E(Y)) = 5.3092 - 0.0298T + 0.0007T^2
$$
If you want to estimate the relative risk of mortality at 15 degrees Celsius versus 10 
degrees Celsius with this model (which is confounded by long-term and seasonal trends, so 
has some issues we'll want to fix), you can take the following process. Take the two 
equations for the expected mortality when $T$ is 15 and 10, respectively: 

$$
log(E(Y|T=15)) = 5.3092 - 0.0298(15) + 0.0007(15^2) \\
log(E(Y|T=10)) = 5.3092 - 0.0298(10) + 0.0007(10^2)  
$$
If you subtract the second from the first, you get: 

$$
log(E(Y|T=15)) - log(E(Y|T=10)) = (5.3092 - 5.3092) - 0.0298(15 - 10) + 0.0007(15^2 - 10^2)
$$
which simplifies to (if the left part isn't clear, review the rules for how you can manipulate
logarithms): 

$$
log(\frac{E(Y|T=15)}{E(Y|T=10)}) = - 0.0298(5) + 0.0007(125) = -0.0615
$$
Exponentiate both sides, to get to the relative risk at 15 degrees versus 10 degrees (in 
other words, the ratio of expected mortality at 15 degrees to that at 10 degrees): 

$$
\frac{E(Y|T=15)}{E(Y|T=10)} = e^{-0.0615} =  0.94
$$
You can see that the basis variables are great in helping us explore how an independent 
variable might be related to the outcome in a non-linear way, but there's a bit of a cost 
in terms of us needing to take some extra steps to interpret the results from that model. 
Also, note that there's not a single coefficient that we can extract from the model as 
a summary of the relationship. Instead, we needed to pick a reference temperature (10 degrees
in this example) and compare to that to get an estimated relative risk. We'll see the 
same pattern as we move to using natural cubic splines to create the basis variables for
temperature.

Now let's move to a spline. The function the spline runs to transform the temperature 
variable into the different basis variables is more complex than for the polynomial example,
but the result is similar: you get several columns to fit in the model for a variable, compared
to the single column you would include if you were only fitting a linear term. If you 
take a look at the output of running the `ns` function on temperature in the data, you can 
see that it creates several new columns (one for each degree of freedom), which will be the
basis variables in the regression:

```{r}
ns(obs$tmean, df = 4) %>% 
  head()
```

The output from `ns` also has some metadata, included in the attributes of the object, that
have some other information about the spline function, including where the knots were
placed and where the boundary knots (which are at the outer ranges of the data) are placed.
You can the `str` function to explore both the data and metadata stored in this object:

```{r}
ns(obs$tmean, df = 4) %>% 
  str()
```

This is saying, for example, that the internal knots for this spline were put at the 
25th, 50th, and 75th quantiles of the temperature data, where were 7.47 degrees, 11.47 degrees, 
and 15.93 degrees. (The defaults for `ns` is to place knots evenly at percentiles, based
on the number of degrees of freedom you specify. You can change this by placing the knots
"by hand", using the `knots` argument in the `ns` function.)

This spline object can be used not just for its basis values, but also to "predict" new
basis values for a new set of temperature values. For example, you could figure out 
what the basis values from this spline would be for every degree of temperature between 
-6 and 29 using the following code:

```{r}
temp_spline <- ns(obs$tmean, df = 4)
temp_spline_preds <- predict(temp_spline, newx = -6:29)
temp_spline_preds %>% 
  head()
```

This is handy, because it will help us visualize the relationship we fit in a regression 
model---we can start with these basis values for each degree in our temperature range, and 
then use the regression coefficients for each basis variable to estimate relative risk 
at that temperature compared to a reference temperature, exactly as we did in the equations
for the polynomial function of temperature earlier, comparing 15 degrees C to the reference
of 10 degrees. 

All we need now are the regression coefficients for each of these temperature basis 
variables. We can extract those from the model we fit earlier, where we included 
`ns(tmean, 4)` as one of the model terms. Here, I'm using `tidy` to get the model 
coefficients and then, because there are *lots* of them (from fitting a spline for 
time with lots of degrees of freedom), I'm using the `str_detect` function from the 
`stringr` package to pick out just those with "tmean" in the `term` column:

```{r}
mod_ctrl_nl_temp %>% 
  tidy() %>% 
  filter(str_detect(term, "tmean"))
```

You can add on `pull` to pull out just the `estimate` column as a vector, which will be 
helpful when we want to multiple these coefficients by the basis values for each degree
of temperature across our temperature range:

```{r}
temp_spline_ests <- mod_ctrl_nl_temp %>% 
  tidy() %>% 
  filter(str_detect(term, "tmean")) %>% 
  pull(estimate)
temp_spline_ests
```

Now, let's put this together to see how relative risk of mortality changes as you move
across the temperature range! First, we can set up a dataframe that has a column with 
each unit of temperature across our range---these are the temperatures where we want to 
estimate relative risk---and then the estimated relative risk at that temperature. By 
default, we'll be comparing to the lowest temperature in the original data, but we'll talk
in a minute about how to adjust to a different reference temperature. You can use 
matrix multiplication (`%*%`) as a shorthand way to multiple each column of the spline
basis variables from `temp_spline_preds` by its estimated coefficient from the regression 
model, saved in `temp_spline_ests`, and then add all of those values together (this is
the same idea as what we did in the equations earlier, for the polynomial basis). Then, 
to get from log relative risk to relative risk, we'll exponentiate that with `exp`. You 
can see the first rows of the results below: 

```{r}
pred_temp_function <- tibble(
  temp = -5:30, 
  temp_func = temp_spline_preds %*% temp_spline_ests,
  rr = exp(temp_func)
)

pred_temp_function %>% 
  head()
```

This is now very easy to plot, with a reference line added at a relative risk of 1.0: 

```{r}
ggplot(pred_temp_function, aes(x = temp, y = rr)) + 
  geom_point() + 
  geom_line() + 
  labs(x = "Temperature", 
       y = "Relative Risk") + 
  geom_hline(yintercept = 1.0, color = "red", linetype = 2)
```

We might want to shift this, so we're comparing the temperature at which mortality risk 
is lowest (sometimes called the *minimum mortality temperature*). This tends to be at 
milder temperatures, in the middle of our range, rather than at the minimum temperature in
the range. To start, let's see what temperature aligns with the lowest relative risk of 
mortality: 

```{r}
pred_temp_function %>% 
  filter(rr == min(rr))
```
We can therefore realign so that the relative risk equals 1.0 when the temperature is 7 degrees
C, and all other relative risks are relative to a reference temperature of 7 degrees C:

```{r}
pred_temp_function <- pred_temp_function %>% 
  mutate(temp_func_reset = temp_func - temp_func[temp == 7],
         rr_reset = exp(temp_func_reset)
)

ggplot(pred_temp_function, aes(x = temp, y = rr_reset)) + 
  geom_point() + 
  geom_line() + 
  labs(x = "Temperature", 
       y = "Relative Risk") + 
  geom_hline(yintercept = 1.0, color = "red", linetype = 2)
```

This is a fairly cumbersome process, as you've seen with this example, and it would be a 
pain if we had to do it every time we wanted to visualize and explore results from fitting 
regressions that include basis variables from splines. Fortunately, there's a nice R package
called `dlnm` (for "distributed lag nonlinear models") that will help take care of a lot of 
these "under the hood" steps for us. Even better, this package will let us move on to more
complex crossbasis functions, where we fit non-linear functions in two dimensions, and help 
us visualize and interpret results from those models.

To start, make sure you have the `dlnm` package installed on your computer, and then load
it in your R session. This package has a function called `crossbasis` that lets you build
a crossbasis function to use in a regression model. We'll talk more about these types
of functions later in this chapter; for right now, we'll be a bit simpler and just use it
to create our spline function of temperature. 

In the `crossbasis` function, you start by putting in the vector of the variable that you 
want to expand into basis variables. In our case, this is temperature, which we have saved
as the `tmean` column in the `obs` dataset. You use the `argvar` to give some information 
about the basis you want to use for that variable. This will both include the type of 
basis function (`"ns"` for a natural cubic spline), and then also arguments you want to 
pass to that basis function, like degrees of freedom (`df`) or knot locations (`knots`) 
if you're fitting a spline. The other arguments allow you to specify a function of lagged
time; we won't use that yet, so you can just include `lag = 0` and `arglag = list(fun = "integer")` to create a crossbasis where we only consider same-day effects in a simple way.
If you look at the output, you'll see it's very similar to the basis created by the 
`ns` call earlier (in fact, the values in each column should be identical):

```{r message = FALSE, warning = FALSE}
library(dlnm)
temp_basis <- crossbasis(obs$tmean, lag = 0, 
                         argvar = list(fun = "ns", df = 4), 
                         arglag = list(fun = "integer"))
temp_basis %>% 
  head()
```

To estimate the regression coefficients, we'll put this whole crossbasis in as one of 
our terms in the `glm` regression equation:

```{r}
dlnm_mod_1 <- glm(all ~ temp_basis + factor(dow, ordered = FALSE) +
                          ns(time, df = 158), 
                        data = obs, family = "quasipoisson")
```

As when we fit the spline earlier, you can see that this gives us a set of coefficients, 
one for each column in the matrix of crossbasis variables:

```{r}
dlnm_mod_1 %>% 
  tidy() %>% 
  filter(str_detect(term, "temp_basis"))
```

However, there's an advantage with using `crossbasis`, even though it's looked pretty similar
to using `ns` up to now. That's that there are some special functions that let us predict
and visualize the model results without having to do all the work we did before. 

For example, there's a function called `crosspred` that will give us a number of values, 
including the estimated relative risk compared to a reference value. To use this, we 
need to input the object name of our crossbasis object (`temp_basis`) and the name of our
regression model object (`dlnm_mod_1`). We also want to tell it which temperature we want
to use as our reference (`cen = 7` to compare everything to 7 degrees C) and what interval
we want for predictions (`by = 1` will give us an estimate for every degree temperature 
along our range). The output is a list with a lot of elements, which you can get a view of 
with `str`:

```{r}
crosspred(basis = temp_basis, model = dlnm_mod_1, cen = 7, by = 1) %>% 
  str()
```

The one we'll look at right now is `allRRfit`. You can extract it with (here I'm showing
how you can do it with piping and the `pluck` function to pull out an element of a list, 
but you could do other approaches, too): 

```{r}
est_rr <- dlnm_mod_1 %>% 
  crosspred(basis = temp_basis, model = ., cen = 7, by = 1) %>% 
  pluck("allRRfit")
est_rr
```

To make it easier to work with, let's put this in a dataframe. Note that the temperatures
are included as the names in the vector, so we can extract those with `names`:

```{r}
dlnm_temp_function <- tibble(tmean = as.numeric(names(est_rr)), 
                             rr = est_rr)

dlnm_temp_function %>% 
  head()
```
This has gotten us (much more quickly!) to estimates of the relative risk of mortality at
each temperature compared to a reference of 7 degrees C. We can plot this the same way we
did earlier, and you'll notice that this plot is identical to the one we created based on the
regression with `ns` earlier:

```{r}
ggplot(dlnm_temp_function, aes(x = tmean, y = rr)) + 
  geom_point() + 
  geom_line() + 
  labs(x = "Temperature", y = "Relative risk") + 
  geom_hline(yintercept = 1.0, color = "red", linetype = 2)
```

The `dlnm` package also includes some functions specifically for plotting. One downside 
is that they're in base R, rather than based on `ggplot2`, which makes them a bit 
harder to customize. However, they're a great way to get a first look at your results:

```{r}
dlnm_mod_1 %>% 
  crosspred(basis = temp_basis, model = ., cen = 6, by = 1) %>% 
  plot()
```

Don't worry if all of this about basis functions is a lot! If this is the first time you've
seen this, it will likely take a few passes to get comfortable with it---and while the idea
of basis variables is fairly straightforward, it's certainly not straightforward to figure
out how to interpret the results of the model that you fit. It is worth the effort, though, 
as this is a very powerful tool when working with regression modeling in environmental 
epidemiology. 

## Cross-basis functions in GLMs

[Using a cross-basis to model an exposure's association with the
outcome in two dimensions (dimensions of time and exposure level)]

