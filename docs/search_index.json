[["index.html", "Advanced Epidemiological Analysis Chapter 1 Overview 1.1 License", " Advanced Epidemiological Analysis Andreas M. Neophytou and G. Brooke Anderson 2021-08-16 Chapter 1 Overview This is a the coursebook for the Colorado State University course ERHS 732, Advanced Epidemiological Analysis. This course provides the opportunity to implement theoretical expertise through designing and conducting advanced epidemiologic research analyses and to gain in-depth experience analyzing datasets from the environmental epidemiology literature. This book is in development over the Fall 2021 semester. 1.1 License This book is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License, while all code in the book is under the MIT license. Click on the Next button (or navigate using the links in the table of contents) to continue. "],["courseinfo.html", "Chapter 2 Course information 2.1 Course learning objectives 2.2 Meeting time and place 2.3 Class Structure and Expectations 2.4 Course grading 2.5 Textbooks and Course Materials", " Chapter 2 Course information This is a the coursebook for the Colorado State University course ERHS 732, Advanced Epidemiological Analysis. This course provides the opportunity to implement theoretical expertise through designing and conducting advanced epidemiologic research analyses and to gain in-depth experience analyzing datasets from the environmental epidemiology literature. This course will complement the student’s training in advanced epidemiological methods, leveraging regression approaches and statistical programming, providing the opportunity to implement their theoretical expertise through designing and conducting advanced epidemiologic research analyses. During the course, students will gain in-depth experience analyzing two datasets from the environmental epidemiology literature—(1) time series data with daily measures of weather, air pollution, and cardiorespiratory outcomes in London, England and (2) a dataset with measures from the Framingham Heart Study. Additional datasets and studies will be discussed and explored as a supplement. This class will utilize a variety of instructional formats, including short lectures, readings, topic specific examples from the substantive literature, discussion and directed group work on in-course coding exercises putting lecture and discussion content into practice. A variety of teaching modalities will be used, including group discussions, student directed discussions, and in-class group exercises. It is expected that before coming to class, students will read the required papers for the week, as well as any associated code included in the papers’ supplemental materials. Students should come to class prepared to do statistical programming (i.e., bring a laptop with statistical software, download any datasets needed for the week etc). Participation is based on in-class coding exercises based on each week’s topic. If a student misses a class, they will be expected to complete the in-course exercise outside of class to receive credit for participation in that exercise. Students will be required to do mid-term and final projects which will be presented in class and submitted as a written write-up describing the project. Prerequisites for this course are: ERHS 534 or ERHS 535 and ERHS 640 and STAR 511 or STAT 511A or STAT 511B 2.1 Course learning objectives The learning objectives for this proposed course complement core epidemiology and statistics courses required by the program and provide the opportunity for students to implement theoretical skills and knowledge gained in those courses in a more applied setting. Upon successful completion of this course students will be able to: List several possible statistical approaches to answering an epidemiological research questions. (Knowledge) Choose among analytical approaches learned in previous courses to identify one that is reasonable for an epidemiological research question. (Application) Design a plan for cleaning and analyzing data to answer an epidemiological research question, drawing on techniques learned in previous and concurrent courses. (Synthesis) Justify the methods and code used to answer an epidemiological research question. (Evaluation) Explain the advantages and limitations of a chosen methodological approach for evaluating epidemiological data. (Evaluation) Apply advanced epidemiological methods to analyze example data, using a regression modeling framework. (Application) Apply statistical programming techniques learned in previous courses to prepare epidemiological data for statistical analysis and to conduct the analysis. (Application) Interpret the output from statistical analyses of data for an epidemiological research question. (Evaluation) Defend conclusions from their analysis. (Comprehension) Write a report describing the methods, results, and conclusions from an epidemiological analysis. (Application) Construct a reproducible document with embedded code to clean and analyze data to answer an epidemiological research question. (Application) 2.2 Meeting time and place The class will meet on Mondays, 2:00–3:40 PM on the Colorado State University campus in MRB 312. 2.3 Class Structure and Expectations Homework/preparation: Every two weeks we will focus on a different topic. It is expected that before coming to class, students will read the required papers for the week, as well as any associated code included in the papers’ supplemental materials. Students should come to class prepared to prepared to do statistical programming (i.e., bring in a laptop with statistical software, download any datasets needed for the week). In-class schedule: Topic overview: Each class will start with a vocabulary quiz on a select number of the words from the chapter’s vocabulary list. Discussion of analysis and coding points: Students and faculty will be divided into small groups to discuss the chapter and think more deeply about the content. This is a time to bring up questions and relate the chapter concepts to other datasets and/or analysis methods you are familiar with. Group work: In small groups, students will work on designing an epidemiological analysis for the week’s topic and developing code to implement that analysis. Students will use the GitHub platform to work collaboratively during and between class meetings. Wrap-up: We will reconvene as one group at the end to discuss topics that came up in small group work and to outline expectations for students before the next meeting. 2.4 Course grading Assessment Components Percentage of Grade Midterm written report 30 Midterm presentation 15 Final written report 30 Final presentation 15 Participation in in-course exercises 10 2.5 Textbooks and Course Materials Readings for this course will focus on peer-reviewed literature that will be posted for the students in the class. Additional references that will be useful to students throughout the semester include: Garrett Grolemund and Hadley Wickham, R for Data Science, O’Reilly, 2017. (Available for free online at https://r4ds.had.co.nz/ and in print through most large book sellers.) Miguel A. Hernán and James M. Robins, Causal Inference: What If, Boca Raton: Chapman &amp; Hall/CRC, 2020. (Available for free online at https://cdn1.sph.harvard.edu/wp-content/uploads/sites/1268/2021/01/ciwhatif_hernanrobins_31jan21.pdf with a print version anticipated in 2021.) Francesca Dominici and Roger D. Peng, Statistical Methods for Environmental Epidemiology with R, Springer, 2008. (Available online through the CSU library or in print through Springer.) "],["time-series-case-crossover-study-designs.html", "Chapter 3 Time series / case-crossover study designs 3.1 Readings 3.2 Time series data 3.3 Exploratory data analysis 3.4 Fitting models 3.5 Chapter vocabulary", " Chapter 3 Time series / case-crossover study designs We’ll start by exploring common characteristics in time series data for environmental epidemiology. In the first have of the class, we’re focusing on a very specific type of study—one that leverages large-scale vital statistics data, collected at a regular time scale (e.g., daily), combined with large-scale measurements of a climate-related exposure, with the goal of estimating the typical relationship between the level of the exposure and risk of a health outcome. For example, we may have daily measurements of particulate matter pollution for a city, measured daily at a set of Environmental Protection Agency (EPA) monitors. We want to investigate how risk of cardiovascular mortality changes in the city from day to day in association with these pollution levels. If we have daily counts of the number of cardiovascular deaths in the city, we can create a statistical model that fits the exposure-response association between particulate matter concentration and daily risk of cardiovascular mortality. These statistical models—and the type of data used to fit them—will be the focus of the first part of this course. 3.1 Readings The required readings for this chapter are: Bhaskaran et al. (2013) This paper provides an overview of time series regression in environmental epidemiology. Vicedo-Cabrera, Sera, and Gasparrini (2019) This paper provides a tutorial of all the steps for a projecting of health impacts of temperature extremes under climate change. One of the steps is to fit the exposure-response association using present-day data (the section on “Estimation of Exposure-Response Associations” in the paper). In this chapter, we will go into details on that step, and that section of the paper is the only required reading for this chapter. Later in the class, we’ll look at other steps covered in this paper. Supplemental material for this paper is available to download by clicking http://links.lww.com/EDE/B504. You will need the data in this supplement for the exercises for class. The following are supplemental readings (i.e., not required, but may be of interest) associated with the material in this chapter: B. Armstrong et al. (2012) Commentary that provides context on how epidemiological research on temperature and health can help inform climate change policy. B. G. Armstrong, Gasparrini, and Tobias (2014) This paper describes different data structures for case-crossover data, as well as how conditional Poisson regression can be used in some cases to fit a statistical model to these data. Supplemental material for this paper is available at https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/1471-2288-14-122#Sec13. Imai et al. (2015) Typically, the time series study design covered in this chapter is used to study non-communicable health outcomes. This paper discusses opportunities and limitations in applying a similar framework for infectious disease. Lu and Zeger (2007) Heavier on statistics. This paper shows how, under conditions often common for environmental epidemiology studies, case-crossover and time series methods are equivalent. Gasparrini (2014) Heavier on statistics. This provides the statistical framework for the distributed lag model for environmental epidemiology time series studies. 3.2 Time series data Let’s start by exploring the type of dataset that can be used for these time series–style studies in environmental epidemiology. In the examples in this chapter, we’ll be using data that comes as part of the Supplemental Material in on of this chapter’s required readings, Vicedo-Cabrera, Sera, and Gasparrini (2019) . Follow the link for the supplement for this article and then look for the file “lndn_obs.csv.” This is the file we’ll use as the example data in this chapter. These data are saved in a csv format (that is, a plain text file, with commas used as the delimiter), and so they can be read into R using the read_csv function from the readr package (part of the tidyverse). For example, you can use the following code to read in these data, assuming you have saved them in a “data” subdirectory of your current working directory: library(tidyverse) # Loads all the tidyverse packages, including readr obs &lt;- read_csv(&quot;data/lndn_obs.csv&quot;) obs ## # A tibble: 8,279 x 14 ## date year month day doy dow all all_0_64 all_65_74 all_75_84 ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1990-01-01 1990 1 1 1 Mon 220 38 38 82 ## 2 1990-01-02 1990 1 2 2 Tue 257 50 67 87 ## 3 1990-01-03 1990 1 3 3 Wed 245 39 59 86 ## 4 1990-01-04 1990 1 4 4 Thu 226 41 45 77 ## 5 1990-01-05 1990 1 5 5 Fri 236 45 54 85 ## 6 1990-01-06 1990 1 6 6 Sat 235 48 48 84 ## 7 1990-01-07 1990 1 7 7 Sun 231 38 49 96 ## 8 1990-01-08 1990 1 8 8 Mon 235 46 57 76 ## 9 1990-01-09 1990 1 9 9 Tue 250 48 54 96 ## 10 1990-01-10 1990 1 10 10 Wed 214 44 46 62 ## # … with 8,269 more rows, and 4 more variables: all_85plus &lt;dbl&gt;, tmean &lt;dbl&gt;, ## # tmin &lt;dbl&gt;, tmax &lt;dbl&gt; This example dataset shows many characteristics that are common for datasets for time series studies in environmental epidemiology. Time series data are essentially a sequence of data points repeatedly taken over a certain time interval (e.g., day, week, month etc). General characteristics of time series data for environmental epidemiology studies are: Observations are given at an aggregated level. For example, instead of individual observations for each person in London, the obs data give counts of deaths throughout London. The level of aggregation is often determined by geopolitical boundaries, for example, counties or ZIP codes in the US. Observations are given at regularly spaced time steps over a period. In the obs dataset, the time interval is day. Typically, values will be provided continuously over that time period, with observations for each time interval. Occasionally, however, the time series data may only be available for particular seasons (e.g., only warm season dates for an ozone study), or there may be some missing data on either the exposure or health outcome over the course of the study period. Observations are available at the same time step (e.g., daily) for (1) the health outcome, (2) the environmental exposure of interest, and (3) potential time-varying confounders. In the obs dataset, the health outcome is mortality (from all causes; sometimes, the health outcome will focus on a specific cause of mortality or other health outcomes such as hospitalizations or emergency room visits). Counts are given for everyone in the city for each day (all column), as well as for specific age categories (all_0_64 for all deaths among those up to 64 years old, and so on). The exposure of interest in the obs dataset is temperature, and three metrics of this are included (tmean, tmin, and tmax). Day of the week is one time-varying factor that could be a confounder, or at least help explain variation in the outcome (mortality). This is included through the dow variable in the obs data. Sometimes, you will also see a marker for holidays included as a potential time-varying confounder, or other exposure variables (temperature is a potential confounder, for example, when investigating the relationship between air pollution and mortality risk). Multiple metrics of an exposure and / or multiple health outcome counts may be included for each time step. In the obs example, three metrics of temperature are included (minimum daily temperature, maximum daily temperature, and mean daily temperature). Several counts of mortality are included, providing information for specific age categories in the population. The different metrics of exposure will typically be fit in separate models, either as a sensitivity analysis or to explore how exposure measurement affects epidemiological results. If different health outcome counts are available, these can be modeled in separate statistical models to determine an exposure-response function for each outcome. 3.3 Exploratory data analysis When working with time series data, it is helpful to start with some exploratory data analysis. This type of time series data will often be secondary data—it is data that was previously collected, as you are re-using it. Exploratory data analysis is particularly important with secondary data like this. For primary data that you collected yourself, following protocols that you designed yourself, you will often be very familiar with the structure of the data and any quirks in it by the time you are ready to fit a statistical model. With secondary data, however, you will typically start with much less familiarity about the data, how it was collected, and any potential issues with it, like missing data and outliers. Exploratory data analysis can help you become familiar with your data. You can use summaries and plots to explore the parameters of the data, and also to identify trends and patterns that may be useful in designing an appropriate statistical model. For example, you can explore how values of the health outcome are distributed, which can help you determine what type of regression model would be appropriate, and to see if there are potential confounders that have regular relationships with both the health outcome and the exposure of interest. You can see how many observations have missing data for the outcome, the exposure, or confounders of interest, and you can see if there are any measurements that look unusual. This can help in identifying quirks in how the data were recorded—for example, in some cases ground-based weather monitors use -99 or -999 to represent missing values, definitely something you want to catch and clean-up in your data (replacing with R’s NA for missing values) before fitting a statistical model! The following applied exercise will take you through some of the questions you might want to answer through this type of exploratory analysis. In general, the tidyverse suite of R packages has loads of tools for exploring and visualizing data in R. The lubridate package from the tidyverse, for example, is an excellent tool for working with date-time data in R, and time series data will typically have at least one column with the timestamp of the observation (e.g., the date for daily data). You may find it worthwhile to explore this package some more. There is a helpful chapter in Wickham and Grolemund (2016), https://r4ds.had.co.nz/dates-and-times.html, as well as a cheatsheet at https://evoldyn.gitlab.io/evomics-2018/ref-sheets/R_lubridate.pdf. For visualizations, if you are still learning techniques in R, two books you may find useful are Healy (2018) (available online at https://socviz.co/) and Chang (2018) (available online at http://www.cookbook-r.com/Graphs/). Applied: Exploring time series data Read the example time series data into R and explore it to answer the following questions: What is the study period for the example obs dataset? (i.e., what dates / years are covered by the time series data?) Are there any missing dates (i.e., dates with nothing recorded) within this time period? Are there any recorded dates where health outcome measurements are missing? Any where exposure measurements are missing? Are there seasonal trends in the exposure? In the outcome? Are there long-term trends in the exposure? In the outcome? Is the outcome associated with day of week? Is the exposure associated with day of week? Based on your exploratory analysis in this section, talk about the potential for confounding when these data are analyzed to estimate the association between daily temperature and city-wide mortality. Is confounding by seasonal trends a concern? How about confounding by long-term trends in exposure and mortality? How about confounding by day of week? Applied exercise: Example code What is the study period for the example obs dataset? (i.e., what dates / years are covered by the time series data?) In the obs dataset, the date of each observation is included in a column called date. The data type of this column is “Date”—you can check this by using the class function from base R: class(obs$date) ## [1] &quot;Date&quot; Since this column has a “Date” data type, you can run some mathematical function calls on it. For example, you can use the min function from base R to get the earliest date in the dataset and the max function to get the latest. min(obs$date) ## [1] &quot;1990-01-01&quot; max(obs$date) ## [1] &quot;2012-08-31&quot; You can also run the range function to get both the earliest and latest dates with a single call: range(obs$date) ## [1] &quot;1990-01-01&quot; &quot;2012-08-31&quot; This provides the range of the study period for these data. One interesting point is that it’s not a round set of years—instead, the data ends during the summer of the last study year. This doesn’t present a big problem, but is certainly something to keep in mind if you’re trying to calculate yearly averages of any values for the dataset. If you’re getting the average of something that varies by season (e.g., temperature), it could be slightly weighted by the months that are included versus excluded in the partial final year of the dataset. Similarly, if you group by year and then count totals by year, the number will be smaller for the last year, since only part of the year’s included. For example, if you wanted to count the total deaths in each year of the study period, it will look like they go down a lot the last year, when really it’s only because only about half of the last year is included in the study period: obs %&gt;% group_by(year) %&gt;% summarize(deaths_in_year = sum(all)) %&gt;% ggplot(aes(x = year, y = deaths_in_year)) + geom_line() + geom_point() Are there any missing dates within this time period? Are there any recorded dates where health outcome measurements are missing? Any where exposure measurements are missing? There are a few things you should check to answer this question. First (and easiest), you can check to see if there are any NA values within any of the observations in the dataset. This helps answer the second and third parts of the question. The summary function will provide a summary of the values in each column of the dataset, including the count of missing values (NAs) if there are any: summary(obs) ## date year month day ## Min. :1990-01-01 Min. :1990 Min. : 1.000 Min. : 1.00 ## 1st Qu.:1995-09-01 1st Qu.:1995 1st Qu.: 3.000 1st Qu.: 8.00 ## Median :2001-05-02 Median :2001 Median : 6.000 Median :16.00 ## Mean :2001-05-02 Mean :2001 Mean : 6.464 Mean :15.73 ## 3rd Qu.:2006-12-31 3rd Qu.:2006 3rd Qu.: 9.000 3rd Qu.:23.00 ## Max. :2012-08-31 Max. :2012 Max. :12.000 Max. :31.00 ## doy dow all all_0_64 ## Min. : 1.0 Length:8279 Min. : 81.0 Min. : 9.0 ## 1st Qu.: 90.5 Class :character 1st Qu.:138.0 1st Qu.:27.0 ## Median :180.0 Mode :character Median :157.0 Median :32.0 ## Mean :181.3 Mean :160.2 Mean :32.4 ## 3rd Qu.:272.0 3rd Qu.:178.0 3rd Qu.:37.0 ## Max. :366.0 Max. :363.0 Max. :64.0 ## all_65_74 all_75_84 all_85plus tmean ## Min. : 6.00 Min. : 17.00 Min. : 17.00 Min. :-5.503 ## 1st Qu.:23.00 1st Qu.: 41.00 1st Qu.: 39.00 1st Qu.: 7.470 ## Median :29.00 Median : 49.00 Median : 45.00 Median :11.465 ## Mean :30.45 Mean : 50.65 Mean : 46.68 Mean :11.614 ## 3rd Qu.:37.00 3rd Qu.: 58.00 3rd Qu.: 53.00 3rd Qu.:15.931 ## Max. :70.00 Max. :138.00 Max. :128.00 Max. :29.143 ## tmin tmax ## Min. :-8.940 Min. :-3.785 ## 1st Qu.: 3.674 1st Qu.:10.300 ## Median : 7.638 Median :14.782 ## Mean : 7.468 Mean :15.058 ## 3rd Qu.:11.438 3rd Qu.:19.830 ## Max. :20.438 Max. :37.087 Based on this analysis, all observations are complete for all dates included in the dataset. There are no listings for NAs for any of the columns, and this indicates no missing values in the dates for which there’s a row in the data. However, this does not guarantee that every date between the start date and end date of the study period are included in the recorded data. Sometimes, some dates might not get recorded at all in the dataset, and the summary function won’t help you determine when this is the case. One common example in environmental epidemiology is with ozone pollution data. These are sometimes only measured in the warm season, and so may be shared in a dataset with all dates outside of the warm season excluded. There are a few alternative explorations you can do to check this. Perhaps the easiest is to check the number of days between the start and end date of the study period, and then see if the number of observations in the dataset is the same: # Calculate number of days in study period obs %&gt;% # Using piping (%&gt;%) throughout to keep code clear pull(date) %&gt;% # Extract the `date` column as a vector range() %&gt;% # Take the range of dates (earliest and latest) diff() # Calculate time difference from start to finish of study ## Time difference of 8278 days # Get number of observations in dataset---should be 1 more than time difference obs %&gt;% nrow() ## [1] 8279 This indicates that there is an observation for every date over the study period, since the number of observations should be one more than the time difference. In the next question, we’ll be plotting observations by time, and typically this will also help you see if there are large chunks of missing dates in the data. Are there seasonal trends in the exposure? In the outcome? You can use a simple plot to visualize patterns over time in both the exposure and the outcome. For example, the following code plots a dot for each daily temperature observation over the study period. The points are set to a smaller size (size = 0.5) and plotted with some transparency (alpha = 0.5) since there are so many observations. ggplot(obs, aes(x = date, y = tmean)) + geom_point(alpha = 0.5, size = 0.5) There is (unsurprisingly) clear evidence here of a strong seasonal trend in mean temperature, with values typically lowest in the winter and highest in the summer. You can plot the outcome variable in the same way: ggplot(obs, aes(x = date, y = all)) + geom_point(alpha = 0.5, size = 0.5) Again, there are seasonal trends, although in this case they are inversed. Mortality tends to be highest in the winter and lowest in the summer. Further, the seasonal pattern is not equally strong in all years—some years it has a much higher winter peak, probably in conjunction with severe influenza seasons. Another way to look for seasonal trends is with a heatmap-style visualization, with day of year along the x-axis and year along the y-axis. This allows you to see patterns that repeat around the same time of the year each year (and also unusual deviations from normal seasonal patterns). For example, here’s a plot showing temperature in each year, where the observations are aligned on the x-axis by time in year. We’re using the doy—which stands for “day of year” (i.e., Jan 1 = 1; Jan 2 = 2; … Dec 31 = 365 as long as it’s not a leap year) as the measure of time in the year. We’ve reversed the y-axis so that the earliest years in the study period start at the top of the visual, then later study years come later—this is a personal style, and it would be no problem to leave the y-axis as-is. We’ve used the viridis color scale for the fill, since that has a number of features that make it preferable to the default R color scale, including that it is perceptible for most types of color blindness and be printed out in grayscale and still be correctly interpreted. library(viridis) ggplot(obs, aes(x = doy, y = year, fill = tmean)) + geom_tile() + scale_y_reverse() + scale_fill_viridis() From this visualization, you can see that temperatures tend to be higher in the summer months and lower in the winter months. “Spells” of extreme heat or cold are visible—where extreme temperatures tend to persist over a period, rather than randomly fluctuating within a season. You can also see unusual events, like the extreme heat wave in the summer of 2003, indicated with the brightest yellow in the plot. We created the same style of plot for the health outcome. In this case, we focused on mortality among the oldest age group, as temperature sensitivity tends to increase with age, so this might be where the strongest patterns are evident. ggplot(obs, aes(x = doy, y = year, fill = all_85plus)) + geom_tile() + scale_y_reverse() + scale_fill_viridis() For mortality, there tends to be an increase in the winter compared to the summer. Some winters have stretches with particularly high mortality—these are likely a result of seasons with strong influenza outbreaks. You can also see on this plot the impact of the 2003 heat wave on mortality among this oldest age group—an unusual spot of light green in the summer. Are there long-term trends in the exposure? In the outcome? Some of the plots we created in the last section help in exploring this question. For example, the following plot shows a clear pattern of decreasing daily mortality counts, on average, over the course of the study period: ggplot(obs, aes(x = date, y = all)) + geom_point(alpha = 0.5, size = 0.5) It can be helpful to add a smooth line to help detect these longer-term patterns, which you can do with geom_smooth: ggplot(obs, aes(x = date, y = all)) + geom_point(alpha = 0.5, size = 0.5) + geom_smooth() You could also take the median mortality count across each year in the study period, although you should take out any years without a full year’s worth of data before you do this, since there are seasonal trends in the outcome: obs %&gt;% group_by(year) %&gt;% filter(year != 2012) %&gt;% # Take out the last year summarize(median_mort = median(all)) %&gt;% ggplot(aes(x = year, y = median_mort)) + geom_line() Again, we see a clear pattern of decreasing mortality rates in this city over time. This means we need to think carefully about long-term time patterns as a potential confounder. It will be particularly important to think about this if the exposure also has a strong pattern over time. For example, air pollution regulations have meant that, in many cities, there may be long-term decreases in pollution concentrations over a study period. Is the outcome associated with day of week? Is the exposure associated with day of week? The data already has day of week as a column in the data (dow). However, this is in a character data type, so it doesn’t have the order of weekdays encoded (e.g., Monday comes before Tuesday). This makes it hard to look for patterns related to things like weekend / weekday. class(obs$dow) ## [1] &quot;character&quot; We could convert this to a factor and encode the weekday order when we do it, but it’s even easier to just recreate the column from the date column. We used the wday function from the lubridate package to do this—it extracts weekday as a factor, with the order of weekdays encoded (using a special “ordered” factor type): library(lubridate) obs &lt;- obs %&gt;% mutate(dow = wday(date, label = TRUE)) class(obs$dow) ## [1] &quot;ordered&quot; &quot;factor&quot; levels(obs$dow) ## [1] &quot;Sun&quot; &quot;Mon&quot; &quot;Tue&quot; &quot;Wed&quot; &quot;Thu&quot; &quot;Fri&quot; &quot;Sat&quot; We looked at the mean, median, and 25th and 75th quantiles of the mortality counts by day of week: obs %&gt;% group_by(dow) %&gt;% summarize(mean(all), median(all), quantile(all, 0.25), quantile(all, 0.75)) ## # A tibble: 7 x 5 ## dow `mean(all)` `median(all)` `quantile(all, 0.25)` `quantile(all, 0.75)` ## * &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Sun 156. 154 136 173 ## 2 Mon 161. 159 138 179 ## 3 Tue 161. 158 139 179 ## 4 Wed 160. 157 138. 179 ## 5 Thu 161. 158 139 179 ## 6 Fri 162. 159 141 179 ## 7 Sat 159. 156 137 178 Mortality tends to be a bit higher on weekdays than weekends, but it’s not a dramatic difference. We did the same check for temperature: obs %&gt;% group_by(dow) %&gt;% summarize(mean(tmean), median(tmean), quantile(tmean, 0.25), quantile(tmean, 0.75)) ## # A tibble: 7 x 5 ## dow `mean(tmean)` `median(tmean)` `quantile(tmean, 0.2… `quantile(tmean, 0.… ## * &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Sun 11.6 11.3 7.48 15.9 ## 2 Mon 11.6 11.4 7.33 15.8 ## 3 Tue 11.5 11.4 7.48 15.9 ## 4 Wed 11.7 11.7 7.64 16.0 ## 5 Thu 11.6 11.5 7.57 16.0 ## 6 Fri 11.6 11.6 7.41 15.8 ## 7 Sat 11.6 11.5 7.53 15.9 In this case, there does not seem to be much of a pattern by weekday. You can also visualize the association using boxplots: ggplot(obs, aes(x = wday(date, label = TRUE), y = all)) + geom_boxplot() You can also try violin plots—these show the full distribution better than boxplots, which only show quantiles. ggplot(obs, aes(x = dow, y = all)) + geom_violin(draw_quantiles = c(0.25, 0.5, 0.75)) All these reinforce that there are some small differences in weekend versus weekday patterns for mortality. There isn’t much pattern by weekday with temperature, so in this case weekday is unlikely to be a confounder (the same is not true with air pollution, which often varies based on commuting patterns and so can have stronger weekend/weekday differences). However, since it does help some in explaining variation in the health outcome, it might be worth including in our models anyway, to help reduce random noise. 3.4 Fitting models One of the readings for this week, Vicedo-Cabrera, Sera, and Gasparrini (2019), includes a section on fitting exposure-response functions to describe the association between daily mean temperature and mortality risk. This article includes example code in its supplemental material, with code for fitting the model to these time series data in the file named “01EstimationERassociation.r.” The model may at first seem complex, but it is made up of a number of fairly straightforward pieces (although some may initially seem complex): The model framework is a generalized linear model (GLM) This GLM is fit assuming an error distribution and a link function appropriate for count data The GLM is fit assuming an error distribution that is also appropriate for data that may be overdispersed The model includes control for day of the week by including a categorical variable The model includes control for long-term and seasonal trends by including a spline (in this case, a natural cubic spline) for the day in the study The model fits a flexible, non-linear association between temperature and mortality risk also using a spline The model fits a flexible non-linear association between temperature on a series of preceeding days and current day and mortality risk on the current day using a distributed lag approach The model jointly describes both of the two previous non-linear associations by fitting these two elements through one construct in the GLM, a cross-basis term In this section, we will work through the elements, building up the code to get to the full model that is fit in Vicedo-Cabrera, Sera, and Gasparrini (2019). Fitting a GLM to time series data The GLM framework unites a number of types of regression models you may have previously worked with. One basic regression model that can be fit within this framework is a linear regression model. However, the framework also allows you to also fit, among others, logistic regression models (useful when the outcome variable can only take one of two values, e.g., success / failure or alive / dead), Poisson regression models (useful when the outcome variable is a count or rate). This generalized framework brings some unity to these different types of regression models. From a practical standpoint, it has allowed software developers to easily provide a common interface to fit these types of models. In R, the common function call to fit GLMs is glm. Within the GLM framework, the elements that separate different regression models include the link function and the error distribution. The error distribution encodes the assumption you are enforcing about how the errors after fitting the model are distributed. If the outcome data are normally distributed (a.k.a., follow a Gaussian distribution), after accounting for variance explained in the outcome by any of the model covariates, then a linear regression model may be appropriate. For count data—like numbers of deaths a day—this is unlikely, unless the average daily mortality count is very high (count data tend to come closer to a normal distribution the further their average gets from 0). For binary data—like whether each person in a study population died on a given day or not—normally distributed errors are also unlikely. Instead, in these two cases, it is typically more appropriate to fit GLMs with Poisson and binomial “families,” respectively, where the family designation includes an appropriate specification for the variance when fitting the model based on these outcome types. The other element that distinguishes different types of regression within the GLM framework is the link function. The link function applies a transformation on the combination of independent variables in the regression equation when fitting the model. With normally distributed data, an identity link is often appropriate—with this link, the combination of independent variables remain unchanged (i.e., keep their initial “identity”). With count data, a log link is often more appropriate, while with binomial data, a logit link is often used. Finally, data will often not perfectly adhere to assumptions. For example, the Poisson family of GLMs assumes that variance follows a Poisson distribution (The probability mass function for Poisson distribution \\(X \\sim {\\sf Poisson}(\\mu)\\) is denoted by \\(f(k;\\mu)=Pr[X=k]= \\displaystyle \\frac{\\mu^{k}e^{-\\mu}}{k!}\\), where \\(k\\) is the number of occurences, and \\(\\mu\\) is equal to the expected number of cases). With this distribution, the variance is equal to the mean (\\(\\mu=E(X)=Var(X)\\)). With real-life data, this assumption is often not valid, and in many cases the variance in real life count data is larger than the mean. This can be accounted for when fitting a GLM by setting an error distribution that does not require the variance to equal the mean—instead, both a mean value and something like a variance are estimated from the data, assuming an overdispersion parameter \\(\\phi\\) so that \\(Var(X)=\\phi E(X)\\). In environmental epidemiology, time series are often fit to allow for this overdispersion. This is because if the data are overdispersed but the model does not account for this, the standard errors on the estimates of the model parameters may be artificially small. If the data are not overdispersed (\\(\\phi=1\\)), the model will identify this when being fit to the data, so it is typically better to prefer to allow for overdispersion in the model (if the size of the data were small, you may want to be parsimonious and avoid unneeded complexity in the model, but this is typically not the case with time series data). In the next section, you will work through the steps of developing a GLM to fit the example dataset obs. For now, you will only fit a linear association between mean daily temperature and mortality risk, eventually including control for day of week. In later work, especially the next chapter, we will build up other components of the model, including control for the potential confounders of long-term and seasonal patterns, as well as advancing the model to fit non-linear associations, distributed by time, through splines, a distributed lag approach, and a cross-basis term. Applied: Fitting a GLM to time series data In R, the function call used to fit GLMs is glm. Most of you have likely covered GLMs, and ideally this function call, in previous courses. If you are unfamiliar with its basic use, you will want to refresh yourself on this topic. [Add some online resources that go over basics of GLMs in R.] Fit a GLM to estimate the association between mean daily temperature (as the independent variable) and daily mortality count (as the dependent variable), first fitting a linear regression. (Since the mortality data are counts, we will want to shift to a different type of regression within the GLM framework, but this step allows you to develop a simple glm call, and to remember where to include the data and the independent and dependent variables within this function call.) Change your function call to fit a regression model in the Poisson family. Change your function call to allow for overdispersion in the outcome data (daily mortality count). How does the estimated coefficient for temperature change between the model fit for #2 and this model? Check both the central estimate and its estimated standard error. Change your function call to include control for day of week. Applied exercise: Example code Fit a GLM to estimate the association between mean daily temperature (as the independent variable) and daily mortality count (as the dependent variable), first fitting a linear regression. This is the model you are fitting: \\(Y_{t}=\\beta_{0}+\\beta_{1}X1_{t}+\\epsilon\\) where \\(Y_{t}\\) is the mortality count on day \\(t\\), \\(X1_{t}\\) is the mean temperature for day \\(t\\) and \\(\\epsilon\\) is the error term. Since this is a linear model we are assuming a Gaussian error distribution \\(\\epsilon \\sim {\\sf N}(0, \\sigma^{2})\\), where \\(\\sigma^{2}\\) is the variance not explained by the covariates (here just temperature). To do this, you will use the glm call. If you would like to save model fit results to use later, you assign the output a name as an R object (mod_linear_reg in the example code). If your study data are in a dataframe, you can specify these data in the glm call with the data parameter. Once you do this, you can use column names directly in the model formula. In the model formula, the dependent variable is specified first (all, the column for daily mortality counts for all ages, in this example), followed by a tilde (~), followed by all independent variables (only tmean in this example). If multiple independent variables are included, they are joined using +—we’ll see an example when we start adding control for confounders later. mod_linear_reg &lt;- glm(all ~ tmean, data = obs) Once you have fit a model and assigned it to an R object, you can explore it and use resulting values. First, the print method for a regression model gives some summary information. This method is automatically called if you enter the model object’s name at the console: mod_linear_reg ## ## Call: glm(formula = all ~ tmean, data = obs) ## ## Coefficients: ## (Intercept) tmean ## 187.647 -2.366 ## ## Degrees of Freedom: 8278 Total (i.e. Null); 8277 Residual ## Null Deviance: 8161000 ## Residual Deviance: 6766000 AIC: 79020 More information is printed if you run the summary method on the model object: summary(mod_linear_reg) ## ## Call: ## glm(formula = all ~ tmean, data = obs) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -77.301 -20.365 -1.605 17.502 169.280 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 187.64658 0.73557 255.10 &lt;2e-16 *** ## tmean -2.36555 0.05726 -41.31 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 817.4629) ## ## Null deviance: 8161196 on 8278 degrees of freedom ## Residual deviance: 6766140 on 8277 degrees of freedom ## AIC: 79019 ## ## Number of Fisher Scoring iterations: 2 Make sure you are familiar with the information provided from the model object, as well as how to interpret values like the coefficient estimates and their standard errors and p-values. These basic elements should have been covered in previous coursework (even if a different programming language was used to fit the model), and so we will not be covering them in great depth here, but instead focusing on some of the more advanced elements of how regression models are commonly fit to data from time series and case-crossover study designs in environmental epidemiology. For a refresher on the basics of fitting statistical models in R, you may want to check out Chapters 22 through 24 of Wickham and Grolemund (2016), a book that is available online. Finally, there are some newer tools for extracting information from model fit objects. The broom package extracts different elements from these objects and returns them in a “tidy” data format, which makes it much easier to use the output further in analysis with functions from the “tidyverse” suite of R packages. These tools are very popular and powerful, and so the broom tools can be very useful in working with output from regression modeling in R. The broom package includes three main functions for extracting data from regression model objects. First, the glance function returns overall data about the model fit, including the AIC and BIC: library(broom) glance(mod_linear_reg) ## # A tibble: 1 x 8 ## null.deviance df.null logLik AIC BIC deviance df.residual nobs ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 8161196. 8278 -39507. 79019. 79041. 6766140. 8277 8279 The tidy function returns data at the level of the model coefficients, including the estimate for each model parameter, its standard error, test statistic, and p-value. tidy(mod_linear_reg) ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 188. 0.736 255. 0 ## 2 tmean -2.37 0.0573 -41.3 0 Finally, the augment function returns data at the level of the original observations, including the fitted value for each observation, the residual between the fitted and true value, and some measures of influence on the model fit. augment(mod_linear_reg) ## # A tibble: 8,279 x 8 ## all tmean .fitted .resid .std.resid .hat .sigma .cooksd ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 220 3.91 178. 41.6 1.46 0.000359 28.6 0.000380 ## 2 257 5.55 175. 82.5 2.89 0.000268 28.6 0.00112 ## 3 245 4.39 177. 67.7 2.37 0.000330 28.6 0.000928 ## 4 226 5.43 175. 51.2 1.79 0.000274 28.6 0.000440 ## 5 236 6.87 171. 64.6 2.26 0.000211 28.6 0.000539 ## 6 235 9.23 166. 69.2 2.42 0.000144 28.6 0.000420 ## 7 231 6.69 172. 59.2 2.07 0.000218 28.6 0.000467 ## 8 235 7.96 169. 66.2 2.31 0.000174 28.6 0.000467 ## 9 250 7.27 170. 79.5 2.78 0.000197 28.6 0.000761 ## 10 214 9.51 165. 48.9 1.71 0.000139 28.6 0.000202 ## # … with 8,269 more rows One way you can use augment is to graph the fitted values for each observation after fitting the model: mod_linear_reg %&gt;% augment() %&gt;% ggplot(aes(x = tmean)) + geom_point(aes(y = all), alpha = 0.4, size = 0.5) + geom_line(aes(y = .fitted), color = &quot;red&quot;) + labs(x = &quot;Mean daily temperature&quot;, y = &quot;Log(Expected mortality count)&quot;) For more on the broom package, including some excellent examples of how it can be used to streamline complex regression analyses, see Robinson (2014). There is also a nice example of how it can be used in one of the chapters of Wickham and Grolemund (2016), available online at https://r4ds.had.co.nz/many-models.html. Change your function call to fit a regression model in the Poisson family. A linear regression is often not appropriate when fitting a model where the outcome variable provides counts, as with the example data. A Poisson regression is often preferred. For a count distribution were \\(Y \\sim {\\sf Poisson(\\mu)}\\) we typically fit a model such as \\(g(Y)=\\beta_{0}+\\beta_{1}X1\\), where \\(g()\\) represents the link function, in this case a log function so that \\(log(Y)=\\beta_{0}+\\beta_{1}X1\\). We can also express this as \\(Y=exp(\\beta_{0}+\\beta_{1}X1)\\). In the glm call, you can specify this with the family parameter, for which “poisson” is one choice. mod_pois_reg &lt;- glm(all ~ tmean, data = obs, family = &quot;poisson&quot;) One thing to keep in mind with this change is that the model now uses a non-identity link between the combination of independent variable(s) and the dependent variable. You will need to keep this in mind when you interpret the estimates of the regression coefficients. While the coefficient estimate for tmean from the linear regression could be interpreted as the expected increase in mortality counts for a one-unit (i.e., one degree Celsius) increase in temperature, now the estimated coefficient should be interpreted as the expected increase in the natural log-transform of mortality count for a one-unit increase in temperature. summary(mod_pois_reg) ## ## Call: ## glm(formula = all ~ tmean, family = &quot;poisson&quot;, data = obs) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -6.5945 -1.6365 -0.1167 1.3652 12.2221 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 5.2445409 0.0019704 2661.67 &lt;2e-16 *** ## tmean -0.0147728 0.0001583 -93.29 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 49297 on 8278 degrees of freedom ## Residual deviance: 40587 on 8277 degrees of freedom ## AIC: 97690 ## ## Number of Fisher Scoring iterations: 4 You can see this even more clearly if you take a look at the association between temperature for each observation and the expected mortality count fit by the model. First, if you look at the fitted values without transforming, they will still be in a state where mortality count is log-transformed. You can see by looking at the range of the y-scale that these values are for the log of expected mortality, rather than expected mortality, and that the fitted association is linear: mod_pois_reg %&gt;% augment() %&gt;% ggplot(aes(x = tmean)) + geom_point(aes(y = log(all)), alpha = 0.4, size = 0.5) + geom_line(aes(y = .fitted), color = &quot;red&quot;) + labs(x = &quot;Mean daily temperature&quot;, y = &quot;Log(Expected mortality count)&quot;) You can use exponentiation to transform the fitted values back to just be the expected mortality count based on the model fit. Once you make this transformation, you can see how the link in the Poisson family specification enforced a curved relationship between mean daily temperature and the untransformed expected mortality count. mod_pois_reg %&gt;% augment() %&gt;% ggplot(aes(x = tmean)) + geom_point(aes(y = all), alpha = 0.4, size = 0.5) + geom_line(aes(y = exp(.fitted)), color = &quot;red&quot;) + labs(x = &quot;Mean daily temperature&quot;, y = &quot;Expected mortality count&quot;) Change your function call to allow for overdispersion in the outcome data (daily mortality count). How does the estimated coefficient for temperature change between the model fit for #2 and this model? Check both the central estimate and its estimated standard error. In the R glm call, there is a family that is similar to Poisson (including using a log link), but that allows for overdispersion. You can specify it with the “quasipoisson” choice for the family parameter in the glm call: mod_ovdisp_reg &lt;- glm(all ~ tmean, data = obs, family = &quot;quasipoisson&quot;) When you use this family, there will be some new information in the summary for the model object. It will now include a dispersion parameter (\\(\\phi\\)). If this is close to 1, then the data were close to the assumed variance for a Poisson distribution (i.e., there was little evidence of overdispersion). In the example, the overdispersion is around 5, suggesting the data are overdispersed (this might come down some when we start including independent variables that explain some of the variation in the outcome variable, like long-term and seasonal trends). summary(mod_ovdisp_reg) ## ## Call: ## glm(formula = all ~ tmean, family = &quot;quasipoisson&quot;, data = obs) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -6.5945 -1.6365 -0.1167 1.3652 12.2221 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.2445409 0.0044087 1189.6 &lt;2e-16 *** ## tmean -0.0147728 0.0003543 -41.7 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for quasipoisson family taken to be 5.006304) ## ## Null deviance: 49297 on 8278 degrees of freedom ## Residual deviance: 40587 on 8277 degrees of freedom ## AIC: NA ## ## Number of Fisher Scoring iterations: 4 If you compare the estimates of the temperature coefficient from the Poisson regression with those when you allow for overdispersion, you’ll see something interesting: tidy(mod_pois_reg) %&gt;% filter(term == &quot;tmean&quot;) ## # A tibble: 1 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 tmean -0.0148 0.000158 -93.3 0 tidy(mod_ovdisp_reg) %&gt;% filter(term == &quot;tmean&quot;) ## # A tibble: 1 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 tmean -0.0148 0.000354 -41.7 0 The central estimate (estimate column) is very similar. However, the estimated standard error is larger when the model allows for overdispersion. This indicates that the Poisson model was too simple, and that its inherent assumption that data were not overdispersed was problematic. If you naively used a Poisson regression in this case, then you would estimate a confidence interval on the temperature coefficient that would be too narrow. This could cause you to conclude that the estimate was statistically significant when you should not have (although in this case, the estimate is statistically significant under both models). Change your function call to include control for day of week. Day of week is included in the data as a categorical variable, using a data type in R called a factor. You are now essentially fitting this model: \\(log(Y)=\\beta_{0}+\\beta_{1}X1+\\gamma^{&#39;}X2\\), where \\(X2\\) is a categorical variable for day of the week and \\(\\gamma^{&#39;}\\) represents a vector of parameters associated with each category. It is pretty straightforward to include factors as independent variables in calls to glm: you just add the column name to the list of other independent variables with a +. In this case, we need to do one more step: earlier, we added order to dow, so it would “remember” the order of the week days (Monday before Tuesday, etc.). However, we need to strip off this order before we include the factor in the glm call. One way to do this is with the factor call, specifying ordered = FALSE. Here is the full call to fit this model: mod_ctrl_dow &lt;- glm(all ~ tmean + factor(dow, ordered = FALSE), data = obs, family = &quot;quasipoisson&quot;) When you look at the summary for the model object, you can see that the model has fit a separate model parameter for six of the seven weekdays. The one weekday that isn’t fit (Sunday in this case) serves as a baseline —these estimates specify how the log of the expected mortality count is expected to differ on, for example, Monday versus Sunday (by about 0.03), if the temperature is the same for the two days. summary(mod_ctrl_dow) ## ## Call: ## glm(formula = all ~ tmean + factor(dow, ordered = FALSE), family = &quot;quasipoisson&quot;, ## data = obs) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -6.3211 -1.6476 -0.1313 1.3549 12.5286 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.2208502 0.0065277 799.804 &lt; 2e-16 *** ## tmean -0.0147723 0.0003538 -41.750 &lt; 2e-16 *** ## factor(dow, ordered = FALSE)Mon 0.0299282 0.0072910 4.105 4.08e-05 *** ## factor(dow, ordered = FALSE)Tue 0.0292575 0.0072920 4.012 6.07e-05 *** ## factor(dow, ordered = FALSE)Wed 0.0255224 0.0073020 3.495 0.000476 *** ## factor(dow, ordered = FALSE)Thu 0.0269580 0.0072985 3.694 0.000222 *** ## factor(dow, ordered = FALSE)Fri 0.0355431 0.0072834 4.880 1.08e-06 *** ## factor(dow, ordered = FALSE)Sat 0.0181489 0.0073158 2.481 0.013129 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for quasipoisson family taken to be 4.992004) ## ## Null deviance: 49297 on 8278 degrees of freedom ## Residual deviance: 40434 on 8271 degrees of freedom ## AIC: NA ## ## Number of Fisher Scoring iterations: 4 You can also see from this summary that the coefficients for the day of the week are all statistically significant. Even though we didn’t see a big difference in mortality counts by day of week in our exploratory analysis, this suggests that it does help explain some variance in mortality observations and will likely be worth including in the final model. The model now includes day of week when fitting an expected mortality count for each observation. As a result, if you plot fitted values of expected mortality versus mean daily temperature, you’ll see some “hoppiness” in the fitted line: mod_ctrl_dow %&gt;% augment() %&gt;% ggplot(aes(x = tmean)) + geom_point(aes(y = all), alpha = 0.4, size = 0.5) + geom_line(aes(y = exp(.fitted)), color = &quot;red&quot;) + labs(x = &quot;Mean daily temperature&quot;, y = &quot;Expected mortality count&quot;) This is because each fitted value is also incorporating the expected influence of day of week on the mortality count, and that varies across the observations (i.e., you could have two days with the same temperature, but different expected mortality from the model, because they occur on different days). If you plot the model fits separately for each day of the week, you’ll see that the line is smooth across all observations from the same day of the week: mod_ctrl_dow %&gt;% augment() %&gt;% ggplot(aes(x = tmean)) + geom_point(aes(y = all), alpha = 0.4, size = 0.5) + geom_line(aes(y = exp(.fitted)), color = &quot;red&quot;) + labs(x = &quot;Mean daily temperature&quot;, y = &quot;Expected mortality count&quot;) + facet_wrap(~ obs$dow) Wrapping up At this point, the coefficient estimates suggests that risk of mortality tends to decrease as temperature increases. Do you think this is reasonable? What else might be important to build into the model based on your analysis up to this point? 3.5 Chapter vocabulary Each class will start with a vocabulary quiz on a select number of the words from the chapter’s vocabulary list. The vocabulary words for this chapter are: time-series study design case-crossover study design exposure health outcome confounder study period seasonal trends long-term trends error distribution generalized linear model (GLM) link function overdispersed categorical variable spline natural cubic spline distributed lag cross-basis term References "],["generalized-linear-models.html", "Chapter 4 Generalized linear models 4.1 Splines in GLMs 4.2 Cross-basis functions in GLMs 4.3 Chapter vocabulary", " Chapter 4 Generalized linear models The readings for this chapter are the same as for the last chapter: Vicedo-Cabrera, Sera, and Gasparrini (2019), with supplemental material available to download by clicking http://links.lww.com/EDE/B504 B. G. Armstrong, Gasparrini, and Tobias (2014), with supplemental material available at https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/1471-2288-14-122#Sec13 4.1 Splines in GLMs We saw from the latest model with a linear for mean daily temperature that the suggested effect on mortality is a decrease in daily mortality counts with increasing temperature. However, as you’ve probably guessed that’s probably not entirely accurate. A linear term for the effect of exposure restricts us to an effect that can be fitted with a straight line (either a null effect or a monotonically increasing or decreasing effect with increasing exposure). This clearly is problematic in some cases. One example is when exploring the association between temperature and health risk. Based on human physiology, we would expect many health risks to be elevated at temperature extremes, whether those are extreme cold or extreme heat. A linear term would be inadequate to describe this kind of U-shaped association. Other effects might have a threshold—for example, heat stroke might have a very low risk at most temperatures, only increasing with temperature above a certain threshold. [Maybe add images of these kinds of associations?] We can capture non-linear patterns in effects, by using different functions of X. Examples are \\(\\sqrt{X}\\), \\(X^{2}\\), or more complex smoothing functions, such as polynomials or splines. Polynomials might at first make a lot of sense, especially since you’ve likely come across polynomial terms in mathematics classes since grade school. However, it turns out that they have some undesirable properties. A key one is that they can have extreme behavior, particularly when using a high-order polynomial, and particularly outside the range of data that are available to fit the model. An alternative that is generally preferred for environmental epidemiology studies is the regression spline. Regression splines are simple parametric smoothing function, which fit separate polynomial in each interval of the range of the predictor; these can be linear, quadratic, and cubic. An example of a (in this case cubic) spline function is \\(X+X^{2}+X^{3}+I((X&gt;X_{0})*(X-X_{0})^3)\\).This particular function is a cubic spline with four degrees of freedom (\\(df=4\\)) and one not (\\(X_{0}\\)). A special type of spline called a natural cubic spline is particularly popular. Unlike a polynomial function, a natural cubic spline “behaves” better outside the range of the data used to fit the model—they are constrained to continue on a [linear?] trajectory once they pass beyond the range of the data. [Maybe add more / clarify / add references on the point of why splines versus polynomials.] Regression splines can be fit in a GLM via the package splines. Two commonly used examples of regression splines are b-splines and natural cubic splines. Vicedo-Cabrera, Sera, and Gasparrini (2019) uses natural cubic splines. Applied: Including a spline in a GLM For this exercise, you will continue to build up the model that you began in the examples in the previous chapter. The example uses the data provided with one of this chapter’s readings, Vicedo-Cabrera, Sera, and Gasparrini (2019). Start by fitting a somewhat simple model—how are daily mortality counts associated with (a) a linear and (b) a non-linear function of time? Is a linear term appropriate to describe this association? What types of patterns are captured by a non-linear function that are missed by a linear function? In the last chapter, the final version of the model used a GLM with an overdispersed Poisson distribution, including control for day of week. Start from this model and add control for long-term and seasonal trends over the study period. Refine your model to fit for a non-linear, rather than linear, function of temperature in the model. Does a non-linear term seem to be more appropriate than a linear term? Applied exercise: Example code Start by fitting a somewhat simple model—how are daily mortality counts associated with (a) a linear and (b) a non-linear function of time? It is helpful to start by loading the R packages you are likely to need, as well as the example dataset. You may also need to re-load the example data and perform the steps taken to clean it in the last chapter: # Load some packages that will likely be useful library(tidyverse) library(viridis) library(lubridate) library(broom) # Load and clean the data obs &lt;- read_csv(&quot;data/lndn_obs.csv&quot;) %&gt;% mutate(dow = wday(date, label = TRUE)) For this first question, the aim is to model the association between time and daily mortality counts within the example data. This approach is often used to explore and, if needed, adjust for temporal factors in the data. There are a number of factors that can act over time to create patterns in both environmental exposures and health outcomes. For example, there may be changes in air pollution exposures over the years of a study because of changes in regulations or growth or decline of factories and automobile traffic in an area. Changes in health care and in population demographics can cause patterns in health outcomes over the study period. At a shorter, seasonal term, there are also factors that could influence both exposures and outcomes, including seasonal changes in climate, seasonal changes in emissions, and seasonal patterns in health outcomes. It can be difficult to pinpoint and measure these temporal factors, and so instead a common practice is to include model control based on the time in the study. This can be measured, for example, as the day since the start of the study period. You can easily add a column for day in study for a dataset that includes date. R saves dates in a special format, which we’re using the in obs dataset: class(obs$date) ## [1] &quot;Date&quot; However, this is just a fancy overlay on a value that’s ultimately saved as a number. Like most Unix programs, the date is saved as the number of days since the Unix “epoch,” January 1, 1970. You can take advantage of this convention—if you use as.numeric around a date in R, it will give you a number that gets one unit higher for every new date. Here’s the example for the first date in our example data: obs$date[1] ## [1] &quot;1990-01-01&quot; as.numeric(obs$date[1]) ## [1] 7305 And here’s the example for the next date: obs$date[2] ## [1] &quot;1990-01-02&quot; as.numeric(obs$date[2]) ## [1] 7306 You can use this convention to add a column that gives days since the first study date. While you could also use the 1:n() call to get a number for each row that goes from 1 to the number of rows, that approach would not catch any “skips” in dates in the data (e.g., missing dates if only warm-season data are included). The use of the dates is more robust: obs &lt;- obs %&gt;% mutate(time = as.numeric(date) - first(as.numeric(date))) obs %&gt;% select(date, time) ## # A tibble: 8,279 x 2 ## date time ## &lt;date&gt; &lt;dbl&gt; ## 1 1990-01-01 0 ## 2 1990-01-02 1 ## 3 1990-01-03 2 ## 4 1990-01-04 3 ## 5 1990-01-05 4 ## 6 1990-01-06 5 ## 7 1990-01-07 6 ## 8 1990-01-08 7 ## 9 1990-01-09 8 ## 10 1990-01-10 9 ## # … with 8,269 more rows As a next step, it is always useful to use exploratory data analysis to look at the patterns that might exist for an association, before you start designing and fitting the regression model. ggplot(obs, aes(x = time, y = all)) + geom_point(size = 0.5, alpha = 0.5) There are clear patterns between time and daily mortality counts in these data. First, there is a clear long-term pattern, with mortality rates declining on average over time. Second, there are clear seasonal patterns, with higher mortality generally in the winter and lower rates in the summer. To model this, we can start with fitting a linear term. In the last chapter, we determined that the mortality outcome data can be fit using a GLM with a Poisson family, allowing for overdispersion as it is common in real-life count data like these. To include time as a linear term, we can just include that column name to the right of the ~ in the model formula: mod_time &lt;- glm(all ~ time, data = obs, family = &quot;quasipoisson&quot;) You can use the augment function from the broom package to pull out the fitted estimate for each of the original observations and plot that, along with the observed data, to get an idea of what this model has captured: mod_time %&gt;% augment() %&gt;% ggplot(aes(x = time)) + geom_point(aes(y = all), alpha = 0.4, size = 0.5) + geom_line(aes(y = exp(.fitted)), color = &quot;red&quot;) + labs(x = &quot;Date in study&quot;, y = &quot;Expected mortality count&quot;) [Check for termplot] This linear trend captures the long-term trend in mortality rates fairly well in this case. This won’t always be the case, as there may be some health outcomes—or some study populations—where the long-term pattern over the study period might be less linear than in this example. Further, the linear term is completely unsuccessful in capturing the shorter-term trends in mortality rate. These oscillate, and so would be impossible to capture over multiple years with a linear trend. Instead, it’s helpful to use a non-linear term for time in the model. We can use a natural cubic spline for this, using the ns function from the splines package. You will need to clarify how flexible the spline function should be, and this can be specified through the degrees of freedom for the spline. A spline with more degrees of freedom will be “wigglier” over a given data range compared to a spline with fewer degrees of freedom. Let’s start by using 158 degrees of freedom, which translates to about 7 degrees of freedom per year: library(splines) mod_time_nonlin &lt;- glm(all ~ ns(time, df = 158), data = obs, family = &quot;quasipoisson&quot;) You can visualize the model results in a similar way to how we visualized the last model. However, there is one extra step. The augment function only carries through columns in the original data (obs) that were directly used in fitting the model. Now that we’re using a transformation of the time column, by wrapping it in ns, the time column is no longer included in the augment output. However, we can easily add it back in using mutate, pulling it from the original obs dataset, and then proceed as before. mod_time_nonlin %&gt;% augment() %&gt;% mutate(time = obs$time) %&gt;% ggplot(aes(x = time)) + geom_point(aes(y = all), alpha = 0.4, size = 0.5) + geom_line(aes(y = exp(.fitted)), color = &quot;red&quot;) + labs(x = &quot;Date in study&quot;, y = &quot;Expected mortality count&quot;) The non-linear term for time has allowed enough flexibility that the model now captures both long-term and seasonal trends in the data. [More on how to pick a good d.f. for an env. epi. model like this]. In practice, researchers often using about 6–8 degrees of freedom per year of the study, in the case of year-round data. You can explore how changing the degrees of freedom changes the way the model fits to the observed data. As you use more degrees of freedom, the line will capture very short-term effects, and may start to interfere with the shorter-term associations between environmental exposures and health risk that you are trying to capture. Even in the example model we just fit, for example, it looks like the control for time may be capturing some patterns that were likely caused by heatwaves (the rare summer peaks, including one from the 1995 heatwave). Conversely, if too few degrees of freedom are used, the model will shift to look much more like the linear model, with inadequate control for seasonal patterns. # A model with many less d.f. for the time spline mod_time_nonlin_lowdf &lt;- glm(all ~ ns(time, df = 10), data = obs, family = &quot;quasipoisson&quot;) mod_time_nonlin_lowdf %&gt;% augment() %&gt;% mutate(time = obs$time) %&gt;% ggplot(aes(x = time)) + geom_point(aes(y = all), alpha = 0.4, size = 0.5) + geom_line(aes(y = exp(.fitted)), color = &quot;red&quot;) + labs(x = &quot;Date in study&quot;, y = &quot;Expected mortality count&quot;) # A model with many more d.f. for the time spline # (Takes a little while to run) mod_time_nonlin_highdf &lt;- glm(all ~ ns(time, df = 400), data = obs, family = &quot;quasipoisson&quot;) mod_time_nonlin_highdf %&gt;% augment() %&gt;% mutate(time = obs$time) %&gt;% ggplot(aes(x = time)) + geom_point(aes(y = all), alpha = 0.4, size = 0.5) + geom_line(aes(y = exp(.fitted)), color = &quot;red&quot;) + labs(x = &quot;Date in study&quot;, y = &quot;Expected mortality count&quot;) In all cases, when you fit a non-linear function of an explanatory variable, it will make the model summary results look much more complicated, e.g.: mod_time_nonlin_lowdf %&gt;% tidy() ## # A tibble: 11 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 5.26 0.00948 555. 0. ## 2 ns(time, df = 10)1 -0.0260 0.0119 -2.18 2.93e- 2 ## 3 ns(time, df = 10)2 -0.0860 0.0155 -5.56 2.85e- 8 ## 4 ns(time, df = 10)3 -0.114 0.0139 -8.15 4.01e- 16 ## 5 ns(time, df = 10)4 -0.196 0.0151 -13.0 4.47e- 38 ## 6 ns(time, df = 10)5 -0.187 0.0148 -12.6 2.80e- 36 ## 7 ns(time, df = 10)6 -0.315 0.0154 -20.5 5.62e- 91 ## 8 ns(time, df = 10)7 -0.337 0.0154 -21.9 1.95e-103 ## 9 ns(time, df = 10)8 -0.358 0.0135 -26.5 1.56e-148 ## 10 ns(time, df = 10)9 -0.467 0.0244 -19.2 4.49e- 80 ## 11 ns(time, df = 10)10 -0.392 0.0126 -31.2 8.01e-202 You can see that there are multiple model coefficients for the variable fit using a spline function, one less than the number of degrees of freedom. These model coefficients are very hard to interpret on their own. When we are using the spline to control for a factor that might serve as a confounder of the association of interest, we typically won’t need to try to interpret these model coefficients—instead, we are interested in accounting for how this factor explains variability in the outcome, without needing to quantify the association as a key result. However, there are also cases where we want to use a spline to fit the association with the exposure that we are interested in. In this case, we will want to be able to interpret model coefficients from the spline. Later in this chapter, we will introduce the dlnm package, which includes functions to both fit and interpret natural cubic splines within GLMs for environmental epidemiology. Start from the last model created in the last chapter and add control for long-term and seasonal trends over the study period. The last model fit in the last chapter was the following, which fits for the association between a linear term of temperature and mortality risk, with control for day of week: mod_ctrl_dow &lt;- glm(all ~ tmean + factor(dow, ordered = FALSE), data = obs, family = &quot;quasipoisson&quot;) To add control for long-term and seasonal trends, you can take the natural cubic spline function of temperature that you just fit and include it among the explanatory / independent variables from the model in the last chapter. If you want to control for only long-term trends, a linear term of the time column could work, as we discovered in the first part of this chapter’s exercise. However, seasonal trends could certainly confound the association of interest. Mortality rates have a clear seasonal pattern, and temperature does as well, and these patterns create the potential for confounding when we look at how temperature and mortality risk are associated, beyond any seasonally-driven pathways. mod_ctrl_dow_time &lt;- glm(all ~ tmean + factor(dow, ordered = FALSE) + ns(time, df = 158), data = obs, family = &quot;quasipoisson&quot;) You can see the influence of this seasonal confounding if you look at the model results. When we look at the results from the model that did not control for long-term and seasonal trends, we get an estimate that mortality rates tend to be lower on days with higher temperature, with a negative term for tmean: mod_ctrl_dow %&gt;% tidy() %&gt;% filter(term == &quot;tmean&quot;) ## # A tibble: 1 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 tmean -0.0148 0.000354 -41.7 0 Conversely, when we include control for long-term and seasonal trends, the estimated association between mortality rates and temperature is reversed, estimating increased mortality rates on days with higher temperature, controlling for long-term and seasonal trends: mod_ctrl_dow_time %&gt;% tidy() %&gt;% filter(term == &quot;tmean&quot;) ## # A tibble: 1 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 tmean 0.00370 0.000395 9.36 1.02e-20 Refine your model to fit for a non-linear, rather than linear, function of temperature in the model. You can use a spline in the same way to fit a non-linear function for the exposure of interest in the model (temperature). We’ll start there. However, as mentioned earlier, it’s a bit tricky to interpret the coefficients from the fit model—you no longer generate a single coefficient for the exposure of interest, but instead several related to the spline. Therefore, once we show how to fit using ns directly, we’ll show how you can do the same thing using specialized functions in the dlnm package. This package includes a lot of nice functions for not only fitting an association using a non-linear term, but also for interpreting the results after the model is fit. First, here is code that can be used to fit the model using ns directly, similarly to the approach we used to control for temporal patterns with a flexible function: mod_ctrl_nl_temp &lt;- glm(all ~ ns(tmean, 4) + factor(dow, ordered = FALSE) + ns(time, df = 158), data = obs, family = &quot;quasipoisson&quot;) mod_time_nonlin_highdf %&gt;% augment() %&gt;% mutate(tmean = obs$tmean) %&gt;% ggplot(aes(x = tmean)) + geom_point(aes(y = all), alpha = 0.4, size = 0.5) + geom_point(aes(y = exp(.fitted)), color = &quot;red&quot;, size = 0.4) + labs(x = &quot;Daily mean temperature&quot;, y = &quot;Expected mortality count&quot;) 4.2 Cross-basis functions in GLMs [Using a cross-basis to model an exposure’s association with the outcome in two dimensions (dimensions of time and exposure level)] 4.3 Chapter vocabulary Each class will start with a vocabulary quiz on a select number of the words from the chapter’s vocabulary list. The vocabulary words for this chapter are: References "],["natural-experiments.html", "Chapter 5 Natural experiments 5.1 Interrupted time series 5.2 Difference-in-differences", " Chapter 5 Natural experiments The readings for this chapter are: Bernal, Cummins, and Gasparrini (2017) (on interrupted time series), with a correction to an equation in the paper at https://academic.oup.com/ije/article/49/4/1414/5900884. Example data and R code for the paper are available to download through a Supplemental Appendix. Barone-Adesi et al. (2011), the scientific paper highlighted as an example in the tutorial in the previous reading Bor et al. (2014) (on interrupted time series) Casey et al. (2018) (on difference-in-differences) Mendola (2018), an Invited Commentary on the previous reading 5.1 Interrupted time series [Interrupted time series assessing effects of policy/intervention in specific point in time] # Load some packages that will likely be useful library(tidyverse) library(viridis) library(lubridate) library(broom) # Load and clean the data obs &lt;- read_csv(&quot;data/lndn_obs.csv&quot;) %&gt;% mutate(dow = wday(date, label = TRUE)) %&gt;% mutate(time = as.numeric(date) - first(as.numeric(date))) london_summer_2012 &lt;- obs %&gt;% filter(ymd(&quot;2012-06-01&quot;) &lt;= date &amp; date &lt;= ymd(&quot;2012-09-30&quot;)) london_olympic_dates &lt;- tibble(date = ymd(c(&quot;2012-07-27&quot;, &quot;2012-08-12&quot;))) ggplot() + geom_polygon(aes(x = ymd(c(&quot;2012-07-27&quot;, &quot;2012-08-12&quot;, &quot;2012-08-12&quot;, &quot;2012-07-27&quot;)), y = c(Inf, Inf, -Inf, -Inf)), fill = &quot;cyan&quot;, alpha = 0.2) + geom_line(data = london_summer_2012, aes(x = date, y = tmean)) + labs(x = &quot;Date&quot;, y = &quot;Mean daily temperature&quot;) Example data from Bernal, Cummins, and Gasparrini (2017): sicily &lt;- read_csv(&quot;data/sicily.csv&quot;) %&gt;% mutate(date = paste(year, month, &quot;15&quot;), # Use middle of the month for plotting date = ymd(date)) Identify dates of the smoking band: sicily %&gt;% group_by(smokban) %&gt;% slice(c(1, n())) ## # A tibble: 4 x 8 ## # Groups: smokban [2] ## year month aces time smokban pop stdpop date ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt; ## 1 2002 1 728 1 0 364277. 379875. 2002-01-15 ## 2 2004 12 886 36 0 364700. 383428. 2004-12-15 ## 3 2005 1 831 37 1 364421. 388153. 2005-01-15 ## 4 2006 11 912 59 1 363833. 390712. 2006-11-15 Recreate Figure 1 from Bernal, Cummins, and Gasparrini (2017): ggplot() + geom_polygon(aes(x = ymd(c(&quot;2005-01-01&quot;, &quot;2006-11-30&quot;, &quot;2006-11-30&quot;, &quot;2005-01-01&quot;)), y = c(Inf, Inf, -Inf, -Inf)), fill = &quot;lightgray&quot;) + geom_point(data = sicily, aes(x = date, y = 10000 * 10 * aces / stdpop), shape = 21) + geom_smooth(data = sicily, aes(x = date, y = 10000 * 10 * aces / stdpop), # Need the extra 10 to line up with Figure in paper---figure out why method = &quot;lm&quot;, se = FALSE, color = &quot;red&quot;, linetype = 3) + labs(x = &quot;Year&quot;, y = &quot;Std rate x 10 000&quot;) + theme_classic() 5.2 Difference-in-differences [Difference-in differences application for intervention introduced in one point in time] References "],["risk-assessment.html", "Chapter 6 Risk assessment", " Chapter 6 Risk assessment [Predict expected heat-related mortality under a climate change scenario] "],["longitudinal-cohort-study-designs.html", "Chapter 7 Longitudinal cohort study designs 7.1 Longitudinal cohort data 7.2 Coding a survival analysis 7.3 Handling complexity", " Chapter 7 Longitudinal cohort study designs The readings for this chapter are Andersson et al. (2019) Wong et al. (1989) The following are a series of instructional papers on survival analysis, that are meant as general background on how to fit survival analysis models. Clark et al. (2003) Bradburn et al. (2003a) Bradburn et al. (2003b) 7.1 Longitudinal cohort data Example datasets are available online, but also made available to you on the course website. For the Framingham Heart Study the example data are available as the file “frmgham2.csv.” It is saved in a csv format, and so they can be read into R using the read_csv function from the readr package (part of the tidyverse). You can use the following code to read in these data, assuming you have saved them in a “data” subdirectory of your current working directory: library(tidyverse) # Loads all the tidyverse packages, including readr fhs &lt;- read_csv(&quot;data/frmgham2.csv&quot;) fhs ## # A tibble: 11,627 x 39 ## RANDID SEX TOTCHOL AGE SYSBP DIABP CURSMOKE CIGPDAY BMI DIABETES BPMEDS ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2448 1 195 39 106 70 0 0 27.0 0 0 ## 2 2448 1 209 52 121 66 0 0 NA 0 0 ## 3 6238 2 250 46 121 81 0 0 28.7 0 0 ## 4 6238 2 260 52 105 69.5 0 0 29.4 0 0 ## 5 6238 2 237 58 108 66 0 0 28.5 0 0 ## 6 9428 1 245 48 128. 80 1 20 25.3 0 0 ## 7 9428 1 283 54 141 89 1 30 25.3 0 0 ## 8 10552 2 225 61 150 95 1 30 28.6 0 0 ## 9 10552 2 232 67 183 109 1 20 30.2 0 0 ## 10 11252 2 285 46 130 84 1 23 23.1 0 0 ## # … with 11,617 more rows, and 28 more variables: HEARTRTE &lt;dbl&gt;, ## # GLUCOSE &lt;dbl&gt;, educ &lt;dbl&gt;, PREVCHD &lt;dbl&gt;, PREVAP &lt;dbl&gt;, PREVMI &lt;dbl&gt;, ## # PREVSTRK &lt;dbl&gt;, PREVHYP &lt;dbl&gt;, TIME &lt;dbl&gt;, PERIOD &lt;dbl&gt;, HDLC &lt;dbl&gt;, ## # LDLC &lt;dbl&gt;, DEATH &lt;dbl&gt;, ANGINA &lt;dbl&gt;, HOSPMI &lt;dbl&gt;, MI_FCHD &lt;dbl&gt;, ## # ANYCHD &lt;dbl&gt;, STROKE &lt;dbl&gt;, CVD &lt;dbl&gt;, HYPERTEN &lt;dbl&gt;, TIMEAP &lt;dbl&gt;, ## # TIMEMI &lt;dbl&gt;, TIMEMIFC &lt;dbl&gt;, TIMECHD &lt;dbl&gt;, TIMESTRK &lt;dbl&gt;, TIMECVD &lt;dbl&gt;, ## # TIMEDTH &lt;dbl&gt;, TIMEHYP &lt;dbl&gt; One important difference compared to a time-series dataset is the RANDID variable. This is the unique identifier for unit for which we have repeated observations for over time. In this case the RANDID variable represents a unique identifier for each study participant, with multiple observations (rows) per participant over time. The TIME variable indicates the number of days that have ellapsed since beginning of follow-up of each observation. (TIME=0 for the first observation of each participant). Number of observations varies between participants (typical) The time spacing between observations is not constant. This is because the repeated observations in the Framingham Heart Study are the result of follow-up exams happening 3 to 5 years apart. Many longitudinal cohorts will instead have observations over a fixed time interval (monthly, annual, biannual etc), resulting in a more balanced dataset. Observations are given for various risk factors, covariates and cardiovascular outcomes. Some will be invariant for each participant over time (SEX, educ), while others will vary with each exam. From a data management perspective, we might want to change all the column names to be in lowercase, rather than uppercase. This will save our pinkies some work as we code with the data! You can make that change with the following code, using the str_to_lower function from the stringr package (part of the tidyverse): fhs &lt;- fhs %&gt;% rename_all(.funs = str_to_lower) fhs ## # A tibble: 11,627 x 39 ## randid sex totchol age sysbp diabp cursmoke cigpday bmi diabetes bpmeds ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2448 1 195 39 106 70 0 0 27.0 0 0 ## 2 2448 1 209 52 121 66 0 0 NA 0 0 ## 3 6238 2 250 46 121 81 0 0 28.7 0 0 ## 4 6238 2 260 52 105 69.5 0 0 29.4 0 0 ## 5 6238 2 237 58 108 66 0 0 28.5 0 0 ## 6 9428 1 245 48 128. 80 1 20 25.3 0 0 ## 7 9428 1 283 54 141 89 1 30 25.3 0 0 ## 8 10552 2 225 61 150 95 1 30 28.6 0 0 ## 9 10552 2 232 67 183 109 1 20 30.2 0 0 ## 10 11252 2 285 46 130 84 1 23 23.1 0 0 ## # … with 11,617 more rows, and 28 more variables: heartrte &lt;dbl&gt;, ## # glucose &lt;dbl&gt;, educ &lt;dbl&gt;, prevchd &lt;dbl&gt;, prevap &lt;dbl&gt;, prevmi &lt;dbl&gt;, ## # prevstrk &lt;dbl&gt;, prevhyp &lt;dbl&gt;, time &lt;dbl&gt;, period &lt;dbl&gt;, hdlc &lt;dbl&gt;, ## # ldlc &lt;dbl&gt;, death &lt;dbl&gt;, angina &lt;dbl&gt;, hospmi &lt;dbl&gt;, mi_fchd &lt;dbl&gt;, ## # anychd &lt;dbl&gt;, stroke &lt;dbl&gt;, cvd &lt;dbl&gt;, hyperten &lt;dbl&gt;, timeap &lt;dbl&gt;, ## # timemi &lt;dbl&gt;, timemifc &lt;dbl&gt;, timechd &lt;dbl&gt;, timestrk &lt;dbl&gt;, timecvd &lt;dbl&gt;, ## # timedth &lt;dbl&gt;, timehyp &lt;dbl&gt; Applied exercise: Exploring longitudinal cohort data Read the example cohort data in R and explore it to answer the following questions: What is the number of participants and number of observations in the fhs dataset? Is there any missingness in the data? How many participants die? What is the distribution of age at time of death? What is the distribution of age at time of incident MI? Are there differences between males and females? Are there differences in smoking between males and females? What is the distribution of BMI among MI cases and non-cases? How about between smokers and non-smokers Based on this exploratory exercise in this section, talk about the potential for confounding when these data are analyzed to estimate the association between smoking and risk of incident MI. Applied exercise: Example code What is the number of participants and the number of observations in the fhs dataset? (i.e what is the sample size and number of person-time observations) In the fhs dataset, the number of participants will be equal to the number of unique ID’s (The RANDID variable which takes a unique value for each participant). We can extract this using the unique function nested within the length function length(unique(fhs$randid)) ## [1] 4434 If you’d like to use tidyverse tools to answer this question, you can do that, as well. The pipe operator (%&gt;%) works on any type of object—it will take your current output and include it as the first parameter value for the function call you pipe into. If you want to perform operations on a column of a dataframe, you can use pull to extract it from the dataframe as a vector, and then pipe that into vector operations: fhs %&gt;% pull(randid) %&gt;% unique() %&gt;% length() ## [1] 4434 It’s entirely a personal choice whether you use the $ operator and “nesting” of function calls, versus pull and piping to do a series of function calls. You can see you get the same result, so it just comes down to the style that you will find easiest to understand when you look at your code later. The number of person-time observations will actually be equal to the length of the dataset. The dim function gives us the length (number of rows) and width (number of columns) for a dataframe or any matrix like object in R. dim(fhs) ## [1] 11627 39 We see that there is approximately an average of 2 to 3 observations per participants. When you know there are repeated measurements, it can be helpful to explore how much variation there is in the number of observations per study subject. You could do that in this dataset with the following code: fhs %&gt;% # Group by the study subject identifier and then count the rows for each group_by(randid) %&gt;% count() %&gt;% # Reorder the dataset so the subjects with the most observations come first arrange(desc(n)) %&gt;% head() ## # A tibble: 6 x 2 ## # Groups: randid [6] ## randid n ## &lt;dbl&gt; &lt;int&gt; ## 1 6238 3 ## 2 11252 3 ## 3 11263 3 ## 4 12806 3 ## 5 14367 3 ## 6 16365 3 You can visualize this, as well. A histogram is one good choice: fhs %&gt;% # Group by the study subject identifier and then count the rows for each group_by(randid) %&gt;% count() %&gt;% ggplot(aes(x = n)) + geom_histogram() All study subjects have between one and three measurements. Most of the study subjects (over 3,000) have three measurements recorded in the dataset. Is there any missingness in the data? We can check for missingness in a number of ways. There are a couple of great packages, visdat and naniar, that include functions for investigating missingness in a dataset. If you don’t have these installed, you can install them using install.packages(\"naniar\") and install.packages(\"visdat\"). The naniar package has a vignette with examples that is a nice starting point for working with both packages. The vis_miss function shows missingness in a dataset in a way that lets you get a top-level snapshot: library(visdat) vis_miss(fhs) Another was to visualize this is with gg_miss_var: library(naniar) gg_miss_var(fhs) Many of the variables are available for all observations, with no missingness, including records of the subject’s ID, measures of death, stroke, CVD, and other events, age, sex, and BMI. Some of the measured values from visits are missing occasionally, like the total cholesterol, and glucose. Other measures asked of the participants (number of cigarettes per day, education) are occasionally missing. Two of the variables—hdlc and ldlc—are missing more often than they are available. You can also do faceting with the gg_miss_var function. For example, you could see if missingness varies by the period of the observation: gg_miss_var(fhs, facet = period) You may also want to check if missingness varies with whether an observation was associated with death of the study subject: gg_miss_var(fhs, facet = death) There are also functions in these packages that allow you to look at how missingness is related across variables. For example, both glucose and totchol are continuous variables, and both are occasionally missing. You can use the geom function geom_miss_point from the nanair package with a ggplot object to explore patterns of missingness among these two variables: fhs %&gt;% ggplot(aes(x = glucose, y = totchol)) + geom_miss_point() The lower left corner shows the observations where both values are missing—it looks like there aren’t too many. For observations with one missing but not the other (the points in red along the x- and y-axes), it looks like the distribution across the non-missing variable is pretty similar to that for observations with both measurements avaiable. In other words, totchol has a similar distribution among observations where glucose is available as observations where glucose is missing. You can also do things like facet by sex to explore patterns at a finer level: fhs %&gt;% ggplot(aes(x = glucose, y = totchol)) + geom_miss_point() + facet_wrap(~ sex) How many participants die? What is the distribution of age at time of death? The death variable in the fhs data is an indicator for mortality if a participant died at any point during follow-up. It is time-invariant taking the value 1 if a participant died at any point or 0 if they were alive at their end of follow-up, so we have to be careful on how to extract the actual number of deaths. If you arrange by the random ID and look at period and death for each subject, you can see that the death variable is the same for all periods for each subject: fhs %&gt;% arrange(randid) %&gt;% select(randid, period, death) ## # A tibble: 11,627 x 3 ## randid period death ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2448 1 0 ## 2 2448 3 0 ## 3 6238 1 0 ## 4 6238 2 0 ## 5 6238 3 0 ## 6 9428 1 0 ## 7 9428 2 0 ## 8 10552 1 1 ## 9 10552 2 1 ## 10 11252 1 0 ## # … with 11,617 more rows We need to think some about this convention of recording the data when we count the deaths. It is often useful to extract the first (and sometimes last) observation, in order to assess certain covariate statistics on the individual level. We can create a dataset including only the first (or last) observation per participant from the fhs data using tidyverse tools. The group_by functions groups data by unique values of designated variables (here randid) and the slice function selects rows as designated. fhs_first &lt;- fhs %&gt;% group_by(randid) %&gt;% slice(1L)%&gt;% ungroup() Alternatively you can use the slice_head function, which allows us to slice a designated number of rows beginning from the first observation. Because we are piping this in the group_by function, we will be slicing rows beginning from the first observation for each randid fhs_first &lt;- fhs %&gt;% group_by(randid) %&gt;% slice_head(n=1)%&gt;% ungroup() We can similarly select the last observation for each participant fhs_last &lt;- fhs %&gt;% group_by(randid) %&gt;% slice(n())%&gt;% ungroup() or using the slice_tail function fhs_last &lt;- fhs %&gt;% group_by(randid) %&gt;% slice_tail(n=1)%&gt;% ungroup() In this dataset we can extract statistics on baseline covariates on the individual level, but also assess the number of participants with specific values, including death=1. For example, we can use the sum function in base R, which generates the sum of all values for a given vector. In this case since each death has the value of 1 the sum function will give as the number of deaths in the sample. sum(fhs_first$death) ## [1] 1550 Conversely using tidyverse tools we can extract the number of observations with death=1 using the count function fhs_first %&gt;% count(death) ## # A tibble: 2 x 2 ## death n ## * &lt;dbl&gt; &lt;int&gt; ## 1 0 2884 ## 2 1 1550 Note that survival or time-to-event outcomes in longitudinal cohort data will often be time-varying. For example, a variable for mortality will take the value of zero until the person-time observation that represents the time interval that the outcome actually happens in. For outcomes such as mortality this will typically be the last observation. We will construct a variable like this in fhs below. In order to estimate the distribution of age at death among those participants who died during follow-up we need to create a new age at death variable. The age variable in fhs represents the participants age at each visit. Typically a death would happen between visits so the last recorded value for age would be less than the age at death. We will use the timedth variable to help us determine the actual age at death. The value of timedth is the number of days from beginning of follow-up until death for those with death=1, while it is a fixed value of timedth=8766 (the maximum duration of follow-up) for those with death=0. We can create a new age at death variable for those with death=1 using the age at baseline and timedth values fhs_first&lt;-fhs_first %&gt;% mutate(agedth=age+timedth/365.25) We can then get summary statistics on this new variable fhs_first %&gt;% summarize(min_agedth = min(agedth), mean_agedth = mean(agedth), max_agedth = max(agedth)) ## # A tibble: 1 x 3 ## min_agedth mean_agedth max_agedth ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 38.4 70.5 93 We can also check on these values by groups of interest such as sex fhs_first %&gt;% group_by(sex) %&gt;% summarize(min_agedth = min(agedth), mean_agedth = mean(agedth), max_agedth = max(agedth)) ## # A tibble: 2 x 4 ## sex min_agedth mean_agedth max_agedth ## * &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 41.6 69.5 91.1 ## 2 2 38.4 71.3 93 What is the distribution of age at time of incident MI? Are there differences between males and females? Are there differences in smoking between males and females? Similar to the question about death (all-cause mortality) we can look at disease incidence, for example myocardial infarction (MI). The fhs dataset has the hospmi variable as an indicator for any participant who had a hospitalization due to MI and timemi gives the number of days from beginning of follow up to the hospitalization due to MI. We can create an age at incident MI hospitalizaton in a similar fashion as the example for age at death. fhs_first&lt;-fhs_first %&gt;% mutate(agemi=age+timemi/365.25) We can then get summary statistics on this new agemi variable fhs_first %&gt;% summarize(min_agemi = min(agemi), mean_agemi = mean(agemi), max_agemi = max(agemi)) ## # A tibble: 1 x 3 ## min_agemi mean_agemi max_agemi ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 37 69.7 93 And by sex fhs_first %&gt;% group_by(sex) %&gt;% summarize(min_agemi = min(agemi), mean_agemi = mean(agemi), max_agemi = max(agemi)) ## # A tibble: 2 x 4 ## sex min_agemi mean_agemi max_agemi ## * &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 37 68.2 91.1 ## 2 2 38.4 70.9 93 We can see that the mean age at incident MI hospitalization among males and females is similar, but with males being somewhat younger on average at the time of incident MI. We can take a closer look at the distibution using boxplots: fhs_first %&gt;% # define the axes for the boxplot ggplot(aes(x = sex, y=agemi)) + geom_boxplot() We see that R didn’t return two separate boxplots by sex, but rather one centered between the two values of sex=1 and sex=2 which are the values for males and females respectively. This is an indicator that the sex variable is of class numeric and is treated as a continuous values rather than categorical. We can verify that this is in fact the case: class(fhs_first$sex) ## [1] &quot;numeric&quot; We can trasform the variable to one of class factor in order for it to be trated as a cateogrical variable fhs_first&lt;-fhs_first %&gt;% mutate(sex=as.factor(sex)) If we repeat the function for the boxplot now we get separate boxplots by sex fhs_first %&gt;% # define the axes for the boxplot ggplot(aes(x = sex, y=agemi)) + geom_boxplot() We can once again see from the the boxplots that females tend to be a little older at incidence of MI. What is the distribution of BMI among MI cases and non-cases? How about between smokers and non-smokers Similar to the exercise above we can compare BMI distributions by MI case status. 7.2 Coding a survival analysis [R package survival] In the context of survival analysis what is modelled is time to an event (also referred to as survival time or failure time). This is a bit different than the models in the linear or glm family that model an outcome that may follow a gaussian (linear regression), binomial (logistic model) or Poisson distribution. Another difference is that the outcome (time to event) will not be determined in some participants, as they will not have experienced the event of interest during their follow-up. These participants are considered ‘censored.’ Censoring can occur in three ways: These are all types of right censoring and in simple survival analysis they are considered to be uninformative (typically not related to exposure). If the censoring is related to the exposure and the outcome then adjustment for censoring has to happen. Let’s assume that we are interested in all cause mortality as the event of interest let’s denote \\(T\\) is time to death and \\(T\\geq 0\\). We define the survival function as \\(S(t)=Pr[T&gt;t]=1-F(t)\\), where the survival function \\(S(t)\\) is the probability that a participant survives past time \\(t\\) (\\(Pr[T&gt;t]=1\\)). \\(F(t)\\) is the Probability Density Function, (sometimes also denoted as the the Cumulative Incidence Function, \\(R(t)\\)) or the probability that that an individual will have a survival time less than or equal to t (\\([Pr(T≤t)]\\)) Time to event \\(t\\) is bounded by \\([0,\\infty)\\) and \\(S(t)\\) is non-increasing as \\(t\\) becomes greater. At \\(t=0\\), \\(S(t)=1\\) and conversely as \\(t\\) approaches \\(\\infty\\), \\(S(t)=0\\). A property of the survival and probabilty density function is \\(S(t) = 1 – F(t)\\): the survival function and the probability density function (or cumulative incidence function (\\(R(t)\\)) sum to 1. Another useful function is the hazard Function, \\(h(t)\\), which is the instantaneous potential of experiencing an event at time \\(t\\), conditional on having survived to that time (\\(h(t)=\\frac{Pr[t&lt;T\\leq t+\\Delta t|T&gt;t]}{\\Delta t}=\\frac{f(t)}{S(t)}\\)). The cumulative Hazard Function, \\(H(t)\\) is defined as the integral of the hazard function from time \\(0\\) to time \\(t\\), which equals the area under the curve \\(h(t)\\) between time \\(0\\) and time \\(t\\) (\\(H(t)=\\int_{0}^{t}h(u)du\\)). If we know any of \\(S(t)\\), \\(H(t)\\) or \\(h(t)\\), we can derive the rest based on the following relationships: \\(h(t)=\\frac{\\partial log(S(t))}{\\partial t}\\) \\(H(t)=-log(S(t))\\) and conversely \\(S(t)=exp(-H(t))\\) The survival package in R allows us to fit these types of models, including a very popular model in survival analysis, the Cox proportional hazards model that was also applied in Wong et al. (1989). A very simple way to estimate survival is the non-parametric Kaplan-Meier estimator. In R we would estimate Survival \\(S(t)\\) with all-cause mortality representing failure as follows: library(survival) S1=Surv(fhs_first$timedth,fhs_first$death) S1 ## [1] 8766+ 8766+ 8766+ 2956 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ ## [13] 8766+ 5592 6411 146 8766+ 1442 8766+ 8766+ 8766+ 8766+ 6410 8766+ ## [25] 8766+ 430 8766+ 8766+ 168 8423 8766+ 8766+ 8766+ 1047 6948 1417 ## [37] 5839 8766+ 8766+ 6269 8766+ 8766+ 8766+ 8766+ 6524 8766+ 2697 8766+ ## [49] 5973 8766+ 1881 8312 3315 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ ## [61] 7102 8766+ 8766+ 5526 5276 4875 1620 5572 8406 8766+ 8766+ 8766+ ## [73] 7078 8766+ 8766+ 8766+ 8766+ 2749 8766+ 8766+ 8766+ 8766+ 5030 2365 ## [85] 8766+ 8766+ 5320 8766+ 7119 7493 8766+ 1833 8766+ 8766+ 8766+ 4514 ## [97] 8676 4365 7643 8766+ 8766+ 2017 3873 8766+ 6036 8766+ 8766+ 5181 ## [109] 8766+ 8766+ 5590 8766+ 1187 2013 8766+ 6246 8766+ 8766+ 7796 8766+ ## [121] 4605 7801 8766+ 8766+ 8766+ 8766+ 8766+ 6020 8766+ 4090 4568 8766+ ## [133] 8766+ 8766+ 8766+ 8766+ 8766+ 5569 8766+ 8271 8766+ 8766+ 1729 8766+ ## [145] 7348 8766+ 8766+ 8766+ 8766+ 7176 2104 6902 5095 5040 8766+ 8766+ ## [157] 8766+ 1607 2138 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 4753 1440 ## [169] 5380 7635 8766+ 6455 8766+ 8766+ 7945 8766+ 8766+ 8766+ 8766+ 7157 ## [181] 8766+ 5272 8766+ 3914 8766+ 8766+ 8766+ 7607 8766+ 7166 4257 8766+ ## [193] 3888 8766+ 8766+ 2346 8766+ 8766+ 4238 709 8766+ 1547 8766+ 8766+ ## [205] 5693 8766+ 8766+ 8084 8766+ 8766+ 4467 8766+ 8766+ 8766+ 8627 8766+ ## [217] 8766+ 234 8766+ 8436 8766+ 6410 8766+ 8766+ 8766+ 8766+ 8766+ 4563 ## [229] 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 1761 8766+ 3595 8766+ ## [241] 8766+ 8766+ 4728 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 1649 8766+ 8516 ## [253] 938 8766+ 5330 8766+ 8671 8766+ 2531 1169 6187 6735 8766+ 7571 ## [265] 8766+ 8766+ 5475 4200 8766+ 8766+ 8766+ 3716 7820 4564 8766+ 4580 ## [277] 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 2205 6949 3683 8766+ 8766+ 8766+ ## [289] 8766+ 3945 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 3450 8766+ 8766+ 8766+ ## [301] 8766+ 8766+ 8766+ 8766+ 5556 8766+ 73 5189 8766+ 8766+ 8766+ 8766+ ## [313] 8766+ 8766+ 8766+ 8766+ 8766+ 4183 8766+ 7051 8766+ 8766+ 8766+ 3176 ## [325] 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 7283 8766+ 8766+ ## [337] 8766+ 8215 8766+ 8002 672 8766+ 8766+ 8766+ 8263 8766+ 8766+ 8766+ ## [349] 8766+ 8141 8766+ 8766+ 8766+ 8766+ 8766+ 1903 2481 3607 8766+ 8766+ ## [361] 8766+ 8766+ 8766+ 8766+ 8128 8766+ 8766+ 5813 5908 6954 8766+ 8766+ ## [373] 803 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 5619 5011 8766+ 1686 8766+ ## [385] 8241 8766+ 8766+ 8766+ 8766+ 2329 6824 7270 8766+ 8766+ 8766+ 8766+ ## [397] 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 4246 8766+ 8766+ 8766+ ## [409] 4444 3156 7897 8766+ 6558 5020 8766+ 8766+ 8766+ 8766+ 7448 8766+ ## [421] 8766+ 8766+ 3774 8766+ 8766+ 8766+ 4270 8766+ 8766+ 8766+ 2029 8766+ ## [433] 7892 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 2857 282 7406 ## [445] 4286 8766+ 4343 8766+ 8766+ 8766+ 8766+ 8766+ 4653 8766+ 8766+ 8766+ ## [457] 8766+ 8766+ 8766+ 8766+ 6291 8766+ 3655 4500 8766+ 8766+ 8766+ 8766+ ## [469] 5687 8766+ 8766+ 8766+ 7308 8766+ 7570 7050 8766+ 8766+ 8766+ 8766+ ## [481] 8766+ 8766+ 8766+ 8766+ 6687 8766+ 8766+ 8766+ 832 8766+ 8766+ 8766+ ## [493] 8413 8766+ 8766+ 8766+ 8766+ 4808 8766+ 8766+ 2424 8766+ 8766+ 5518 ## [505] 3176 7574 8766+ 7850 8766+ 8766+ 8766+ 8766+ 8766+ 7739 8625 8766+ ## [517] 8766+ 1517 8766+ 8766+ 3379 8766+ 8766+ 8766+ 8766+ 8766+ 3910 8766+ ## [529] 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 7223 5167 8766+ 8766+ 5410 ## [541] 2707 8766+ 2462 58 1142 8766+ 8766+ 8766+ 6734 8766+ 8766+ 5437 ## [553] 8766+ 8766+ 8766+ 8766+ 8766+ 7303 6808 8766+ 8766+ 3405 8766+ 1920 ## [565] 8766+ 8766+ 8766+ 8766+ 7858 8766+ 8766+ 7707 8766+ 267 1622 8766+ ## [577] 8766+ 8766+ 8766+ 2141 8766+ 8766+ 8766+ 8766+ 8766+ 3503 8766+ 8766+ ## [589] 1336 4341 8766+ 7210 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ ## [601] 8766+ 6295 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ ## [613] 8766+ 8766+ 8766+ 2580 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8727 7684 ## [625] 2157 7610 3587 987 4441 2287 6977 8766+ 8766+ 5769 8766+ 8766+ ## [637] 4145 8766+ 8766+ 8766+ 8766+ 8092 7771 992 8766+ 8766+ 8766+ 612 ## [649] 8766+ 8766+ 8766+ 7795 8766+ 4972 5062 8766+ 8766+ 8766+ 8766+ 8513 ## [661] 8766+ 8766+ 7633 3945 8766+ 8766+ 8766+ 8766+ 6214 8766+ 8766+ 8766+ ## [673] 3807 5530 8766+ 8766+ 6019 3999 8766+ 8766+ 3391 6938 8766+ 5930 ## [685] 4561 8766+ 8766+ 1904 7996 8357 8766+ 8766+ 8766+ 8766+ 8513 8766+ ## [697] 8766+ 8187 8766+ 8766+ 8766+ 8766+ 4860 8766+ 8766+ 7225 5641 4817 ## [709] 8766+ 8766+ 8766+ 8766+ 8766+ 2796 5674 8766+ 7104 8766+ 8766+ 8766+ ## [721] 1757 8766+ 5794 8766+ 5071 8766+ 8766+ 6965 8766+ 7766 8766+ 5263 ## [733] 1370 8766+ 6984 5351 8766+ 8766+ 8766+ 8766+ 3434 4145 8766+ 8766+ ## [745] 8766+ 8766+ 8766+ 7799 7493 5843 8766+ 7569 8766+ 8766+ 8766+ 8766+ ## [757] 8766+ 8766+ 8766+ 2117 4356 8766+ 1748 8766+ 4118 8766+ 8766+ 8766+ ## [769] 8048 8766+ 8766+ 4237 2550 8766+ 8766+ 8610 8766+ 7601 7005 8766+ ## [781] 8766+ 5722 5764 8766+ 8766+ 6900 8766+ 8766+ 8766+ 8766+ 2975 8766+ ## [793] 2194 8766+ 1269 8766+ 8766+ 8766+ 2720 8766+ 3208 8766+ 7373 8766+ ## [805] 8766+ 7035 8766+ 8766+ 8766+ 8766+ 6001 8766+ 8766+ 8766+ 8766+ 8766+ ## [817] 8766+ 6968 8766+ 3565 8766+ 8766+ 8766+ 4618 1168 6495 8766+ 8766+ ## [829] 4931 8766+ 8766+ 5542 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 3848 8766+ ## [841] 5122 8766+ 6452 8766+ 8766+ 8766+ 8766+ 8766+ 7905 8766+ 8766+ 8766+ ## [853] 7294 8766+ 8766+ 6610 5879 8766+ 8766+ 8766+ 8766+ 4576 307 8766+ ## [865] 3133 8766+ 8766+ 1885 8766+ 2015 8766+ 8766+ 8766+ 4887 5954 8530 ## [877] 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8550 8766+ 8766+ 8766+ ## [889] 8285 8766+ 8766+ 8766+ 8766+ 4685 8766+ 8285 1175 1416 8766+ 8766+ ## [901] 8766+ 8766+ 6511 8177 8766+ 8766+ 8766+ 3549 3463 8766+ 8766+ 8766+ ## [913] 2042 8766+ 8766+ 7436 1490 8766+ 8766+ 3413 3091 8766+ 8766+ 8766+ ## [925] 8766+ 3368 8766+ 8766+ 8766+ 184 3632 8766+ 6078 8766+ 4097 8766+ ## [937] 8766+ 5931 8766+ 6819 8766+ 8766+ 8766+ 8766+ 8766+ 7340 2218 8766+ ## [949] 822 1805 4019 8766+ 8766+ 8766+ 8766+ 8766+ 8234 8281 8195 6718 ## [961] 8766+ 8766+ 8766+ 8766+ 1986 8766+ 8766+ 5448 6190 5226 717 4369 ## [973] 6719 8766+ 7972 8766+ 8766+ 8766+ 8766+ 8766+ 4318 8766+ 8766+ 8766+ ## [985] 1778 8766+ 8766+ 8766+ 2335 8766+ 3878 4119 693 7507 8766+ 8766+ ## [997] 8766+ 7179 8766+ 8766+ 2812 8766+ 8722 8766+ 7998 8766+ 8766+ 8766+ ## [1009] 700 8766+ 8766+ 8766+ 1879 6434 7484 8766+ 424 6763 8766+ 8766+ ## [1021] 8766+ 6188 5065 8052 8766+ 8766+ 8766+ 7447 8766+ 8766+ 8766+ 5313 ## [1033] 8766+ 8766+ 7809 8766+ 8766+ 8766+ 8766+ 8766+ 8275 8310 8766+ 8766+ ## [1045] 8766+ 3731 7876 8766+ 8766+ 7378 8766+ 8766+ 8650 8766+ 6672 8361 ## [1057] 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 3282 8766+ 8766+ ## [1069] 8766+ 8766+ 2995 8766+ 8766+ 8766+ 7921 8766+ 8766+ 8766+ 8766+ 4637 ## [1081] 8766+ 5066 2568 3017 8766+ 8766+ 8766+ 8766+ 8766+ 1235 8766+ 8766+ ## [1093] 8766+ 7710 4725 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ ## [1105] 8766+ 2682 8766+ 8766+ 6130 8766+ 8766+ 1770 6737 8766+ 8766+ 3692 ## [1117] 8766+ 1575 8766+ 8766+ 8766+ 5740 5901 5958 6633 8766+ 8766+ 3025 ## [1129] 8766+ 8766+ 8766+ 8766+ 1547 8766+ 8766+ 1368 5252 6152 8766+ 3571 ## [1141] 8766+ 8766+ 7220 8766+ 8766+ 6049 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ ## [1153] 8766+ 5861 8766+ 8766+ 1506 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 7611 ## [1165] 5685 8766+ 6224 7539 8766+ 8766+ 8766+ 6808 8766+ 7256 5600 4820 ## [1177] 2285 8766+ 5229 8766+ 5272 8766+ 8766+ 5387 8766+ 4154 6184 5820 ## [1189] 1154 8766+ 8766+ 4792 8766+ 8766+ 6737 8766+ 8766+ 8766+ 8766+ 8766+ ## [1201] 5616 8766+ 3032 8766+ 8766+ 8766+ 4729 8766+ 8766+ 2143 6692 4470 ## [1213] 8766+ 4616 6986 8766+ 8766+ 1989 8766+ 6877 8766+ 8766+ 8766+ 4050 ## [1225] 8766+ 367 8766+ 6813 8766+ 8766+ 8766+ 8766+ 5145 8705 5962 8766+ ## [1237] 1883 5722 3510 8766+ 7685 4714 8766+ 8766+ 8766+ 3777 7654 8766+ ## [1249] 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 6418 8766+ 8766+ 6210 8766+ 8766+ ## [1261] 8132 8766+ 8766+ 6824 8766+ 8766+ 8766+ 8766+ 6254 2666 2079 8766+ ## [1273] 8766+ 8490 8766+ 8766+ 8766+ 8766+ 8766+ 6182 8766+ 8766+ 8404 8766+ ## [1285] 26 8766+ 8766+ 8766+ 8766+ 8766+ 6316 8549 8766+ 8766+ 8766+ 6770 ## [1297] 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 6124 8766+ 8766+ 8707 8766+ 8766+ ## [1309] 2395 8766+ 8766+ 8766+ 7895 8613 8766+ 7212 8766+ 8766+ 8766+ 8766+ ## [1321] 4635 1433 8766+ 6899 8766+ 8766+ 8766+ 8766+ 8766+ 8734 8766+ 8766+ ## [1333] 5194 8766+ 4359 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 2562 8766+ 6554 ## [1345] 6944 8766+ 1395 8766+ 8766+ 8766+ 8766+ 8766+ 6155 1103 4975 8766+ ## [1357] 8766+ 7014 6647 8766+ 8214 8766+ 8766+ 8766+ 8766+ 8766+ 1985 8766+ ## [1369] 8098 8766+ 5399 5816 8766+ 8766+ 8766+ 8766+ 8766+ 4677 8766+ 8766+ ## [1381] 8766+ 8766+ 8766+ 8766+ 7328 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ ## [1393] 8766+ 8766+ 8766+ 8766+ 8766+ 1710 2270 3953 8766+ 8766+ 8766+ 8766+ ## [1405] 7455 8766+ 8766+ 7015 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 3763 ## [1417] 6660 8766+ 8766+ 8766+ 3619 2990 4890 8766+ 8766+ 8766+ 8766+ 8766+ ## [1429] 2503 8766+ 8766+ 8744 8319 5157 8766+ 8766+ 8669 1462 8766+ 4969 ## [1441] 8766+ 8766+ 6940 8766+ 8766+ 8766+ 8766+ 3459 2112 4895 7307 8078 ## [1453] 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 6700 8766+ 8766+ 7899 8766+ 8766+ ## [1465] 8766+ 8766+ 2965 8618 2520 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ ## [1477] 4873 8766+ 8766+ 8766+ 8766+ 8766+ 8332 8766+ 8766+ 8766+ 8766+ 5878 ## [1489] 8766+ 8766+ 4492 3640 8766+ 8766+ 6028 8766+ 7327 8766+ 8766+ 8766+ ## [1501] 8766+ 8766+ 8766+ 4723 8766+ 8766+ 8766+ 6132 8766+ 8766+ 4522 8766+ ## [1513] 8766+ 8404 8766+ 8766+ 7567 3624 8766+ 4392 8766+ 8766+ 8766+ 8766+ ## [1525] 8766+ 7026 8766+ 8766+ 8766+ 8766+ 3776 8374 8766+ 8766+ 3789 8766+ ## [1537] 8766+ 8766+ 6584 8766+ 7337 3622 8766+ 8766+ 8766+ 8766+ 5876 5905 ## [1549] 6024 4700 8766+ 8630 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ ## [1561] 8766+ 8766+ 8766+ 8766+ 8766+ 7950 6104 8766+ 8766+ 5002 5644 8766+ ## [1573] 8766+ 8766+ 8766+ 8766+ 8766+ 4128 8321 8766+ 8766+ 3406 8766+ 8766+ ## [1585] 1984 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 7591 ## [1597] 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 6615 7774 3130 ## [1609] 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 1825 8766+ 8766+ 1587 8766+ ## [1621] 8766+ 8766+ 6817 5252 8766+ 8766+ 3657 8766+ 8766+ 6670 4051 8766+ ## [1633] 8036 8766+ 8766+ 2980 8766+ 8766+ 8766+ 7600 8766+ 8766+ 8766+ 8766+ ## [1645] 6180 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 5137 4359 8766+ 2160 1942 ## [1657] 8766+ 3394 7428 7667 8766+ 8766+ 8657 8766+ 945 8766+ 7974 8766+ ## [1669] 8690 5458 7710 4576 7774 8766+ 8766+ 8766+ 8766+ 2984 8766+ 8766+ ## [1681] 8766+ 8766+ 8766+ 5200 8766+ 8766+ 8766+ 8766+ 7251 6283 8766+ 8766+ ## [1693] 8766+ 3076 8766+ 8766+ 8766+ 8766+ 8766+ 6284 1943 8766+ 8766+ 8766+ ## [1705] 8766+ 8766+ 8766+ 8766+ 7587 8766+ 8766+ 1933 8766+ 8766+ 8766+ 8766+ ## [1717] 8766+ 8766+ 8766+ 8766+ 8402 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 7887 ## [1729] 8766+ 8766+ 8766+ 6070 8766+ 8766+ 8766+ 8766+ 8766+ 7668 8766+ 8766+ ## [1741] 8766+ 8766+ 8766+ 8766+ 8766+ 2142 3184 6632 8766+ 2148 8766+ 8157 ## [1753] 5911 8766+ 8766+ 8766+ 7036 8766+ 8766+ 8766+ 498 8766+ 8766+ 8766+ ## [1765] 8766+ 3147 2807 3859 3591 4705 8766+ 8766+ 8766+ 8766+ 8766+ 3797 ## [1777] 8766+ 8766+ 8285 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 6164 ## [1789] 8519 4168 8766+ 7729 8766+ 8766+ 564 8766+ 858 8766+ 8766+ 8766+ ## [1801] 8766+ 7679 8766+ 5655 6007 2570 8766+ 8766+ 8766+ 8766+ 8766+ 6777 ## [1813] 8766+ 8766+ 5384 8766+ 7644 8766+ 8766+ 2864 8766+ 8766+ 3295 8766+ ## [1825] 6876 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ ## [1837] 2620 8766+ 8766+ 2983 8766+ 4719 8631 4854 8766+ 8766+ 8766+ 3518 ## [1849] 6248 7419 8766+ 8766+ 8766+ 8766+ 8766+ 2674 8766+ 7594 8759 1692 ## [1861] 8766+ 8766+ 8766+ 8766+ 7630 8766+ 8766+ 4542 8766+ 8349 8683 8766+ ## [1873] 8766+ 544 8766+ 8766+ 45 8766+ 8766+ 8766+ 8766+ 8766+ 3721 8766+ ## [1885] 1939 8766+ 8766+ 8766+ 8766+ 8766+ 4352 8766+ 8766+ 8766+ 8766+ 8016 ## [1897] 8766+ 6804 7367 2243 7365 5789 8766+ 1023 3455 8766+ 8766+ 2634 ## [1909] 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 1492 ## [1921] 8766+ 8766+ 4995 8295 8766+ 8766+ 8766+ 2587 3953 8766+ 5439 8218 ## [1933] 8371 8766+ 5232 4060 2400 8766+ 8766+ 5320 6583 5375 6320 8766+ ## [1945] 3211 4162 8766+ 5413 8766+ 7613 623 6952 4967 8766+ 8172 8766+ ## [1957] 8753 8766+ 8766+ 8766+ 5139 8766+ 8766+ 8766+ 4894 2784 4293 5761 ## [1969] 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 2752 8766+ 8766+ 8240 8766+ ## [1981] 6975 8766+ 8656 8766+ 8766+ 8766+ 3694 4036 8766+ 8766+ 8122 6531 ## [1993] 6655 8370 8766+ 7216 8766+ 8766+ 8766+ 2960 8766+ 8766+ 4684 8766+ ## [2005] 8766+ 8766+ 8766+ 8766+ 8046 8766+ 5806 8766+ 8766+ 7592 3726 8766+ ## [2017] 8766+ 8766+ 8766+ 8766+ 8717 8766+ 6438 8766+ 8766+ 8766+ 4762 8766+ ## [2029] 8766+ 3655 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8608 6201 8766+ ## [2041] 8766+ 5600 40 8766+ 2192 8690 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ ## [2053] 8766+ 8766+ 8766+ 5169 8726 8766+ 8766+ 6274 8133 3348 8766+ 607 ## [2065] 8766+ 5623 8766+ 8766+ 3337 8766+ 46 8766+ 8766+ 6144 3981 8766+ ## [2077] 8766+ 8766+ 8766+ 8766+ 8766+ 1197 8766+ 8766+ 4915 8766+ 8766+ 8766+ ## [2089] 8766+ 4793 7569 8766+ 1587 8766+ 8766+ 8766+ 6618 8766+ 8766+ 8766+ ## [2101] 8766+ 8766+ 5439 430 8766+ 8766+ 7542 8766+ 8766+ 8766+ 8766+ 8766+ ## [2113] 8766+ 5322 8766+ 8766+ 8766+ 4275 8766+ 8766+ 8454 8766+ 4181 537 ## [2125] 8766+ 8766+ 8766+ 8766+ 6170 5040 8766+ 8766+ 8766+ 8766+ 8766+ 2146 ## [2137] 8766+ 8766+ 8766+ 8766+ 2573 2830 8766+ 8766+ 8766+ 8020 7184 8766+ ## [2149] 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 3683 7460 8766+ 8766+ 3840 8766+ ## [2161] 8766+ 8766+ 4360 3219 8766+ 8766+ 8766+ 8164 3065 8766+ 8766+ 8766+ ## [2173] 4921 8766+ 8274 8766+ 8744 8766+ 8766+ 3898 8766+ 8766+ 8766+ 8766+ ## [2185] 8766+ 5755 8766+ 756 8766+ 8766+ 8766+ 6538 1099 8766+ 5931 7623 ## [2197] 8766+ 8766+ 8766+ 8766+ 4496 2759 8766+ 8766+ 8766+ 8766+ 7630 8766+ ## [2209] 2478 8766+ 8766+ 8766+ 8766+ 3251 8741 5885 1454 4146 8766+ 8172 ## [2221] 8766+ 8766+ 5714 6533 8766+ 8766+ 5883 788 8766+ 8766+ 8766+ 6561 ## [2233] 8766+ 2046 8766+ 5997 8766+ 8766+ 3601 331 8766+ 8766+ 8766+ 8766+ ## [2245] 8766+ 7283 8766+ 4245 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ ## [2257] 8766+ 8766+ 8766+ 7439 8766+ 7305 8766+ 8766+ 6572 4235 8766+ 8766+ ## [2269] 8766+ 8766+ 8766+ 8766+ 6176 8766+ 8469 8766+ 5618 5376 8766+ 8766+ ## [2281] 385 8766+ 7499 3779 5265 2606 8766+ 7307 8766+ 8411 6591 3489 ## [2293] 8766+ 4775 8766+ 8766+ 3888 8766+ 8546 6226 8766+ 2488 8766+ 7623 ## [2305] 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 4659 8766+ 8766+ 2021 3672 3173 ## [2317] 8766+ 8766+ 7569 8766+ 2453 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 4266 ## [2329] 8766+ 8766+ 8766+ 8766+ 7193 4934 8766+ 8766+ 8766+ 8766+ 8766+ 1221 ## [2341] 5322 8766+ 2364 8766+ 7328 8766+ 8766+ 8242 8766+ 8766+ 8766+ 8766+ ## [2353] 8766+ 8766+ 8766+ 8766+ 8766+ 7667 7980 8266 8766+ 8766+ 8766+ 6519 ## [2365] 8766+ 8766+ 8766+ 8766+ 4041 4870 8766+ 8766+ 1848 7395 3600 8766+ ## [2377] 8766+ 4485 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ ## [2389] 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 34 3269 8766+ 4262 8766+ 8766+ ## [2401] 8766+ 7826 8766+ 8766+ 5269 8766+ 8766+ 4494 7352 8766+ 8766+ 8766+ ## [2413] 8766+ 8158 8697 8766+ 5553 8766+ 8766+ 3132 8766+ 8766+ 7592 4757 ## [2425] 2434 8766+ 8766+ 5655 6209 8766+ 8723 5565 8766+ 8766+ 2993 8766+ ## [2437] 4652 8766+ 8766+ 3460 8766+ 8766+ 2641 8766+ 1936 1947 8766+ 8766+ ## [2449] 8766+ 8766+ 8766+ 1765 8766+ 8766+ 8766+ 5709 8766+ 8766+ 8766+ 8513 ## [2461] 8766+ 6353 4127 8766+ 8766+ 1761 2289 8766+ 8766+ 5898 6381 8766+ ## [2473] 8766+ 8766+ 1400 8766+ 8766+ 8766+ 8766+ 8766+ 6029 4104 584 8766+ ## [2485] 2614 8766+ 5337 3352 8766+ 1644 8766+ 8064 8766+ 8766+ 889 8766+ ## [2497] 8556 8766+ 8766+ 1440 8766+ 8766+ 7609 8766+ 8766+ 5413 8415 3471 ## [2509] 8766+ 6199 3573 8543 7554 8766+ 8766+ 8766+ 3802 8766+ 8766+ 8766+ ## [2521] 7818 8766+ 8766+ 6974 7347 827 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ ## [2533] 8766+ 4324 1660 8337 8766+ 8766+ 3738 8766+ 8766+ 8766+ 3077 7038 ## [2545] 8007 8766+ 8766+ 8766+ 5150 8766+ 8766+ 7091 8766+ 8766+ 8766+ 1040 ## [2557] 5732 8766+ 8766+ 8766+ 3128 8766+ 2391 8766+ 4780 5660 8471 6531 ## [2569] 8766+ 8766+ 8766+ 7051 8766+ 8766+ 8766+ 8766+ 7681 8766+ 5906 5145 ## [2581] 2891 8766+ 7758 8766+ 8766+ 8766+ 5127 8766+ 8766+ 7066 7560 8766+ ## [2593] 2058 8766+ 8766+ 8529 8676 1303 8766+ 8648 7045 8766+ 8766+ 8766+ ## [2605] 8766+ 8766+ 8766+ 8766+ 6412 8766+ 7689 8766+ 8766+ 8766+ 2389 8766+ ## [2617] 1632 8766+ 8766+ 2601 5072 2116 946 8766+ 4276 8766+ 7772 8766+ ## [2629] 3273 8498 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ ## [2641] 8766+ 8766+ 7183 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 1443 ## [2653] 8766+ 8766+ 8766+ 5466 8766+ 8766+ 8386 8766+ 8766+ 8766+ 8766+ 8766+ ## [2665] 8664 7322 7294 8766+ 8766+ 8766+ 8766+ 8766+ 4423 6131 8766+ 6343 ## [2677] 8766+ 7469 6464 8766+ 8066 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 1204 ## [2689] 8766+ 8766+ 7618 8766+ 6385 8766+ 8766+ 8766+ 8766+ 5870 8766+ 3796 ## [2701] 8766+ 7803 8766+ 8766+ 6187 8766+ 8766+ 8766+ 5579 8766+ 8766+ 3514 ## [2713] 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 4758 8766+ 8766+ 8173 8766+ 7070 ## [2725] 2937 8766+ 8766+ 8766+ 8766+ 2008 8766+ 8766+ 6183 7784 8766+ 8766+ ## [2737] 8766+ 8766+ 8766+ 8766+ 8766+ 5283 3660 8766+ 7269 5581 8766+ 8766+ ## [2749] 8766+ 8766+ 2677 8766+ 5026 4082 8766+ 8766+ 8766+ 5085 8766+ 4878 ## [2761] 8766+ 8766+ 8766+ 1815 6394 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 2841 ## [2773] 3752 8766+ 7552 6245 5119 8766+ 8766+ 8766+ 8766+ 4867 5353 8766+ ## [2785] 8766+ 8766+ 8766+ 8766+ 8738 8766+ 8766+ 8766+ 7624 8766+ 8766+ 5843 ## [2797] 174 7951 1386 8700 5867 8766+ 7086 8766+ 8766+ 8766+ 7514 8766+ ## [2809] 8766+ 8016 5727 1912 8766+ 8766+ 4560 5154 6832 8766+ 3429 4378 ## [2821] 8766+ 8766+ 6085 6063 8766+ 8766+ 3758 8766+ 8766+ 8766+ 8766+ 8766+ ## [2833] 5444 6474 2457 4900 8766+ 7267 8766+ 8766+ 8766+ 287 8233 8766+ ## [2845] 8766+ 6389 8766+ 4243 4154 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ ## [2857] 8766+ 8766+ 7661 6585 3176 8766+ 4447 8766+ 8766+ 5773 8766+ 5589 ## [2869] 5591 6412 8766+ 8766+ 4484 8766+ 5702 3130 8766+ 8766+ 2465 8766+ ## [2881] 8119 8220 8273 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ ## [2893] 8766+ 8766+ 8766+ 4158 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ ## [2905] 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ ## [2917] 8766+ 8766+ 7543 8766+ 8766+ 8766+ 3272 1355 8766+ 7689 8766+ 8766+ ## [2929] 6602 8766+ 145 8766+ 7885 8766+ 5101 2738 8766+ 8766+ 5000 8766+ ## [2941] 8766+ 8766+ 8766+ 8766+ 8766+ 5428 8766+ 8766+ 8766+ 5733 8766+ 8766+ ## [2953] 8766+ 5604 7889 8766+ 8766+ 8766+ 8766+ 7115 8766+ 4053 8766+ 7436 ## [2965] 8766+ 8766+ 8766+ 5322 8766+ 5515 8766+ 8766+ 5396 8766+ 4249 8766+ ## [2977] 8766+ 8766+ 8766+ 2971 8766+ 7360 8766+ 5746 8766+ 8766+ 8335 6034 ## [2989] 8766+ 7113 8766+ 7448 8766+ 8766+ 8766+ 8766+ 8708 8766+ 8766+ 8766+ ## [3001] 8702 8766+ 1988 8766+ 8246 8766+ 8766+ 7537 8766+ 8766+ 5098 8766+ ## [3013] 1781 8766+ 8766+ 6201 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ ## [3025] 4205 3568 8766+ 8766+ 8766+ 8766+ 8766+ 6030 8766+ 8766+ 8766+ 5496 ## [3037] 8766+ 6722 8766+ 8766+ 8766+ 327 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ ## [3049] 8766+ 2964 5629 8766+ 6925 8766+ 4142 8766+ 3429 7634 8766+ 2247 ## [3061] 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 5786 6781 8766+ 126 ## [3073] 8766+ 8766+ 8766+ 8013 2348 8766+ 7403 8766+ 8766+ 8766+ 8766+ 8766+ ## [3085] 8766+ 7146 8766+ 6084 8766+ 4903 8766+ 4907 2719 5309 2193 8766+ ## [3097] 8766+ 87 8766+ 8766+ 8766+ 7518 8766+ 8766+ 4730 8766+ 6464 8766+ ## [3109] 8766+ 8766+ 4513 8766+ 8766+ 8766+ 6497 8766+ 4024 8766+ 753 8766+ ## [3121] 8766+ 8766+ 8766+ 2424 8766+ 8766+ 8766+ 8766+ 4683 5252 8766+ 6212 ## [3133] 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ ## [3145] 8766+ 4034 3730 8766+ 8766+ 8766+ 8766+ 8766+ 7769 8766+ 8766+ 429 ## [3157] 6973 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 4028 133 8766+ ## [3169] 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 6831 6129 8766+ 8766+ 8766+ ## [3181] 8766+ 8766+ 5009 5384 8766+ 1102 8766+ 8766+ 8766+ 8766+ 8766+ 8470 ## [3193] 3573 8766+ 8766+ 8766+ 378 6002 4442 8766+ 8766+ 8766+ 8766+ 7506 ## [3205] 4763 7935 8766+ 8766+ 4516 8507 2311 8766+ 8766+ 3490 8766+ 1214 ## [3217] 8505 8766+ 8766+ 8766+ 8766+ 4977 6006 8766+ 8766+ 2105 2235 7230 ## [3229] 7005 8766+ 8401 8766+ 5296 8766+ 5362 8766+ 5321 8766+ 647 8766+ ## [3241] 8766+ 8766+ 8766+ 8766+ 8766+ 6437 6080 8766+ 6798 8766+ 7940 5538 ## [3253] 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 2634 8766+ 8766+ 8766+ 8766+ 8766+ ## [3265] 8766+ 8766+ 8766+ 7576 8766+ 8766+ 8766+ 8766+ 8766+ 4396 8766+ 7083 ## [3277] 8766+ 8766+ 8766+ 2298 5446 8766+ 8766+ 8766+ 6350 2618 8766+ 8766+ ## [3289] 8766+ 6449 5007 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 6970 8766+ ## [3301] 7837 7504 8766+ 8766+ 2397 6756 8766+ 3077 8766+ 8766+ 6227 8766+ ## [3313] 8766+ 8766+ 3728 8766+ 8766+ 7851 8766+ 8766+ 6407 6272 1197 8766+ ## [3325] 8766+ 8766+ 7632 8766+ 6926 8050 8766+ 2666 2896 8766+ 8766+ 8766+ ## [3337] 1305 8766+ 8766+ 5017 8766+ 5010 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ ## [3349] 8766+ 1273 7074 8060 8766+ 8766+ 8766+ 4882 8766+ 8766+ 7135 8766+ ## [3361] 8766+ 8766+ 8766+ 8766+ 8766+ 1350 6910 8766+ 8766+ 1563 8766+ 8766+ ## [3373] 8766+ 3109 8766+ 8766+ 8766+ 8766+ 4961 2818 339 8766+ 7866 7118 ## [3385] 8766+ 3941 8766+ 8766+ 5526 7151 8766+ 8766+ 8766+ 7711 8766+ 8298 ## [3397] 8766+ 8766+ 8766+ 8766+ 3625 8766+ 4585 8766+ 7192 8766+ 8766+ 8766+ ## [3409] 7306 8766+ 8766+ 8766+ 8766+ 8674 7766 4924 8766+ 8766+ 8766+ 8766+ ## [3421] 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 4295 8766+ 8357 8766+ 6356 ## [3433] 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 6621 8766+ 5629 8766+ ## [3445] 8766+ 8766+ 8212 8766+ 8766+ 8525 8766+ 8766+ 8766+ 3898 8766+ 8766+ ## [3457] 8766+ 8766+ 8766+ 593 8766+ 8766+ 7649 6626 8766+ 8317 8766+ 8766+ ## [3469] 8766+ 4907 8766+ 3833 8766+ 2807 8766+ 8766+ 8307 8766+ 7041 806 ## [3481] 8766+ 5621 8766+ 8766+ 8766+ 8766+ 4011 8766+ 4988 8766+ 8766+ 2826 ## [3493] 1906 8766+ 7246 8766+ 5992 8766+ 8766+ 8766+ 8766+ 2948 8766+ 8766+ ## [3505] 3698 8766+ 8766+ 6001 8766+ 8766+ 2406 4643 8766+ 8766+ 8766+ 8766+ ## [3517] 5554 3564 8766+ 8766+ 8766+ 8766+ 7820 8766+ 8766+ 8766+ 8766+ 8766+ ## [3529] 8766+ 8766+ 8766+ 8766+ 8766+ 6942 4560 5437 8101 8766+ 8766+ 8766+ ## [3541] 8766+ 6577 8766+ 8766+ 8766+ 8766+ 8747 8766+ 8766+ 8766+ 8766+ 8766+ ## [3553] 8766+ 7267 8766+ 1396 8766+ 8766+ 8766+ 5316 2589 1847 5731 8766+ ## [3565] 8766+ 8766+ 3356 8766+ 8766+ 8766+ 5035 8766+ 8766+ 7064 526 7225 ## [3577] 8766+ 8766+ 8627 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 4844 8766+ ## [3589] 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8274 2708 8766+ 8766+ 8766+ 3932 ## [3601] 8766+ 8766+ 3538 8766+ 8766+ 1215 8766+ 8766+ 8766+ 7155 8766+ 8766+ ## [3613] 8766+ 8766+ 5088 8766+ 8766+ 5499 2474 8622 8766+ 8766+ 8766+ 8511 ## [3625] 8766+ 8766+ 7738 7428 8766+ 8766+ 8766+ 8766+ 8766+ 7840 8766+ 8766+ ## [3637] 8766+ 7953 3247 5400 8766+ 8766+ 8766+ 537 8766+ 8766+ 1959 3837 ## [3649] 3416 8766+ 7049 8766+ 815 8766+ 8766+ 8766+ 4554 869 8766+ 8766+ ## [3661] 4102 8766+ 8766+ 8766+ 266 8766+ 8766+ 8766+ 4319 8766+ 8766+ 8766+ ## [3673] 4452 8766+ 8766+ 8766+ 1629 3944 8766+ 8766+ 7671 8766+ 8766+ 8766+ ## [3685] 8766+ 8766+ 7519 8766+ 8766+ 8766+ 8766+ 8766+ 6427 6684 8766+ 8766+ ## [3697] 8417 6746 2230 8766+ 8766+ 8576 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ ## [3709] 7794 8766+ 8766+ 8766+ 8054 8766+ 3760 996 3253 8766+ 7052 8766+ ## [3721] 8766+ 8766+ 7157 8291 8238 8595 8518 8766+ 8766+ 8766+ 8766+ 1377 ## [3733] 8766+ 7541 8766+ 8766+ 8766+ 3773 8766+ 8766+ 8766+ 8538 8766+ 2567 ## [3745] 8766+ 8766+ 8766+ 5900 5936 6367 5527 8293 8766+ 8766+ 163 8469 ## [3757] 8766+ 8766+ 8766+ 8766+ 8766+ 4338 8766+ 8766+ 4654 8031 3802 935 ## [3769] 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 5732 8766+ 8766+ 8766+ 8766+ 7241 ## [3781] 8766+ 3851 8766+ 8766+ 8640 6340 5597 8766+ 8766+ 8766+ 8766+ 8766+ ## [3793] 2590 8766+ 8766+ 8766+ 8766+ 1003 8766+ 8766+ 6561 5888 8766+ 8766+ ## [3805] 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 1863 8766+ 8766+ 8766+ 8766+ ## [3817] 6997 8766+ 8766+ 8766+ 8766+ 3407 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ ## [3829] 8766+ 3339 8766+ 8766+ 6893 5490 5891 8766+ 7963 8766+ 8766+ 8766+ ## [3841] 3536 8766+ 8766+ 7359 2609 8766+ 8766+ 828 5620 764 8766+ 5638 ## [3853] 6281 8766+ 6531 8766+ 6460 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ ## [3865] 8766+ 8766+ 8344 8766+ 8766+ 8766+ 7431 8766+ 8766+ 8766+ 6604 8766+ ## [3877] 8766+ 8766+ 8766+ 7823 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 3587 8766+ ## [3889] 8766+ 8766+ 8766+ 8766+ 6506 8766+ 8766+ 8766+ 4185 8766+ 6695 8766+ ## [3901] 5611 1582 8766+ 8766+ 8766+ 8766+ 5131 8766+ 4494 4936 8766+ 6747 ## [3913] 8766+ 8766+ 8766+ 8766+ 8766+ 5788 8766+ 8766+ 3776 8727 5123 8766+ ## [3925] 8766+ 8766+ 8766+ 5499 8766+ 8766+ 178 7793 8766+ 8766+ 5977 8766+ ## [3937] 8766+ 8766+ 8766+ 887 8766+ 8766+ 8766+ 8766+ 6423 8766+ 8766+ 8766+ ## [3949] 6262 8766+ 3488 8766+ 8305 8766+ 8452 8766+ 8766+ 8766+ 8766+ 8766+ ## [3961] 8766+ 2540 7557 8766+ 6205 7066 8451 3161 4511 8766+ 1740 8766+ ## [3973] 8766+ 8766+ 8766+ 8766+ 3970 7820 8766+ 7317 5081 6746 8766+ 4845 ## [3985] 8766+ 8766+ 6290 8766+ 4484 8766+ 4537 8766+ 6755 8766+ 1141 8766+ ## [3997] 8766+ 8766+ 8766+ 8766+ 6846 8223 8766+ 8766+ 442 8766+ 8766+ 5452 ## [4009] 8766+ 7661 4182 6387 3704 8766+ 8766+ 53 8766+ 2122 8766+ 8766+ ## [4021] 5183 8766+ 8766+ 5775 415 5206 8766+ 1568 8766+ 8766+ 8277 8766+ ## [4033] 8766+ 8766+ 5667 8766+ 8766+ 7357 8766+ 2425 8766+ 1792 6260 8766+ ## [4045] 5687 8766+ 8766+ 8766+ 2981 8766+ 4984 5154 8163 8766+ 8766+ 8766+ ## [4057] 6532 8111 8766+ 6937 8766+ 8766+ 8766+ 8766+ 7819 8766+ 3129 8766+ ## [4069] 8766+ 8766+ 3998 4082 8766+ 8766+ 6568 8766+ 8766+ 8766+ 8766+ 1752 ## [4081] 7436 7888 8766+ 8766+ 1747 8766+ 7012 8766+ 8766+ 8766+ 8766+ 8766+ ## [4093] 7608 8766+ 8766+ 8766+ 3550 6404 8766+ 8766+ 8766+ 7708 3893 8766+ ## [4105] 8766+ 7520 771 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 305 7548 ## [4117] 8766+ 8766+ 5128 8766+ 8766+ 8766+ 8766+ 8766+ 7073 2420 8766+ 8766+ ## [4129] 8766+ 1755 6934 4351 8766+ 854 8766+ 8766+ 8766+ 8766+ 8137 8766+ ## [4141] 8766+ 3646 7019 8766+ 8766+ 7059 8766+ 8766+ 8766+ 8766+ 7483 6757 ## [4153] 8766+ 7583 8766+ 8766+ 8766+ 5545 8766+ 8766+ 5351 6517 8766+ 8766+ ## [4165] 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ ## [4177] 8766+ 8766+ 8766+ 8766+ 8766+ 572 5159 8244 6026 8766+ 8766+ 8766+ ## [4189] 8766+ 8766+ 8595 6630 8766+ 8766+ 8766+ 8766+ 8766+ 4321 8766+ 8766+ ## [4201] 8392 8766+ 7505 6598 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 4949 8766+ ## [4213] 8766+ 8766+ 8766+ 8766+ 310 8766+ 8766+ 8766+ 8766+ 3790 8766+ 6678 ## [4225] 6851 8766+ 8766+ 8766+ 8766+ 8766+ 7563 8766+ 6518 8766+ 8766+ 8766+ ## [4237] 8766+ 7884 5921 5245 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8020 8766+ ## [4249] 5953 8766+ 1240 8766+ 8766+ 4896 1886 8766+ 8766+ 8766+ 8766+ 8766+ ## [4261] 1100 2549 1607 8766+ 8766+ 8766+ 2345 8766+ 7985 8766+ 8766+ 8264 ## [4273] 8766+ 3288 8766+ 8766+ 6136 8766+ 8766+ 8766+ 8766+ 2159 3973 8577 ## [4285] 7117 8766+ 8766+ 8060 5024 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ ## [4297] 7077 4999 6092 6258 8766+ 7092 8766+ 182 5778 8766+ 8766+ 5269 ## [4309] 2869 8766+ 8766+ 5581 885 8766+ 6502 8766+ 8766+ 4442 8766+ 8766+ ## [4321] 8766+ 5906 8766+ 8766+ 8766+ 8766+ 8766+ 4788 8766+ 8766+ 8766+ 8766+ ## [4333] 3882 6411 6833 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 8766+ 7306 8766+ ## [4345] 6060 1644 8766+ 4776 8766+ 8766+ 6769 8766+ 8766+ 1466 8766+ 7389 ## [4357] 8766+ 8589 8766+ 8766+ 8766+ 8766+ 3765 8766+ 8766+ 8766+ 5756 8766+ ## [4369] 8766+ 8766+ 8766+ 8766+ 7472 8766+ 5136 5899 8766+ 8766+ 8766+ 8766+ ## [4381] 8766+ 334 4357 8766+ 6691 1788 938 1626 8766+ 8766+ 7363 6990 ## [4393] 8766+ 8766+ 8766+ 8766+ 5764 8766+ 8766+ 8766+ 2364 7923 6434 8766+ ## [4405] 8766+ 8501 2312 8766+ 5538 8766+ 5894 8766+ 8766+ 1552 8766+ 8494 ## [4417] 1911 8766+ 8766+ 8650 7362 4177 8766+ 6562 8766+ 8457 565 4300 ## [4429] 7746 6433 6729 8766+ 8766+ 8766+ The numbers assigned to each individual is their censoring times, with each number with a plus sign indicating that the participant was censored at that times without developing the outcome (haven’t failed/died), while those without the plus sign are the times at which participants developed the outcome (failure/death). library(survminer) ##for plotting survival plots with ggplot2 ## Loading required package: ggpubr fit1&lt;-survfit(Surv(timedth, death) ~ 1, data = fhs_first) summary(fit1) ## Call: survfit(formula = Surv(timedth, death) ~ 1, data = fhs_first) ## ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 26 4434 1 1.000 0.000226 0.999 1.000 ## 34 4433 1 1.000 0.000319 0.999 1.000 ## 40 4432 1 0.999 0.000390 0.999 1.000 ## 45 4431 1 0.999 0.000451 0.998 1.000 ## 46 4430 1 0.999 0.000504 0.998 1.000 ## 53 4429 1 0.999 0.000552 0.998 1.000 ## 58 4428 1 0.998 0.000596 0.997 1.000 ## 73 4427 1 0.998 0.000637 0.997 0.999 ## 87 4426 1 0.998 0.000676 0.997 0.999 ## 126 4425 1 0.998 0.000712 0.996 0.999 ## 133 4424 1 0.998 0.000747 0.996 0.999 ## 145 4423 1 0.997 0.000780 0.996 0.999 ## 146 4422 1 0.997 0.000812 0.995 0.999 ## 163 4421 1 0.997 0.000843 0.995 0.998 ## 168 4420 1 0.997 0.000872 0.995 0.998 ## 174 4419 1 0.996 0.000900 0.995 0.998 ## 178 4418 1 0.996 0.000928 0.994 0.998 ## 182 4417 1 0.996 0.000955 0.994 0.998 ## 184 4416 1 0.996 0.000981 0.994 0.998 ## 234 4415 1 0.995 0.001006 0.994 0.997 ## 266 4414 1 0.995 0.001031 0.993 0.997 ## 267 4413 1 0.995 0.001055 0.993 0.997 ## 282 4412 1 0.995 0.001079 0.993 0.997 ## 287 4411 1 0.995 0.001102 0.992 0.997 ## 305 4410 1 0.994 0.001124 0.992 0.997 ## 307 4409 1 0.994 0.001147 0.992 0.996 ## 310 4408 1 0.994 0.001168 0.992 0.996 ## 327 4407 1 0.994 0.001190 0.991 0.996 ## 331 4406 1 0.993 0.001211 0.991 0.996 ## 334 4405 1 0.993 0.001231 0.991 0.996 ## 339 4404 1 0.993 0.001251 0.991 0.995 ## 367 4403 1 0.993 0.001271 0.990 0.995 ## 378 4402 1 0.993 0.001291 0.990 0.995 ## 385 4401 1 0.992 0.001310 0.990 0.995 ## 415 4400 1 0.992 0.001329 0.990 0.995 ## 424 4399 1 0.992 0.001348 0.989 0.995 ## 429 4398 1 0.992 0.001366 0.989 0.994 ## 430 4397 2 0.991 0.001402 0.988 0.994 ## 442 4395 1 0.991 0.001420 0.988 0.994 ## 498 4394 1 0.991 0.001437 0.988 0.994 ## 526 4393 1 0.991 0.001455 0.988 0.993 ## 537 4392 2 0.990 0.001489 0.987 0.993 ## 544 4390 1 0.990 0.001505 0.987 0.993 ## 564 4389 1 0.990 0.001522 0.987 0.993 ## 565 4388 1 0.989 0.001538 0.986 0.992 ## 572 4387 1 0.989 0.001554 0.986 0.992 ## 584 4386 1 0.989 0.001570 0.986 0.992 ## 593 4385 1 0.989 0.001586 0.986 0.992 ## 607 4384 1 0.988 0.001601 0.985 0.992 ## 612 4383 1 0.988 0.001617 0.985 0.991 ## 623 4382 1 0.988 0.001632 0.985 0.991 ## 647 4381 1 0.988 0.001647 0.985 0.991 ## 672 4380 1 0.988 0.001662 0.984 0.991 ## 693 4379 1 0.987 0.001677 0.984 0.991 ## 700 4378 1 0.987 0.001692 0.984 0.990 ## 709 4377 1 0.987 0.001706 0.984 0.990 ## 717 4376 1 0.987 0.001721 0.983 0.990 ## 753 4375 1 0.986 0.001735 0.983 0.990 ## 756 4374 1 0.986 0.001749 0.983 0.990 ## 764 4373 1 0.986 0.001763 0.983 0.989 ## 771 4372 1 0.986 0.001777 0.982 0.989 ## 788 4371 1 0.986 0.001791 0.982 0.989 ## 803 4370 1 0.985 0.001805 0.982 0.989 ## 806 4369 1 0.985 0.001819 0.982 0.989 ## 815 4368 1 0.985 0.001832 0.981 0.988 ## 822 4367 1 0.985 0.001845 0.981 0.988 ## 827 4366 1 0.984 0.001859 0.981 0.988 ## 828 4365 1 0.984 0.001872 0.981 0.988 ## 832 4364 1 0.984 0.001885 0.980 0.988 ## 854 4363 1 0.984 0.001898 0.980 0.987 ## 858 4362 1 0.984 0.001911 0.980 0.987 ## 869 4361 1 0.983 0.001924 0.980 0.987 ## 885 4360 1 0.983 0.001937 0.979 0.987 ## 887 4359 1 0.983 0.001949 0.979 0.987 ## 889 4358 1 0.983 0.001962 0.979 0.986 ## 935 4357 1 0.982 0.001974 0.979 0.986 ## 938 4356 2 0.982 0.001999 0.978 0.986 ## 945 4354 1 0.982 0.002011 0.978 0.986 ## 946 4353 1 0.982 0.002023 0.978 0.985 ## 987 4352 1 0.981 0.002035 0.977 0.985 ## 992 4351 1 0.981 0.002047 0.977 0.985 ## 996 4350 1 0.981 0.002059 0.977 0.985 ## 1003 4349 1 0.981 0.002071 0.977 0.985 ## 1023 4348 1 0.980 0.002083 0.976 0.984 ## 1040 4347 1 0.980 0.002095 0.976 0.984 ## 1047 4346 1 0.980 0.002106 0.976 0.984 ## 1099 4345 1 0.980 0.002118 0.976 0.984 ## 1100 4344 1 0.979 0.002129 0.975 0.984 ## 1102 4343 1 0.979 0.002141 0.975 0.983 ## 1103 4342 1 0.979 0.002152 0.975 0.983 ## 1141 4341 1 0.979 0.002163 0.975 0.983 ## 1142 4340 1 0.979 0.002175 0.974 0.983 ## 1154 4339 1 0.978 0.002186 0.974 0.983 ## 1168 4338 1 0.978 0.002197 0.974 0.982 ## 1169 4337 1 0.978 0.002208 0.974 0.982 ## 1175 4336 1 0.978 0.002219 0.973 0.982 ## 1187 4335 1 0.977 0.002230 0.973 0.982 ## 1197 4334 2 0.977 0.002251 0.973 0.981 ## 1204 4332 1 0.977 0.002262 0.972 0.981 ## 1214 4331 1 0.977 0.002273 0.972 0.981 ## 1215 4330 1 0.976 0.002283 0.972 0.981 ## 1221 4329 1 0.976 0.002294 0.972 0.981 ## 1235 4328 1 0.976 0.002305 0.971 0.980 ## 1240 4327 1 0.976 0.002315 0.971 0.980 ## 1269 4326 1 0.975 0.002325 0.971 0.980 ## 1273 4325 1 0.975 0.002336 0.971 0.980 ## 1303 4324 1 0.975 0.002346 0.970 0.980 ## 1305 4323 1 0.975 0.002356 0.970 0.979 ## 1336 4322 1 0.975 0.002367 0.970 0.979 ## 1350 4321 1 0.974 0.002377 0.970 0.979 ## 1355 4320 1 0.974 0.002387 0.969 0.979 ## 1368 4319 1 0.974 0.002397 0.969 0.979 ## 1370 4318 1 0.974 0.002407 0.969 0.978 ## 1377 4317 1 0.973 0.002417 0.969 0.978 ## 1386 4316 1 0.973 0.002427 0.968 0.978 ## 1395 4315 1 0.973 0.002437 0.968 0.978 ## 1396 4314 1 0.973 0.002447 0.968 0.978 ## 1400 4313 1 0.972 0.002457 0.968 0.977 ## 1416 4312 1 0.972 0.002466 0.967 0.977 ## 1417 4311 1 0.972 0.002476 0.967 0.977 ## 1433 4310 1 0.972 0.002486 0.967 0.977 ## 1440 4309 2 0.971 0.002505 0.966 0.976 ## 1442 4307 1 0.971 0.002514 0.966 0.976 ## 1443 4306 1 0.971 0.002524 0.966 0.976 ## 1454 4305 1 0.971 0.002533 0.966 0.976 ## 1462 4304 1 0.970 0.002543 0.965 0.975 ## 1466 4303 1 0.970 0.002552 0.965 0.975 ## 1490 4302 1 0.970 0.002562 0.965 0.975 ## 1492 4301 1 0.970 0.002571 0.965 0.975 ## 1506 4300 1 0.970 0.002580 0.965 0.975 ## 1517 4299 1 0.969 0.002589 0.964 0.974 ## 1547 4298 2 0.969 0.002608 0.964 0.974 ## 1552 4296 1 0.969 0.002617 0.964 0.974 ## 1563 4295 1 0.968 0.002626 0.963 0.974 ## 1568 4294 1 0.968 0.002635 0.963 0.973 ## 1575 4293 1 0.968 0.002644 0.963 0.973 ## 1582 4292 1 0.968 0.002653 0.963 0.973 ## 1587 4291 2 0.967 0.002671 0.962 0.973 ## 1607 4289 2 0.967 0.002689 0.962 0.972 ## 1620 4287 1 0.967 0.002698 0.961 0.972 ## 1622 4286 1 0.966 0.002706 0.961 0.972 ## 1626 4285 1 0.966 0.002715 0.961 0.972 ## 1629 4284 1 0.966 0.002724 0.961 0.971 ## 1632 4283 1 0.966 0.002732 0.960 0.971 ## 1644 4282 2 0.965 0.002750 0.960 0.971 ## 1649 4280 1 0.965 0.002758 0.960 0.970 ## 1660 4279 1 0.965 0.002767 0.959 0.970 ## 1686 4278 1 0.965 0.002775 0.959 0.970 ## 1692 4277 1 0.964 0.002784 0.959 0.970 ## 1710 4276 1 0.964 0.002792 0.959 0.970 ## 1729 4275 1 0.964 0.002801 0.958 0.969 ## 1740 4274 1 0.964 0.002809 0.958 0.969 ## 1747 4273 1 0.963 0.002818 0.958 0.969 ## 1748 4272 1 0.963 0.002826 0.958 0.969 ## 1752 4271 1 0.963 0.002834 0.957 0.969 ## 1755 4270 1 0.963 0.002843 0.957 0.968 ## 1757 4269 1 0.963 0.002851 0.957 0.968 ## 1761 4268 2 0.962 0.002867 0.957 0.968 ## 1765 4266 1 0.962 0.002875 0.956 0.968 ## 1770 4265 1 0.962 0.002884 0.956 0.967 ## 1778 4264 1 0.961 0.002892 0.956 0.967 ## 1781 4263 1 0.961 0.002900 0.956 0.967 ## 1788 4262 1 0.961 0.002908 0.955 0.967 ## 1792 4261 1 0.961 0.002916 0.955 0.966 ## 1805 4260 1 0.961 0.002924 0.955 0.966 ## 1815 4259 1 0.960 0.002932 0.955 0.966 ## 1825 4258 1 0.960 0.002940 0.954 0.966 ## 1833 4257 1 0.960 0.002948 0.954 0.966 ## 1847 4256 1 0.960 0.002956 0.954 0.965 ## 1848 4255 1 0.959 0.002964 0.954 0.965 ## 1863 4254 1 0.959 0.002972 0.953 0.965 ## 1879 4253 1 0.959 0.002979 0.953 0.965 ## 1881 4252 1 0.959 0.002987 0.953 0.965 ## 1883 4251 1 0.959 0.002995 0.953 0.964 ## 1885 4250 1 0.958 0.003003 0.952 0.964 ## 1886 4249 1 0.958 0.003011 0.952 0.964 ## 1903 4248 1 0.958 0.003018 0.952 0.964 ## 1904 4247 1 0.958 0.003026 0.952 0.964 ## 1906 4246 1 0.957 0.003034 0.951 0.963 ## 1911 4245 1 0.957 0.003041 0.951 0.963 ## 1912 4244 1 0.957 0.003049 0.951 0.963 ## 1920 4243 1 0.957 0.003057 0.951 0.963 ## 1933 4242 1 0.956 0.003064 0.950 0.962 ## 1936 4241 1 0.956 0.003072 0.950 0.962 ## 1939 4240 1 0.956 0.003079 0.950 0.962 ## 1942 4239 1 0.956 0.003087 0.950 0.962 ## 1943 4238 1 0.956 0.003094 0.950 0.962 ## 1947 4237 1 0.955 0.003102 0.949 0.961 ## 1959 4236 1 0.955 0.003109 0.949 0.961 ## 1984 4235 1 0.955 0.003117 0.949 0.961 ## 1985 4234 1 0.955 0.003124 0.949 0.961 ## 1986 4233 1 0.954 0.003132 0.948 0.961 ## 1988 4232 1 0.954 0.003139 0.948 0.960 ## 1989 4231 1 0.954 0.003146 0.948 0.960 ## 2008 4230 1 0.954 0.003154 0.948 0.960 ## 2013 4229 1 0.954 0.003161 0.947 0.960 ## 2015 4228 1 0.953 0.003168 0.947 0.960 ## 2017 4227 1 0.953 0.003175 0.947 0.959 ## 2021 4226 1 0.953 0.003183 0.947 0.959 ## 2029 4225 1 0.953 0.003190 0.946 0.959 ## 2042 4224 1 0.952 0.003197 0.946 0.959 ## 2046 4223 1 0.952 0.003204 0.946 0.958 ## 2058 4222 1 0.952 0.003211 0.946 0.958 ## 2079 4221 1 0.952 0.003219 0.945 0.958 ## 2104 4220 1 0.952 0.003226 0.945 0.958 ## 2105 4219 1 0.951 0.003233 0.945 0.958 ## 2112 4218 1 0.951 0.003240 0.945 0.957 ## 2116 4217 1 0.951 0.003247 0.944 0.957 ## 2117 4216 1 0.951 0.003254 0.944 0.957 ## 2122 4215 1 0.950 0.003261 0.944 0.957 ## 2138 4214 1 0.950 0.003268 0.944 0.957 ## 2141 4213 1 0.950 0.003275 0.944 0.956 ## 2142 4212 1 0.950 0.003282 0.943 0.956 ## 2143 4211 1 0.949 0.003289 0.943 0.956 ## 2146 4210 1 0.949 0.003296 0.943 0.956 ## 2148 4209 1 0.949 0.003303 0.943 0.956 ## 2157 4208 1 0.949 0.003310 0.942 0.955 ## 2159 4207 1 0.949 0.003317 0.942 0.955 ## 2160 4206 1 0.948 0.003324 0.942 0.955 ## 2192 4205 1 0.948 0.003330 0.942 0.955 ## 2193 4204 1 0.948 0.003337 0.941 0.954 ## 2194 4203 1 0.948 0.003344 0.941 0.954 ## 2205 4202 1 0.947 0.003351 0.941 0.954 ## 2218 4201 1 0.947 0.003358 0.941 0.954 ## 2230 4200 1 0.947 0.003364 0.940 0.954 ## 2235 4199 1 0.947 0.003371 0.940 0.953 ## 2243 4198 1 0.947 0.003378 0.940 0.953 ## 2247 4197 1 0.946 0.003385 0.940 0.953 ## 2270 4196 1 0.946 0.003391 0.939 0.953 ## 2285 4195 1 0.946 0.003398 0.939 0.953 ## 2287 4194 1 0.946 0.003405 0.939 0.952 ## 2289 4193 1 0.945 0.003411 0.939 0.952 ## 2298 4192 1 0.945 0.003418 0.939 0.952 ## 2311 4191 1 0.945 0.003425 0.938 0.952 ## 2312 4190 1 0.945 0.003431 0.938 0.951 ## 2329 4189 1 0.945 0.003438 0.938 0.951 ## 2335 4188 1 0.944 0.003444 0.938 0.951 ## 2345 4187 1 0.944 0.003451 0.937 0.951 ## 2346 4186 1 0.944 0.003457 0.937 0.951 ## 2348 4185 1 0.944 0.003464 0.937 0.950 ## 2364 4184 2 0.943 0.003477 0.936 0.950 ## 2365 4182 1 0.943 0.003483 0.936 0.950 ## 2389 4181 1 0.943 0.003490 0.936 0.950 ## 2391 4180 1 0.942 0.003496 0.936 0.949 ## 2395 4179 1 0.942 0.003503 0.935 0.949 ## 2397 4178 1 0.942 0.003509 0.935 0.949 ## 2400 4177 1 0.942 0.003516 0.935 0.949 ## 2406 4176 1 0.942 0.003522 0.935 0.949 ## 2420 4175 1 0.941 0.003528 0.934 0.948 ## 2424 4174 2 0.941 0.003541 0.934 0.948 ## 2425 4172 1 0.941 0.003547 0.934 0.948 ## 2434 4171 1 0.940 0.003554 0.934 0.947 ## 2453 4170 1 0.940 0.003560 0.933 0.947 ## 2457 4169 1 0.940 0.003566 0.933 0.947 ## 2462 4168 1 0.940 0.003573 0.933 0.947 ## 2465 4167 1 0.940 0.003579 0.933 0.947 ## 2474 4166 1 0.939 0.003585 0.932 0.946 ## 2478 4165 1 0.939 0.003591 0.932 0.946 ## 2481 4164 1 0.939 0.003597 0.932 0.946 ## 2488 4163 1 0.939 0.003604 0.932 0.946 ## 2503 4162 1 0.938 0.003610 0.931 0.946 ## 2520 4161 1 0.938 0.003616 0.931 0.945 ## 2531 4160 1 0.938 0.003622 0.931 0.945 ## 2540 4159 1 0.938 0.003628 0.931 0.945 ## 2549 4158 1 0.938 0.003634 0.930 0.945 ## 2550 4157 1 0.937 0.003641 0.930 0.944 ## 2562 4156 1 0.937 0.003647 0.930 0.944 ## 2567 4155 1 0.937 0.003653 0.930 0.944 ## 2568 4154 1 0.937 0.003659 0.929 0.944 ## 2570 4153 1 0.936 0.003665 0.929 0.944 ## 2573 4152 1 0.936 0.003671 0.929 0.943 ## 2580 4151 1 0.936 0.003677 0.929 0.943 ## 2587 4150 1 0.936 0.003683 0.929 0.943 ## 2589 4149 1 0.935 0.003689 0.928 0.943 ## 2590 4148 1 0.935 0.003695 0.928 0.943 ## 2601 4147 1 0.935 0.003701 0.928 0.942 ## 2606 4146 1 0.935 0.003707 0.928 0.942 ## 2609 4145 1 0.935 0.003713 0.927 0.942 ## 2614 4144 1 0.934 0.003719 0.927 0.942 ## 2618 4143 1 0.934 0.003725 0.927 0.941 ## 2620 4142 1 0.934 0.003731 0.927 0.941 ## 2634 4141 2 0.933 0.003743 0.926 0.941 ## 2641 4139 1 0.933 0.003748 0.926 0.941 ## 2666 4138 2 0.933 0.003760 0.925 0.940 ## 2674 4136 1 0.933 0.003766 0.925 0.940 ## 2677 4135 1 0.932 0.003772 0.925 0.940 ## 2682 4134 1 0.932 0.003778 0.925 0.940 ## 2697 4133 1 0.932 0.003783 0.925 0.939 ## 2707 4132 1 0.932 0.003789 0.924 0.939 ## 2708 4131 1 0.931 0.003795 0.924 0.939 ## 2719 4130 1 0.931 0.003801 0.924 0.939 ## 2720 4129 1 0.931 0.003807 0.924 0.938 ## 2738 4128 1 0.931 0.003812 0.923 0.938 ## 2749 4127 1 0.931 0.003818 0.923 0.938 ## 2752 4126 1 0.930 0.003824 0.923 0.938 ## 2759 4125 1 0.930 0.003830 0.923 0.938 ## 2784 4124 1 0.930 0.003835 0.922 0.937 ## 2796 4123 1 0.930 0.003841 0.922 0.937 ## 2807 4122 2 0.929 0.003852 0.922 0.937 ## 2812 4120 1 0.929 0.003858 0.921 0.937 ## 2818 4119 1 0.929 0.003864 0.921 0.936 ## 2826 4118 1 0.929 0.003869 0.921 0.936 ## 2830 4117 1 0.928 0.003875 0.921 0.936 ## 2841 4116 1 0.928 0.003880 0.920 0.936 ## 2857 4115 1 0.928 0.003886 0.920 0.935 ## 2864 4114 1 0.928 0.003892 0.920 0.935 ## 2869 4113 1 0.927 0.003897 0.920 0.935 ## 2891 4112 1 0.927 0.003903 0.920 0.935 ## 2896 4111 1 0.927 0.003908 0.919 0.935 ## 2937 4110 1 0.927 0.003914 0.919 0.934 ## 2948 4109 1 0.926 0.003919 0.919 0.934 ## 2956 4108 1 0.926 0.003925 0.919 0.934 ## 2960 4107 1 0.926 0.003931 0.918 0.934 ## 2964 4106 1 0.926 0.003936 0.918 0.934 ## 2965 4105 1 0.926 0.003942 0.918 0.933 ## 2971 4104 1 0.925 0.003947 0.918 0.933 ## 2975 4103 1 0.925 0.003953 0.917 0.933 ## 2980 4102 1 0.925 0.003958 0.917 0.933 ## 2981 4101 1 0.925 0.003963 0.917 0.932 ## 2983 4100 1 0.924 0.003969 0.917 0.932 ## 2984 4099 1 0.924 0.003974 0.916 0.932 ## 2990 4098 1 0.924 0.003980 0.916 0.932 ## 2993 4097 1 0.924 0.003985 0.916 0.932 ## 2995 4096 1 0.924 0.003991 0.916 0.931 ## 3017 4095 1 0.923 0.003996 0.916 0.931 ## 3025 4094 1 0.923 0.004001 0.915 0.931 ## 3032 4093 1 0.923 0.004007 0.915 0.931 ## 3065 4092 1 0.923 0.004012 0.915 0.931 ## 3076 4091 1 0.922 0.004017 0.915 0.930 ## 3077 4090 2 0.922 0.004028 0.914 0.930 ## 3091 4088 1 0.922 0.004033 0.914 0.930 ## 3109 4087 1 0.922 0.004039 0.914 0.929 ## 3128 4086 1 0.921 0.004044 0.913 0.929 ## 3129 4085 1 0.921 0.004049 0.913 0.929 ## 3130 4084 2 0.921 0.004060 0.913 0.929 ## 3132 4082 1 0.920 0.004065 0.912 0.928 ## 3133 4081 1 0.920 0.004070 0.912 0.928 ## 3147 4080 1 0.920 0.004076 0.912 0.928 ## 3156 4079 1 0.920 0.004081 0.912 0.928 ## 3161 4078 1 0.919 0.004086 0.912 0.928 ## 3173 4077 1 0.919 0.004091 0.911 0.927 ## 3176 4076 3 0.919 0.004107 0.911 0.927 ## 3184 4073 1 0.918 0.004112 0.910 0.926 ## 3208 4072 1 0.918 0.004117 0.910 0.926 ## 3211 4071 1 0.918 0.004122 0.910 0.926 ## 3219 4070 1 0.918 0.004128 0.910 0.926 ## 3247 4069 1 0.917 0.004133 0.909 0.926 ## 3251 4068 1 0.917 0.004138 0.909 0.925 ## 3253 4067 1 0.917 0.004143 0.909 0.925 ## 3269 4066 1 0.917 0.004148 0.909 0.925 ## 3272 4065 1 0.917 0.004153 0.908 0.925 ## 3273 4064 1 0.916 0.004158 0.908 0.925 ## 3282 4063 1 0.916 0.004163 0.908 0.924 ## 3288 4062 1 0.916 0.004168 0.908 0.924 ## 3295 4061 1 0.916 0.004174 0.908 0.924 ## 3315 4060 1 0.915 0.004179 0.907 0.924 ## 3337 4059 1 0.915 0.004184 0.907 0.923 ## 3339 4058 1 0.915 0.004189 0.907 0.923 ## 3348 4057 1 0.915 0.004194 0.907 0.923 ## 3352 4056 1 0.915 0.004199 0.906 0.923 ## 3356 4055 1 0.914 0.004204 0.906 0.923 ## 3368 4054 1 0.914 0.004209 0.906 0.922 ## 3379 4053 1 0.914 0.004214 0.906 0.922 ## 3391 4052 1 0.914 0.004219 0.905 0.922 ## 3394 4051 1 0.913 0.004224 0.905 0.922 ## 3405 4050 1 0.913 0.004229 0.905 0.921 ## 3406 4049 1 0.913 0.004234 0.905 0.921 ## 3407 4048 1 0.913 0.004239 0.904 0.921 ## 3413 4047 1 0.912 0.004244 0.904 0.921 ## 3416 4046 1 0.912 0.004249 0.904 0.921 ## 3429 4045 2 0.912 0.004258 0.904 0.920 ## 3434 4043 1 0.912 0.004263 0.903 0.920 ## 3450 4042 1 0.911 0.004268 0.903 0.920 ## 3455 4041 1 0.911 0.004273 0.903 0.920 ## 3459 4040 1 0.911 0.004278 0.903 0.919 ## 3460 4039 1 0.911 0.004283 0.902 0.919 ## 3463 4038 1 0.910 0.004288 0.902 0.919 ## 3471 4037 1 0.910 0.004293 0.902 0.919 ## 3488 4036 1 0.910 0.004297 0.902 0.918 ## 3489 4035 1 0.910 0.004302 0.901 0.918 ## 3490 4034 1 0.910 0.004307 0.901 0.918 ## 3503 4033 1 0.909 0.004312 0.901 0.918 ## 3510 4032 1 0.909 0.004317 0.901 0.918 ## 3514 4031 1 0.909 0.004322 0.900 0.917 ## 3518 4030 1 0.909 0.004326 0.900 0.917 ## 3536 4029 1 0.908 0.004331 0.900 0.917 ## 3538 4028 1 0.908 0.004336 0.900 0.917 ## 3549 4027 1 0.908 0.004341 0.900 0.917 ## 3550 4026 1 0.908 0.004346 0.899 0.916 ## 3564 4025 1 0.908 0.004350 0.899 0.916 ## 3565 4024 1 0.907 0.004355 0.899 0.916 ## 3568 4023 1 0.907 0.004360 0.899 0.916 ## 3571 4022 1 0.907 0.004365 0.898 0.915 ## 3573 4021 2 0.906 0.004374 0.898 0.915 ## 3587 4019 2 0.906 0.004384 0.897 0.915 ## 3591 4017 1 0.906 0.004388 0.897 0.914 ## 3595 4016 1 0.906 0.004393 0.897 0.914 ## 3600 4015 1 0.905 0.004398 0.897 0.914 ## 3601 4014 1 0.905 0.004402 0.896 0.914 ## 3607 4013 1 0.905 0.004407 0.896 0.914 ## 3619 4012 1 0.905 0.004412 0.896 0.913 ## 3622 4011 1 0.904 0.004416 0.896 0.913 ## 3624 4010 1 0.904 0.004421 0.896 0.913 ## 3625 4009 1 0.904 0.004426 0.895 0.913 ## 3632 4008 1 0.904 0.004430 0.895 0.912 ## 3640 4007 1 0.903 0.004435 0.895 0.912 ## 3646 4006 1 0.903 0.004440 0.895 0.912 ## 3655 4005 2 0.903 0.004449 0.894 0.912 ## 3657 4003 1 0.903 0.004453 0.894 0.911 ## 3660 4002 1 0.902 0.004458 0.894 0.911 ## 3672 4001 1 0.902 0.004463 0.893 0.911 ## 3683 4000 2 0.902 0.004472 0.893 0.910 ## 3692 3998 1 0.901 0.004476 0.893 0.910 ## 3694 3997 1 0.901 0.004481 0.892 0.910 ## 3698 3996 1 0.901 0.004485 0.892 0.910 ## 3704 3995 1 0.901 0.004490 0.892 0.910 ## 3716 3994 1 0.901 0.004494 0.892 0.909 ## 3721 3993 1 0.900 0.004499 0.892 0.909 ## 3726 3992 1 0.900 0.004503 0.891 0.909 ## 3728 3991 1 0.900 0.004508 0.891 0.909 ## 3730 3990 1 0.900 0.004513 0.891 0.909 ## 3731 3989 1 0.899 0.004517 0.891 0.908 ## 3738 3988 1 0.899 0.004522 0.890 0.908 ## 3752 3987 1 0.899 0.004526 0.890 0.908 ## 3758 3986 1 0.899 0.004530 0.890 0.908 ## 3760 3985 1 0.899 0.004535 0.890 0.907 ## 3763 3984 1 0.898 0.004539 0.889 0.907 ## 3765 3983 1 0.898 0.004544 0.889 0.907 ## 3773 3982 1 0.898 0.004548 0.889 0.907 ## 3774 3981 1 0.898 0.004553 0.889 0.907 ## 3776 3980 2 0.897 0.004562 0.888 0.906 ## 3777 3978 1 0.897 0.004566 0.888 0.906 ## 3779 3977 1 0.897 0.004570 0.888 0.906 ## 3789 3976 1 0.896 0.004575 0.888 0.905 ## 3790 3975 1 0.896 0.004579 0.887 0.905 ## 3796 3974 1 0.896 0.004584 0.887 0.905 ## 3797 3973 1 0.896 0.004588 0.887 0.905 ## 3802 3972 2 0.895 0.004597 0.886 0.904 ## 3807 3970 1 0.895 0.004601 0.886 0.904 ## 3833 3969 1 0.895 0.004606 0.886 0.904 ## 3837 3968 1 0.895 0.004610 0.886 0.904 ## 3840 3967 1 0.894 0.004614 0.885 0.904 ## 3848 3966 1 0.894 0.004619 0.885 0.903 ## 3851 3965 1 0.894 0.004623 0.885 0.903 ## 3859 3964 1 0.894 0.004627 0.885 0.903 ## 3873 3963 1 0.894 0.004632 0.885 0.903 ## 3878 3962 1 0.893 0.004636 0.884 0.902 ## 3882 3961 1 0.893 0.004640 0.884 0.902 ## 3888 3960 2 0.893 0.004649 0.884 0.902 ## 3893 3958 1 0.892 0.004653 0.883 0.902 ## 3898 3957 2 0.892 0.004662 0.883 0.901 ## 3910 3955 1 0.892 0.004666 0.883 0.901 ## 3914 3954 1 0.892 0.004670 0.882 0.901 ## 3932 3953 1 0.891 0.004675 0.882 0.901 ## 3941 3952 1 0.891 0.004679 0.882 0.900 ## 3944 3951 1 0.891 0.004683 0.882 0.900 ## 3945 3950 2 0.890 0.004692 0.881 0.900 ## 3953 3948 2 0.890 0.004700 0.881 0.899 ## 3970 3946 1 0.890 0.004704 0.881 0.899 ## 3973 3945 1 0.889 0.004708 0.880 0.899 ## 3981 3944 1 0.889 0.004713 0.880 0.899 ## 3998 3943 1 0.889 0.004717 0.880 0.898 ## 3999 3942 1 0.889 0.004721 0.880 0.898 ## 4011 3941 1 0.889 0.004725 0.879 0.898 ## 4019 3940 1 0.888 0.004729 0.879 0.898 ## 4024 3939 1 0.888 0.004734 0.879 0.897 ## 4028 3938 1 0.888 0.004738 0.879 0.897 ## 4034 3937 1 0.888 0.004742 0.878 0.897 ## 4036 3936 1 0.887 0.004746 0.878 0.897 ## 4041 3935 1 0.887 0.004750 0.878 0.897 ## 4050 3934 1 0.887 0.004754 0.878 0.896 ## 4051 3933 1 0.887 0.004758 0.878 0.896 ## 4053 3932 1 0.887 0.004763 0.877 0.896 ## 4060 3931 1 0.886 0.004767 0.877 0.896 ## 4082 3930 2 0.886 0.004775 0.877 0.895 ## 4090 3928 1 0.886 0.004779 0.876 0.895 ## 4097 3927 1 0.885 0.004783 0.876 0.895 ## 4102 3926 1 0.885 0.004787 0.876 0.895 ## 4104 3925 1 0.885 0.004791 0.876 0.894 ## 4118 3924 1 0.885 0.004795 0.875 0.894 ## 4119 3923 1 0.885 0.004799 0.875 0.894 ## 4127 3922 1 0.884 0.004804 0.875 0.894 ## 4128 3921 1 0.884 0.004808 0.875 0.894 ## 4142 3920 1 0.884 0.004812 0.874 0.893 ## 4145 3919 2 0.883 0.004820 0.874 0.893 ## 4146 3917 1 0.883 0.004824 0.874 0.893 ## 4154 3916 2 0.883 0.004832 0.873 0.892 ## 4158 3914 1 0.882 0.004836 0.873 0.892 ## 4162 3913 1 0.882 0.004840 0.873 0.892 ## 4168 3912 1 0.882 0.004844 0.873 0.892 ## 4177 3911 1 0.882 0.004848 0.872 0.891 ## 4181 3910 1 0.882 0.004852 0.872 0.891 ## 4182 3909 1 0.881 0.004856 0.872 0.891 ## 4183 3908 1 0.881 0.004860 0.872 0.891 ## 4185 3907 1 0.881 0.004864 0.871 0.891 ## 4200 3906 1 0.881 0.004868 0.871 0.890 ## 4205 3905 1 0.880 0.004872 0.871 0.890 ## 4235 3904 1 0.880 0.004876 0.871 0.890 ## 4237 3903 1 0.880 0.004880 0.871 0.890 ## 4238 3902 1 0.880 0.004884 0.870 0.889 ## 4243 3901 1 0.880 0.004888 0.870 0.889 ## 4245 3900 1 0.879 0.004892 0.870 0.889 ## 4246 3899 1 0.879 0.004896 0.870 0.889 ## 4249 3898 1 0.879 0.004900 0.869 0.889 ## 4257 3897 1 0.879 0.004904 0.869 0.888 ## 4262 3896 1 0.878 0.004907 0.869 0.888 ## 4266 3895 1 0.878 0.004911 0.869 0.888 ## 4270 3894 1 0.878 0.004915 0.868 0.888 ## 4275 3893 1 0.878 0.004919 0.868 0.887 ## 4276 3892 1 0.878 0.004923 0.868 0.887 ## 4286 3891 1 0.877 0.004927 0.868 0.887 ## 4293 3890 1 0.877 0.004931 0.867 0.887 ## 4295 3889 1 0.877 0.004935 0.867 0.887 ## 4300 3888 1 0.877 0.004939 0.867 0.886 ## 4318 3887 1 0.876 0.004943 0.867 0.886 ## 4319 3886 1 0.876 0.004946 0.867 0.886 ## 4321 3885 1 0.876 0.004950 0.866 0.886 ## 4324 3884 1 0.876 0.004954 0.866 0.885 ## 4338 3883 1 0.876 0.004958 0.866 0.885 ## 4341 3882 1 0.875 0.004962 0.866 0.885 ## 4343 3881 1 0.875 0.004966 0.865 0.885 ## 4351 3880 1 0.875 0.004970 0.865 0.885 ## 4352 3879 1 0.875 0.004973 0.865 0.884 ## 4356 3878 1 0.874 0.004977 0.865 0.884 ## 4357 3877 1 0.874 0.004981 0.864 0.884 ## 4359 3876 2 0.874 0.004989 0.864 0.884 ## 4360 3874 1 0.873 0.004992 0.864 0.883 ## 4365 3873 1 0.873 0.004996 0.864 0.883 ## 4369 3872 1 0.873 0.005000 0.863 0.883 ## 4378 3871 1 0.873 0.005004 0.863 0.883 ## 4392 3870 1 0.873 0.005008 0.863 0.882 ## 4396 3869 1 0.872 0.005011 0.863 0.882 ## 4423 3868 1 0.872 0.005015 0.862 0.882 ## 4441 3867 1 0.872 0.005019 0.862 0.882 ## 4442 3866 2 0.871 0.005026 0.862 0.881 ## 4444 3864 1 0.871 0.005030 0.861 0.881 ## 4447 3863 1 0.871 0.005034 0.861 0.881 ## 4452 3862 1 0.871 0.005038 0.861 0.881 ## 4467 3861 1 0.871 0.005041 0.861 0.880 ## 4470 3860 1 0.870 0.005045 0.860 0.880 ## 4484 3859 2 0.870 0.005053 0.860 0.880 ## 4485 3857 1 0.870 0.005056 0.860 0.880 ## 4492 3856 1 0.869 0.005060 0.860 0.879 ## 4494 3855 2 0.869 0.005068 0.859 0.879 ## 4496 3853 1 0.869 0.005071 0.859 0.879 ## 4500 3852 1 0.869 0.005075 0.859 0.879 ## 4511 3851 1 0.868 0.005079 0.858 0.878 ## 4513 3850 1 0.868 0.005082 0.858 0.878 ## 4514 3849 1 0.868 0.005086 0.858 0.878 ## 4516 3848 1 0.868 0.005090 0.858 0.878 ## 4522 3847 1 0.867 0.005093 0.857 0.877 ## 4537 3846 1 0.867 0.005097 0.857 0.877 ## 4542 3845 1 0.867 0.005101 0.857 0.877 ## 4554 3844 1 0.867 0.005104 0.857 0.877 ## 4560 3843 2 0.866 0.005112 0.856 0.876 ## 4561 3841 1 0.866 0.005115 0.856 0.876 ## 4563 3840 1 0.866 0.005119 0.856 0.876 ## 4564 3839 1 0.866 0.005123 0.856 0.876 ## 4568 3838 1 0.865 0.005126 0.855 0.875 ## 4576 3837 2 0.865 0.005133 0.855 0.875 ## 4580 3835 1 0.865 0.005137 0.855 0.875 ## 4585 3834 1 0.864 0.005141 0.854 0.875 ## 4605 3833 1 0.864 0.005144 0.854 0.874 ## 4616 3832 1 0.864 0.005148 0.854 0.874 ## 4618 3831 1 0.864 0.005151 0.854 0.874 ## 4635 3830 1 0.864 0.005155 0.854 0.874 ## 4637 3829 1 0.863 0.005159 0.853 0.873 ## 4643 3828 1 0.863 0.005162 0.853 0.873 ## 4652 3827 1 0.863 0.005166 0.853 0.873 ## 4653 3826 1 0.863 0.005169 0.853 0.873 ## 4654 3825 1 0.862 0.005173 0.852 0.873 ## 4659 3824 1 0.862 0.005176 0.852 0.872 ## 4677 3823 1 0.862 0.005180 0.852 0.872 ## 4683 3822 1 0.862 0.005184 0.852 0.872 ## 4684 3821 1 0.862 0.005187 0.851 0.872 ## 4685 3820 1 0.861 0.005191 0.851 0.872 ## 4700 3819 1 0.861 0.005194 0.851 0.871 ## 4705 3818 1 0.861 0.005198 0.851 0.871 ## 4714 3817 1 0.861 0.005201 0.850 0.871 ## 4719 3816 1 0.860 0.005205 0.850 0.871 ## 4723 3815 1 0.860 0.005208 0.850 0.870 ## 4725 3814 1 0.860 0.005212 0.850 0.870 ## 4728 3813 1 0.860 0.005215 0.850 0.870 ## 4729 3812 1 0.859 0.005219 0.849 0.870 ## 4730 3811 1 0.859 0.005222 0.849 0.870 ## 4753 3810 1 0.859 0.005226 0.849 0.869 ## 4757 3809 1 0.859 0.005229 0.849 0.869 ## 4758 3808 1 0.859 0.005233 0.848 0.869 ## 4762 3807 1 0.858 0.005236 0.848 0.869 ## 4763 3806 1 0.858 0.005240 0.848 0.868 ## 4775 3805 1 0.858 0.005243 0.848 0.868 ## 4776 3804 1 0.858 0.005247 0.847 0.868 ## 4780 3803 1 0.857 0.005250 0.847 0.868 ## 4788 3802 1 0.857 0.005254 0.847 0.868 ## 4792 3801 1 0.857 0.005257 0.847 0.867 ## 4793 3800 1 0.857 0.005261 0.847 0.867 ## 4808 3799 1 0.857 0.005264 0.846 0.867 ## 4817 3798 1 0.856 0.005267 0.846 0.867 ## 4820 3797 1 0.856 0.005271 0.846 0.867 ## 4844 3796 1 0.856 0.005274 0.846 0.866 ## 4845 3795 1 0.856 0.005278 0.845 0.866 ## 4854 3794 1 0.855 0.005281 0.845 0.866 ## 4860 3793 1 0.855 0.005285 0.845 0.866 ## 4867 3792 1 0.855 0.005288 0.845 0.865 ## 4870 3791 1 0.855 0.005291 0.844 0.865 ## 4873 3790 1 0.855 0.005295 0.844 0.865 ## 4875 3789 1 0.854 0.005298 0.844 0.865 ## 4878 3788 1 0.854 0.005302 0.844 0.865 ## 4882 3787 1 0.854 0.005305 0.844 0.864 ## 4887 3786 1 0.854 0.005308 0.843 0.864 ## 4890 3785 1 0.853 0.005312 0.843 0.864 ## 4894 3784 1 0.853 0.005315 0.843 0.864 ## 4895 3783 1 0.853 0.005319 0.843 0.863 ## 4896 3782 1 0.853 0.005322 0.842 0.863 ## 4900 3781 1 0.853 0.005325 0.842 0.863 ## 4903 3780 1 0.852 0.005329 0.842 0.863 ## 4907 3779 2 0.852 0.005335 0.841 0.862 ## 4915 3777 1 0.852 0.005339 0.841 0.862 ## 4921 3776 1 0.851 0.005342 0.841 0.862 ## 4924 3775 1 0.851 0.005345 0.841 0.862 ## 4931 3774 1 0.851 0.005349 0.841 0.861 ## 4934 3773 1 0.851 0.005352 0.840 0.861 ## 4936 3772 1 0.850 0.005355 0.840 0.861 ## 4949 3771 1 0.850 0.005359 0.840 0.861 ## 4961 3770 1 0.850 0.005362 0.840 0.861 ## 4967 3769 1 0.850 0.005365 0.839 0.860 ## 4969 3768 1 0.850 0.005369 0.839 0.860 ## 4972 3767 1 0.849 0.005372 0.839 0.860 ## 4975 3766 1 0.849 0.005375 0.839 0.860 ## 4977 3765 1 0.849 0.005379 0.838 0.860 ## 4984 3764 1 0.849 0.005382 0.838 0.859 ## 4988 3763 1 0.848 0.005385 0.838 0.859 ## 4995 3762 1 0.848 0.005388 0.838 0.859 ## 4999 3761 1 0.848 0.005392 0.837 0.859 ## 5000 3760 1 0.848 0.005395 0.837 0.858 ## 5002 3759 1 0.848 0.005398 0.837 0.858 ## 5007 3758 1 0.847 0.005402 0.837 0.858 ## 5009 3757 1 0.847 0.005405 0.837 0.858 ## 5010 3756 1 0.847 0.005408 0.836 0.858 ## 5011 3755 1 0.847 0.005411 0.836 0.857 ## 5017 3754 1 0.846 0.005415 0.836 0.857 ## 5020 3753 1 0.846 0.005418 0.836 0.857 ## 5024 3752 1 0.846 0.005421 0.835 0.857 ## 5026 3751 1 0.846 0.005424 0.835 0.856 ## 5030 3750 1 0.846 0.005428 0.835 0.856 ## 5035 3749 1 0.845 0.005431 0.835 0.856 ## 5040 3748 2 0.845 0.005437 0.834 0.856 ## 5062 3746 1 0.845 0.005441 0.834 0.855 ## 5065 3745 1 0.844 0.005444 0.834 0.855 ## 5066 3744 1 0.844 0.005447 0.834 0.855 ## 5071 3743 1 0.844 0.005450 0.833 0.855 ## 5072 3742 1 0.844 0.005453 0.833 0.854 ## 5081 3741 1 0.843 0.005457 0.833 0.854 ## 5085 3740 1 0.843 0.005460 0.833 0.854 ## 5088 3739 1 0.843 0.005463 0.832 0.854 ## 5095 3738 1 0.843 0.005466 0.832 0.854 ## 5098 3737 1 0.843 0.005469 0.832 0.853 ## 5101 3736 1 0.842 0.005473 0.832 0.853 ## 5119 3735 1 0.842 0.005476 0.831 0.853 ## 5122 3734 1 0.842 0.005479 0.831 0.853 ## 5123 3733 1 0.842 0.005482 0.831 0.852 ## 5127 3732 1 0.841 0.005485 0.831 0.852 ## 5128 3731 1 0.841 0.005488 0.831 0.852 ## 5131 3730 1 0.841 0.005492 0.830 0.852 ## 5136 3729 1 0.841 0.005495 0.830 0.852 ## 5137 3728 1 0.841 0.005498 0.830 0.851 ## 5139 3727 1 0.840 0.005501 0.830 0.851 ## 5145 3726 2 0.840 0.005507 0.829 0.851 ## 5150 3724 1 0.840 0.005510 0.829 0.851 ## 5154 3723 2 0.839 0.005517 0.828 0.850 ## 5157 3721 1 0.839 0.005520 0.828 0.850 ## 5159 3720 1 0.839 0.005523 0.828 0.850 ## 5167 3719 1 0.839 0.005526 0.828 0.849 ## 5169 3718 1 0.838 0.005529 0.828 0.849 ## 5181 3717 1 0.838 0.005532 0.827 0.849 ## 5183 3716 1 0.838 0.005535 0.827 0.849 ## 5189 3715 1 0.838 0.005539 0.827 0.849 ## 5194 3714 1 0.837 0.005542 0.827 0.848 ## 5200 3713 1 0.837 0.005545 0.826 0.848 ## 5206 3712 1 0.837 0.005548 0.826 0.848 ## 5226 3711 1 0.837 0.005551 0.826 0.848 ## 5229 3710 1 0.836 0.005554 0.826 0.847 ## 5232 3709 1 0.836 0.005557 0.825 0.847 ## 5245 3708 1 0.836 0.005560 0.825 0.847 ## 5252 3707 3 0.835 0.005569 0.825 0.846 ## 5263 3704 1 0.835 0.005572 0.824 0.846 ## 5265 3703 1 0.835 0.005575 0.824 0.846 ## 5269 3702 2 0.834 0.005582 0.824 0.845 ## 5272 3700 2 0.834 0.005588 0.823 0.845 ## 5276 3698 1 0.834 0.005591 0.823 0.845 ## 5283 3697 1 0.834 0.005594 0.823 0.845 ## 5296 3696 1 0.833 0.005597 0.822 0.844 ## 5309 3695 1 0.833 0.005600 0.822 0.844 ## 5313 3694 1 0.833 0.005603 0.822 0.844 ## 5316 3693 1 0.833 0.005606 0.822 0.844 ## 5320 3692 2 0.832 0.005612 0.821 0.843 ## 5321 3690 1 0.832 0.005615 0.821 0.843 ## 5322 3689 3 0.831 0.005624 0.820 0.842 ## 5330 3686 1 0.831 0.005627 0.820 0.842 ## 5337 3685 1 0.831 0.005630 0.820 0.842 ## 5351 3684 2 0.830 0.005636 0.819 0.842 ## 5353 3682 1 0.830 0.005639 0.819 0.841 ## 5362 3681 1 0.830 0.005642 0.819 0.841 ## 5375 3680 1 0.830 0.005645 0.819 0.841 ## 5376 3679 1 0.829 0.005648 0.819 0.841 ## 5380 3678 1 0.829 0.005651 0.818 0.840 ## 5384 3677 2 0.829 0.005657 0.818 0.840 ## 5387 3675 1 0.829 0.005660 0.818 0.840 ## 5396 3674 1 0.828 0.005663 0.817 0.840 ## 5399 3673 1 0.828 0.005665 0.817 0.839 ## 5400 3672 1 0.828 0.005668 0.817 0.839 ## 5410 3671 1 0.828 0.005671 0.817 0.839 ## 5413 3670 2 0.827 0.005677 0.816 0.838 ## 5428 3668 1 0.827 0.005680 0.816 0.838 ## 5437 3667 2 0.827 0.005686 0.815 0.838 ## 5439 3665 2 0.826 0.005692 0.815 0.837 ## 5444 3663 1 0.826 0.005695 0.815 0.837 ## 5446 3662 1 0.826 0.005698 0.815 0.837 ## 5448 3661 1 0.825 0.005701 0.814 0.837 ## 5452 3660 1 0.825 0.005703 0.814 0.836 ## 5458 3659 1 0.825 0.005706 0.814 0.836 ## 5466 3658 1 0.825 0.005709 0.814 0.836 ## 5475 3657 1 0.825 0.005712 0.813 0.836 ## 5490 3656 1 0.824 0.005715 0.813 0.836 ## 5496 3655 1 0.824 0.005718 0.813 0.835 ## 5499 3654 2 0.824 0.005724 0.812 0.835 ## 5515 3652 1 0.823 0.005727 0.812 0.835 ## 5518 3651 1 0.823 0.005729 0.812 0.834 ## 5526 3650 2 0.823 0.005735 0.812 0.834 ## 5527 3648 1 0.823 0.005738 0.811 0.834 ## 5530 3647 1 0.822 0.005741 0.811 0.834 ## 5538 3646 2 0.822 0.005747 0.811 0.833 ## 5542 3644 1 0.822 0.005749 0.810 0.833 ## 5545 3643 1 0.821 0.005752 0.810 0.833 ## 5553 3642 1 0.821 0.005755 0.810 0.833 ## 5554 3641 1 0.821 0.005758 0.810 0.832 ## 5556 3640 1 0.821 0.005761 0.809 0.832 ## 5565 3639 1 0.820 0.005764 0.809 0.832 ## 5569 3638 1 0.820 0.005766 0.809 0.832 ## 5572 3637 1 0.820 0.005769 0.809 0.831 ## 5579 3636 1 0.820 0.005772 0.809 0.831 ## 5581 3635 2 0.819 0.005778 0.808 0.831 ## 5589 3633 1 0.819 0.005781 0.808 0.831 ## 5590 3632 1 0.819 0.005783 0.808 0.830 ## 5591 3631 1 0.819 0.005786 0.807 0.830 ## 5592 3630 1 0.818 0.005789 0.807 0.830 ## 5597 3629 1 0.818 0.005792 0.807 0.830 ## 5600 3628 2 0.818 0.005797 0.806 0.829 ## 5604 3626 1 0.818 0.005800 0.806 0.829 ## 5611 3625 1 0.817 0.005803 0.806 0.829 ## 5616 3624 1 0.817 0.005806 0.806 0.829 ## 5618 3623 1 0.817 0.005808 0.806 0.828 ## 5619 3622 1 0.817 0.005811 0.805 0.828 ## 5620 3621 1 0.816 0.005814 0.805 0.828 ## 5621 3620 1 0.816 0.005817 0.805 0.828 ## 5623 3619 1 0.816 0.005820 0.805 0.827 ## 5629 3618 2 0.816 0.005825 0.804 0.827 ## 5638 3616 1 0.815 0.005828 0.804 0.827 ## 5641 3615 1 0.815 0.005831 0.804 0.827 ## 5644 3614 1 0.815 0.005833 0.803 0.826 ## 5655 3613 2 0.814 0.005839 0.803 0.826 ## 5660 3611 1 0.814 0.005841 0.803 0.826 ## 5667 3610 1 0.814 0.005844 0.803 0.825 ## 5674 3609 1 0.814 0.005847 0.802 0.825 ## 5685 3608 1 0.813 0.005850 0.802 0.825 ## 5687 3607 2 0.813 0.005855 0.802 0.825 ## 5693 3605 1 0.813 0.005858 0.801 0.824 ## 5702 3604 1 0.813 0.005861 0.801 0.824 ## 5709 3603 1 0.812 0.005863 0.801 0.824 ## 5714 3602 1 0.812 0.005866 0.801 0.824 ## 5722 3601 2 0.812 0.005871 0.800 0.823 ## 5727 3599 1 0.811 0.005874 0.800 0.823 ## 5731 3598 1 0.811 0.005877 0.800 0.823 ## 5732 3597 2 0.811 0.005882 0.799 0.822 ## 5733 3595 1 0.811 0.005885 0.799 0.822 ## 5740 3594 1 0.810 0.005888 0.799 0.822 ## 5746 3593 1 0.810 0.005890 0.799 0.822 ## 5755 3592 1 0.810 0.005893 0.798 0.822 ## 5756 3591 1 0.810 0.005896 0.798 0.821 ## 5761 3590 1 0.809 0.005898 0.798 0.821 ## 5764 3589 2 0.809 0.005904 0.797 0.821 ## 5769 3587 1 0.809 0.005906 0.797 0.820 ## 5773 3586 1 0.809 0.005909 0.797 0.820 ## 5775 3585 1 0.808 0.005912 0.797 0.820 ## 5778 3584 1 0.808 0.005914 0.797 0.820 ## 5786 3583 1 0.808 0.005917 0.796 0.820 ## 5788 3582 1 0.808 0.005919 0.796 0.819 ## 5789 3581 1 0.807 0.005922 0.796 0.819 ## 5794 3580 1 0.807 0.005925 0.796 0.819 ## 5806 3579 1 0.807 0.005927 0.795 0.819 ## 5813 3578 1 0.807 0.005930 0.795 0.818 ## 5816 3577 1 0.806 0.005933 0.795 0.818 ## 5820 3576 1 0.806 0.005935 0.795 0.818 ## 5839 3575 1 0.806 0.005938 0.794 0.818 ## 5843 3574 2 0.806 0.005943 0.794 0.817 ## 5861 3572 1 0.805 0.005946 0.794 0.817 ## 5867 3571 1 0.805 0.005948 0.794 0.817 ## 5870 3570 1 0.805 0.005951 0.793 0.817 ## 5876 3569 1 0.805 0.005954 0.793 0.816 ## 5878 3568 1 0.804 0.005956 0.793 0.816 ## 5879 3567 1 0.804 0.005959 0.793 0.816 ## 5883 3566 1 0.804 0.005961 0.792 0.816 ## 5885 3565 1 0.804 0.005964 0.792 0.816 ## 5888 3564 1 0.804 0.005967 0.792 0.815 ## 5891 3563 1 0.803 0.005969 0.792 0.815 ## 5894 3562 1 0.803 0.005972 0.791 0.815 ## 5898 3561 1 0.803 0.005974 0.791 0.815 ## 5899 3560 1 0.803 0.005977 0.791 0.814 ## 5900 3559 1 0.802 0.005979 0.791 0.814 ## 5901 3558 1 0.802 0.005982 0.791 0.814 ## 5905 3557 1 0.802 0.005985 0.790 0.814 ## 5906 3556 2 0.802 0.005990 0.790 0.813 ## 5908 3554 1 0.801 0.005992 0.790 0.813 ## 5911 3553 1 0.801 0.005995 0.789 0.813 ## 5921 3552 1 0.801 0.005997 0.789 0.813 ## 5930 3551 1 0.801 0.006000 0.789 0.812 ## 5931 3550 2 0.800 0.006005 0.788 0.812 ## 5936 3548 1 0.800 0.006008 0.788 0.812 ## 5953 3547 1 0.800 0.006010 0.788 0.812 ## 5954 3546 1 0.800 0.006013 0.788 0.811 ## 5958 3545 1 0.799 0.006015 0.788 0.811 ## 5962 3544 1 0.799 0.006018 0.787 0.811 ## 5973 3543 1 0.799 0.006020 0.787 0.811 ## 5977 3542 1 0.799 0.006023 0.787 0.810 ## 5992 3541 1 0.798 0.006025 0.787 0.810 ## 5997 3540 1 0.798 0.006028 0.786 0.810 ## 6001 3539 2 0.798 0.006033 0.786 0.810 ## 6002 3537 1 0.797 0.006035 0.786 0.809 ## 6006 3536 1 0.797 0.006038 0.786 0.809 ## 6007 3535 1 0.797 0.006040 0.785 0.809 ## 6019 3534 1 0.797 0.006043 0.785 0.809 ## 6020 3533 1 0.797 0.006045 0.785 0.809 ## 6024 3532 1 0.796 0.006048 0.785 0.808 ## 6026 3531 1 0.796 0.006050 0.784 0.808 ## 6028 3530 1 0.796 0.006053 0.784 0.808 ## 6029 3529 1 0.796 0.006055 0.784 0.808 ## 6030 3528 1 0.795 0.006058 0.784 0.807 ## 6034 3527 1 0.795 0.006060 0.783 0.807 ## 6036 3526 1 0.795 0.006063 0.783 0.807 ## 6049 3525 1 0.795 0.006065 0.783 0.807 ## 6060 3524 1 0.795 0.006068 0.783 0.807 ## 6063 3523 1 0.794 0.006070 0.783 0.806 ## 6070 3522 1 0.794 0.006073 0.782 0.806 ## 6078 3521 1 0.794 0.006075 0.782 0.806 ## 6080 3520 1 0.794 0.006078 0.782 0.806 ## 6084 3519 1 0.793 0.006080 0.782 0.805 ## 6085 3518 1 0.793 0.006082 0.781 0.805 ## 6092 3517 1 0.793 0.006085 0.781 0.805 ## 6104 3516 1 0.793 0.006087 0.781 0.805 ## 6124 3515 1 0.793 0.006090 0.781 0.805 ## 6129 3514 1 0.792 0.006092 0.780 0.804 ## 6130 3513 1 0.792 0.006095 0.780 0.804 ## 6131 3512 1 0.792 0.006097 0.780 0.804 ## 6132 3511 1 0.792 0.006100 0.780 0.804 ## 6136 3510 1 0.791 0.006102 0.780 0.803 ## 6144 3509 1 0.791 0.006104 0.779 0.803 ## 6152 3508 1 0.791 0.006107 0.779 0.803 ## 6155 3507 1 0.791 0.006109 0.779 0.803 ## 6164 3506 1 0.790 0.006112 0.779 0.803 ## 6170 3505 1 0.790 0.006114 0.778 0.802 ## 6176 3504 1 0.790 0.006116 0.778 0.802 ## 6180 3503 1 0.790 0.006119 0.778 0.802 ## 6182 3502 1 0.790 0.006121 0.778 0.802 ## 6183 3501 1 0.789 0.006124 0.777 0.801 ## 6184 3500 1 0.789 0.006126 0.777 0.801 ## 6187 3499 2 0.789 0.006131 0.777 0.801 ## 6188 3497 1 0.788 0.006133 0.777 0.801 ## 6190 3496 1 0.788 0.006136 0.776 0.800 ## 6199 3495 1 0.788 0.006138 0.776 0.800 ## 6201 3494 2 0.788 0.006143 0.776 0.800 ## 6205 3492 1 0.787 0.006145 0.775 0.799 ## 6209 3491 1 0.787 0.006148 0.775 0.799 ## 6210 3490 1 0.787 0.006150 0.775 0.799 ## 6212 3489 1 0.787 0.006152 0.775 0.799 ## 6214 3488 1 0.786 0.006155 0.774 0.799 ## 6224 3487 1 0.786 0.006157 0.774 0.798 ## 6226 3486 1 0.786 0.006159 0.774 0.798 ## 6227 3485 1 0.786 0.006162 0.774 0.798 ## 6245 3484 1 0.786 0.006164 0.774 0.798 ## 6246 3483 1 0.785 0.006167 0.773 0.797 ## 6248 3482 1 0.785 0.006169 0.773 0.797 ## 6254 3481 1 0.785 0.006171 0.773 0.797 ## 6258 3480 1 0.785 0.006174 0.773 0.797 ## 6260 3479 1 0.784 0.006176 0.772 0.797 ## 6262 3478 1 0.784 0.006178 0.772 0.796 ## 6269 3477 1 0.784 0.006181 0.772 0.796 ## 6272 3476 1 0.784 0.006183 0.772 0.796 ## 6274 3475 1 0.783 0.006185 0.771 0.796 ## 6281 3474 1 0.783 0.006188 0.771 0.795 ## 6283 3473 1 0.783 0.006190 0.771 0.795 ## 6284 3472 1 0.783 0.006192 0.771 0.795 ## 6290 3471 1 0.783 0.006195 0.771 0.795 ## 6291 3470 1 0.782 0.006197 0.770 0.795 ## 6295 3469 1 0.782 0.006199 0.770 0.794 ## 6316 3468 1 0.782 0.006201 0.770 0.794 ## 6320 3467 1 0.782 0.006204 0.770 0.794 ## 6340 3466 1 0.781 0.006206 0.769 0.794 ## 6343 3465 1 0.781 0.006208 0.769 0.793 ## 6350 3464 1 0.781 0.006211 0.769 0.793 ## 6353 3463 1 0.781 0.006213 0.769 0.793 ## 6356 3462 1 0.781 0.006215 0.768 0.793 ## 6367 3461 1 0.780 0.006218 0.768 0.793 ## 6381 3460 1 0.780 0.006220 0.768 0.792 ## 6385 3459 1 0.780 0.006222 0.768 0.792 ## 6387 3458 1 0.780 0.006224 0.768 0.792 ## 6389 3457 1 0.779 0.006227 0.767 0.792 ## 6394 3456 1 0.779 0.006229 0.767 0.792 ## 6404 3455 1 0.779 0.006231 0.767 0.791 ## 6407 3454 1 0.779 0.006234 0.767 0.791 ## 6410 3453 2 0.778 0.006238 0.766 0.791 ## 6411 3451 2 0.778 0.006243 0.766 0.790 ## 6412 3449 2 0.777 0.006247 0.765 0.790 ## 6418 3447 1 0.777 0.006249 0.765 0.790 ## 6423 3446 1 0.777 0.006252 0.765 0.789 ## 6427 3445 1 0.777 0.006254 0.765 0.789 ## 6433 3444 1 0.776 0.006256 0.764 0.789 ## 6434 3443 2 0.776 0.006261 0.764 0.788 ## 6437 3441 1 0.776 0.006263 0.764 0.788 ## 6438 3440 1 0.776 0.006265 0.763 0.788 ## 6449 3439 1 0.775 0.006267 0.763 0.788 ## 6452 3438 1 0.775 0.006270 0.763 0.788 ## 6455 3437 1 0.775 0.006272 0.763 0.787 ## 6460 3436 1 0.775 0.006274 0.762 0.787 ## 6464 3435 2 0.774 0.006279 0.762 0.787 ## 6474 3433 1 0.774 0.006281 0.762 0.786 ## 6495 3432 1 0.774 0.006283 0.762 0.786 ## 6497 3431 1 0.774 0.006285 0.761 0.786 ## 6502 3430 1 0.773 0.006287 0.761 0.786 ## 6506 3429 1 0.773 0.006290 0.761 0.786 ## 6511 3428 1 0.773 0.006292 0.761 0.785 ## 6517 3427 1 0.773 0.006294 0.760 0.785 ## 6518 3426 1 0.772 0.006296 0.760 0.785 ## 6519 3425 1 0.772 0.006298 0.760 0.785 ## 6524 3424 1 0.772 0.006301 0.760 0.784 ## 6531 3423 3 0.771 0.006307 0.759 0.784 ## 6532 3420 1 0.771 0.006309 0.759 0.784 ## 6533 3419 1 0.771 0.006312 0.759 0.783 ## 6538 3418 1 0.771 0.006314 0.758 0.783 ## 6554 3417 1 0.770 0.006316 0.758 0.783 ## 6558 3416 1 0.770 0.006318 0.758 0.783 ## 6561 3415 2 0.770 0.006322 0.757 0.782 ## 6562 3413 1 0.770 0.006325 0.757 0.782 ## 6568 3412 1 0.769 0.006327 0.757 0.782 ## 6572 3411 1 0.769 0.006329 0.757 0.782 ## 6577 3410 1 0.769 0.006331 0.757 0.781 ## 6583 3409 1 0.769 0.006333 0.756 0.781 ## 6584 3408 1 0.768 0.006335 0.756 0.781 ## 6585 3407 1 0.768 0.006338 0.756 0.781 ## 6591 3406 1 0.768 0.006340 0.756 0.780 ## 6598 3405 1 0.768 0.006342 0.755 0.780 ## 6602 3404 1 0.767 0.006344 0.755 0.780 ## 6604 3403 1 0.767 0.006346 0.755 0.780 ## 6610 3402 1 0.767 0.006348 0.755 0.780 ## 6615 3401 1 0.767 0.006350 0.754 0.779 ## 6618 3400 1 0.767 0.006353 0.754 0.779 ## 6621 3399 1 0.766 0.006355 0.754 0.779 ## 6626 3398 1 0.766 0.006357 0.754 0.779 ## 6630 3397 1 0.766 0.006359 0.754 0.778 ## 6632 3396 1 0.766 0.006361 0.753 0.778 ## 6633 3395 1 0.765 0.006363 0.753 0.778 ## 6647 3394 1 0.765 0.006365 0.753 0.778 ## 6655 3393 1 0.765 0.006367 0.753 0.778 ## 6660 3392 1 0.765 0.006370 0.752 0.777 ## 6670 3391 1 0.765 0.006372 0.752 0.777 ## 6672 3390 1 0.764 0.006374 0.752 0.777 ## 6678 3389 1 0.764 0.006376 0.752 0.777 ## 6684 3388 1 0.764 0.006378 0.751 0.776 ## 6687 3387 1 0.764 0.006380 0.751 0.776 ## 6691 3386 1 0.763 0.006382 0.751 0.776 ## 6692 3385 1 0.763 0.006384 0.751 0.776 ## 6695 3384 1 0.763 0.006386 0.751 0.776 ## 6700 3383 1 0.763 0.006389 0.750 0.775 ## 6718 3382 1 0.763 0.006391 0.750 0.775 ## 6719 3381 1 0.762 0.006393 0.750 0.775 ## 6722 3380 1 0.762 0.006395 0.750 0.775 ## 6729 3379 1 0.762 0.006397 0.749 0.774 ## 6734 3378 1 0.762 0.006399 0.749 0.774 ## 6735 3377 1 0.761 0.006401 0.749 0.774 ## 6737 3376 2 0.761 0.006405 0.748 0.774 ## 6746 3374 2 0.760 0.006409 0.748 0.773 ## 6747 3372 1 0.760 0.006411 0.748 0.773 ## 6755 3371 1 0.760 0.006413 0.748 0.773 ## 6756 3370 1 0.760 0.006416 0.747 0.772 ## 6757 3369 1 0.760 0.006418 0.747 0.772 ## 6763 3368 1 0.759 0.006420 0.747 0.772 ## 6769 3367 1 0.759 0.006422 0.747 0.772 ## 6770 3366 1 0.759 0.006424 0.746 0.772 ## 6777 3365 1 0.759 0.006426 0.746 0.771 ## 6781 3364 1 0.758 0.006428 0.746 0.771 ## 6798 3363 1 0.758 0.006430 0.746 0.771 ## 6804 3362 1 0.758 0.006432 0.746 0.771 ## 6808 3361 2 0.758 0.006436 0.745 0.770 ## 6813 3359 1 0.757 0.006438 0.745 0.770 ## 6817 3358 1 0.757 0.006440 0.745 0.770 ## 6819 3357 1 0.757 0.006442 0.744 0.770 ## 6824 3356 2 0.756 0.006446 0.744 0.769 ## 6831 3354 1 0.756 0.006448 0.744 0.769 ## 6832 3353 1 0.756 0.006450 0.743 0.769 ## 6833 3352 1 0.756 0.006452 0.743 0.769 ## 6846 3351 1 0.756 0.006454 0.743 0.768 ## 6851 3350 1 0.755 0.006456 0.743 0.768 ## 6876 3349 1 0.755 0.006458 0.743 0.768 ## 6877 3348 1 0.755 0.006460 0.742 0.768 ## 6893 3347 1 0.755 0.006462 0.742 0.767 ## 6899 3346 1 0.754 0.006464 0.742 0.767 ## 6900 3345 1 0.754 0.006466 0.742 0.767 ## 6902 3344 1 0.754 0.006468 0.741 0.767 ## 6910 3343 1 0.754 0.006470 0.741 0.767 ## 6925 3342 1 0.753 0.006472 0.741 0.766 ## 6926 3341 1 0.753 0.006474 0.741 0.766 ## 6934 3340 1 0.753 0.006476 0.740 0.766 ## 6937 3339 1 0.753 0.006478 0.740 0.766 ## 6938 3338 1 0.753 0.006480 0.740 0.765 ## 6940 3337 1 0.752 0.006482 0.740 0.765 ## 6942 3336 1 0.752 0.006484 0.740 0.765 ## 6944 3335 1 0.752 0.006486 0.739 0.765 ## 6948 3334 1 0.752 0.006488 0.739 0.765 ## 6949 3333 1 0.751 0.006490 0.739 0.764 ## 6952 3332 1 0.751 0.006492 0.739 0.764 ## 6954 3331 1 0.751 0.006494 0.738 0.764 ## 6965 3330 1 0.751 0.006496 0.738 0.764 ## 6968 3329 1 0.751 0.006498 0.738 0.763 ## 6970 3328 1 0.750 0.006500 0.738 0.763 ## 6973 3327 1 0.750 0.006502 0.737 0.763 ## 6974 3326 1 0.750 0.006504 0.737 0.763 ## 6975 3325 1 0.750 0.006506 0.737 0.763 ## 6977 3324 1 0.749 0.006508 0.737 0.762 ## 6984 3323 1 0.749 0.006510 0.737 0.762 ## 6986 3322 1 0.749 0.006512 0.736 0.762 ## 6990 3321 1 0.749 0.006514 0.736 0.762 ## 6997 3320 1 0.749 0.006515 0.736 0.761 ## 7005 3319 2 0.748 0.006519 0.735 0.761 ## 7012 3317 1 0.748 0.006521 0.735 0.761 ## 7014 3316 1 0.748 0.006523 0.735 0.761 ## 7015 3315 1 0.747 0.006525 0.735 0.760 ## 7019 3314 1 0.747 0.006527 0.734 0.760 ## 7026 3313 1 0.747 0.006529 0.734 0.760 ## 7035 3312 1 0.747 0.006531 0.734 0.760 ## 7036 3311 1 0.747 0.006533 0.734 0.759 ## 7038 3310 1 0.746 0.006535 0.734 0.759 ## 7041 3309 1 0.746 0.006537 0.733 0.759 ## 7045 3308 1 0.746 0.006539 0.733 0.759 ## 7049 3307 1 0.746 0.006541 0.733 0.759 ## 7050 3306 1 0.745 0.006542 0.733 0.758 ## 7051 3305 2 0.745 0.006546 0.732 0.758 ## 7052 3303 1 0.745 0.006548 0.732 0.758 ## 7059 3302 1 0.744 0.006550 0.732 0.757 ## 7064 3301 1 0.744 0.006552 0.732 0.757 ## 7066 3300 2 0.744 0.006556 0.731 0.757 ## 7070 3298 1 0.744 0.006558 0.731 0.757 ## 7073 3297 1 0.743 0.006560 0.731 0.756 ## 7074 3296 1 0.743 0.006561 0.730 0.756 ## 7077 3295 1 0.743 0.006563 0.730 0.756 ## 7078 3294 1 0.743 0.006565 0.730 0.756 ## 7083 3293 1 0.742 0.006567 0.730 0.755 ## 7086 3292 1 0.742 0.006569 0.729 0.755 ## 7091 3291 1 0.742 0.006571 0.729 0.755 ## 7092 3290 1 0.742 0.006573 0.729 0.755 ## 7102 3289 1 0.742 0.006575 0.729 0.755 ## 7104 3288 1 0.741 0.006576 0.729 0.754 ## 7113 3287 1 0.741 0.006578 0.728 0.754 ## 7115 3286 1 0.741 0.006580 0.728 0.754 ## 7117 3285 1 0.741 0.006582 0.728 0.754 ## 7118 3284 1 0.740 0.006584 0.728 0.753 ## 7119 3283 1 0.740 0.006586 0.727 0.753 ## 7135 3282 1 0.740 0.006588 0.727 0.753 ## 7146 3281 1 0.740 0.006589 0.727 0.753 ## 7151 3280 1 0.740 0.006591 0.727 0.753 ## 7155 3279 1 0.739 0.006593 0.726 0.752 ## 7157 3278 2 0.739 0.006597 0.726 0.752 ## 7166 3276 1 0.739 0.006599 0.726 0.752 ## 7176 3275 1 0.738 0.006600 0.726 0.751 ## 7179 3274 1 0.738 0.006602 0.725 0.751 ## 7183 3273 1 0.738 0.006604 0.725 0.751 ## 7184 3272 1 0.738 0.006606 0.725 0.751 ## 7192 3271 1 0.737 0.006608 0.725 0.751 ## 7193 3270 1 0.737 0.006610 0.724 0.750 ## 7210 3269 1 0.737 0.006611 0.724 0.750 ## 7212 3268 1 0.737 0.006613 0.724 0.750 ## 7216 3267 1 0.737 0.006615 0.724 0.750 ## 7220 3266 1 0.736 0.006617 0.724 0.749 ## 7223 3265 1 0.736 0.006619 0.723 0.749 ## 7225 3264 2 0.736 0.006622 0.723 0.749 ## 7230 3262 1 0.735 0.006624 0.723 0.749 ## 7241 3261 1 0.735 0.006626 0.722 0.748 ## 7246 3260 1 0.735 0.006628 0.722 0.748 ## 7251 3259 1 0.735 0.006630 0.722 0.748 ## 7256 3258 1 0.735 0.006631 0.722 0.748 ## 7267 3257 2 0.734 0.006635 0.721 0.747 ## 7269 3255 1 0.734 0.006637 0.721 0.747 ## 7270 3254 1 0.734 0.006639 0.721 0.747 ## 7283 3253 2 0.733 0.006642 0.720 0.746 ## 7294 3251 2 0.733 0.006646 0.720 0.746 ## 7303 3249 1 0.733 0.006647 0.720 0.746 ## 7305 3248 1 0.732 0.006649 0.719 0.745 ## 7306 3247 2 0.732 0.006653 0.719 0.745 ## 7307 3245 2 0.731 0.006656 0.718 0.745 ## 7308 3243 1 0.731 0.006658 0.718 0.744 ## 7317 3242 1 0.731 0.006660 0.718 0.744 ## 7322 3241 1 0.731 0.006662 0.718 0.744 ## 7327 3240 1 0.730 0.006663 0.718 0.744 ## 7328 3239 2 0.730 0.006667 0.717 0.743 ## 7337 3237 1 0.730 0.006669 0.717 0.743 ## 7340 3236 1 0.730 0.006670 0.717 0.743 ## 7347 3235 1 0.729 0.006672 0.716 0.743 ## 7348 3234 1 0.729 0.006674 0.716 0.742 ## 7352 3233 1 0.729 0.006676 0.716 0.742 ## 7357 3232 1 0.729 0.006677 0.716 0.742 ## 7359 3231 1 0.728 0.006679 0.715 0.742 ## 7360 3230 1 0.728 0.006681 0.715 0.741 ## 7362 3229 1 0.728 0.006683 0.715 0.741 ## 7363 3228 1 0.728 0.006684 0.715 0.741 ## 7365 3227 1 0.728 0.006686 0.715 0.741 ## 7367 3226 1 0.727 0.006688 0.714 0.741 ## 7373 3225 1 0.727 0.006690 0.714 0.740 ## 7378 3224 1 0.727 0.006691 0.714 0.740 ## 7389 3223 1 0.727 0.006693 0.714 0.740 ## 7395 3222 1 0.726 0.006695 0.713 0.740 ## 7403 3221 1 0.726 0.006696 0.713 0.739 ## 7406 3220 1 0.726 0.006698 0.713 0.739 ## 7419 3219 1 0.726 0.006700 0.713 0.739 ## 7428 3218 2 0.725 0.006703 0.712 0.739 ## 7431 3216 1 0.725 0.006705 0.712 0.738 ## 7436 3215 3 0.724 0.006710 0.711 0.738 ## 7439 3212 1 0.724 0.006712 0.711 0.737 ## 7447 3211 1 0.724 0.006714 0.711 0.737 ## 7448 3210 2 0.724 0.006717 0.710 0.737 ## 7455 3208 1 0.723 0.006719 0.710 0.737 ## 7460 3207 1 0.723 0.006720 0.710 0.736 ## 7469 3206 1 0.723 0.006722 0.710 0.736 ## 7472 3205 1 0.723 0.006724 0.710 0.736 ## 7483 3204 1 0.722 0.006725 0.709 0.736 ## 7484 3203 1 0.722 0.006727 0.709 0.735 ## 7493 3202 2 0.722 0.006730 0.709 0.735 ## 7499 3200 1 0.721 0.006732 0.708 0.735 ## 7504 3199 1 0.721 0.006734 0.708 0.735 ## 7505 3198 1 0.721 0.006735 0.708 0.734 ## 7506 3197 1 0.721 0.006737 0.708 0.734 ## 7507 3196 1 0.721 0.006739 0.707 0.734 ## 7514 3195 1 0.720 0.006740 0.707 0.734 ## 7518 3194 1 0.720 0.006742 0.707 0.733 ## 7519 3193 1 0.720 0.006744 0.707 0.733 ## 7520 3192 1 0.720 0.006745 0.707 0.733 ## 7537 3191 1 0.719 0.006747 0.706 0.733 ## 7539 3190 1 0.719 0.006749 0.706 0.733 ## 7541 3189 1 0.719 0.006750 0.706 0.732 ## 7542 3188 1 0.719 0.006752 0.706 0.732 ## 7543 3187 1 0.719 0.006754 0.705 0.732 ## 7548 3186 1 0.718 0.006755 0.705 0.732 ## 7552 3185 1 0.718 0.006757 0.705 0.731 ## 7554 3184 1 0.718 0.006759 0.705 0.731 ## 7557 3183 1 0.718 0.006760 0.705 0.731 ## 7560 3182 1 0.717 0.006762 0.704 0.731 ## 7563 3181 1 0.717 0.006763 0.704 0.731 ## 7567 3180 1 0.717 0.006765 0.704 0.730 ## 7569 3179 3 0.716 0.006770 0.703 0.730 ## 7570 3176 1 0.716 0.006772 0.703 0.729 ## 7571 3175 1 0.716 0.006773 0.703 0.729 ## 7574 3174 1 0.716 0.006775 0.702 0.729 ## 7576 3173 1 0.715 0.006776 0.702 0.729 ## 7583 3172 1 0.715 0.006778 0.702 0.729 ## 7587 3171 1 0.715 0.006780 0.702 0.728 ## 7591 3170 1 0.715 0.006781 0.702 0.728 ## 7592 3169 2 0.714 0.006785 0.701 0.728 ## 7594 3167 1 0.714 0.006786 0.701 0.727 ## 7600 3166 1 0.714 0.006788 0.701 0.727 ## 7601 3165 1 0.714 0.006789 0.700 0.727 ## 7607 3164 1 0.713 0.006791 0.700 0.727 ## 7608 3163 1 0.713 0.006793 0.700 0.727 ## 7609 3162 1 0.713 0.006794 0.700 0.726 ## 7610 3161 1 0.713 0.006796 0.699 0.726 ## 7611 3160 1 0.712 0.006797 0.699 0.726 ## 7613 3159 1 0.712 0.006799 0.699 0.726 ## 7618 3158 1 0.712 0.006800 0.699 0.725 ## 7623 3157 2 0.712 0.006804 0.698 0.725 ## 7624 3155 1 0.711 0.006805 0.698 0.725 ## 7630 3154 2 0.711 0.006808 0.698 0.724 ## 7632 3152 1 0.711 0.006810 0.697 0.724 ## 7633 3151 1 0.710 0.006812 0.697 0.724 ## 7634 3150 1 0.710 0.006813 0.697 0.724 ## 7635 3149 1 0.710 0.006815 0.697 0.723 ## 7643 3148 1 0.710 0.006816 0.697 0.723 ## 7644 3147 1 0.710 0.006818 0.696 0.723 ## 7649 3146 1 0.709 0.006819 0.696 0.723 ## 7654 3145 1 0.709 0.006821 0.696 0.723 ## 7661 3144 2 0.709 0.006824 0.695 0.722 ## 7667 3142 2 0.708 0.006827 0.695 0.722 ## 7668 3140 1 0.708 0.006829 0.695 0.721 ## 7671 3139 1 0.708 0.006830 0.694 0.721 ## 7679 3138 1 0.707 0.006832 0.694 0.721 ## 7681 3137 1 0.707 0.006833 0.694 0.721 ## 7684 3136 1 0.707 0.006835 0.694 0.721 ## 7685 3135 1 0.707 0.006836 0.694 0.720 ## 7689 3134 2 0.706 0.006839 0.693 0.720 ## 7707 3132 1 0.706 0.006841 0.693 0.720 ## 7708 3131 1 0.706 0.006843 0.693 0.719 ## 7710 3130 2 0.705 0.006846 0.692 0.719 ## 7711 3128 1 0.705 0.006847 0.692 0.719 ## 7729 3127 1 0.705 0.006849 0.692 0.719 ## 7738 3126 1 0.705 0.006850 0.691 0.718 ## 7739 3125 1 0.705 0.006852 0.691 0.718 ## 7746 3124 1 0.704 0.006853 0.691 0.718 ## 7758 3123 1 0.704 0.006855 0.691 0.718 ## 7766 3122 2 0.704 0.006858 0.690 0.717 ## 7769 3120 1 0.703 0.006859 0.690 0.717 ## 7771 3119 1 0.703 0.006861 0.690 0.717 ## 7772 3118 1 0.703 0.006862 0.690 0.717 ## 7774 3117 2 0.703 0.006865 0.689 0.716 ## 7784 3115 1 0.702 0.006867 0.689 0.716 ## 7793 3114 1 0.702 0.006868 0.689 0.716 ## 7794 3113 1 0.702 0.006870 0.689 0.715 ## 7795 3112 1 0.702 0.006871 0.688 0.715 ## 7796 3111 1 0.701 0.006873 0.688 0.715 ## 7799 3110 1 0.701 0.006874 0.688 0.715 ## 7801 3109 1 0.701 0.006876 0.688 0.715 ## 7803 3108 1 0.701 0.006877 0.687 0.714 ## 7809 3107 1 0.700 0.006879 0.687 0.714 ## 7818 3106 1 0.700 0.006880 0.687 0.714 ## 7819 3105 1 0.700 0.006882 0.687 0.714 ## 7820 3104 3 0.699 0.006886 0.686 0.713 ## 7823 3101 1 0.699 0.006888 0.686 0.713 ## 7826 3100 1 0.699 0.006889 0.686 0.713 ## 7837 3099 1 0.699 0.006890 0.685 0.712 ## 7840 3098 1 0.698 0.006892 0.685 0.712 ## 7850 3097 1 0.698 0.006893 0.685 0.712 ## 7851 3096 1 0.698 0.006895 0.685 0.712 ## 7858 3095 1 0.698 0.006896 0.684 0.711 ## 7866 3094 1 0.698 0.006898 0.684 0.711 ## 7876 3093 1 0.697 0.006899 0.684 0.711 ## 7884 3092 1 0.697 0.006901 0.684 0.711 ## 7885 3091 1 0.697 0.006902 0.683 0.711 ## 7887 3090 1 0.697 0.006904 0.683 0.710 ## 7888 3089 1 0.696 0.006905 0.683 0.710 ## 7889 3088 1 0.696 0.006907 0.683 0.710 ## 7892 3087 1 0.696 0.006908 0.683 0.710 ## 7895 3086 1 0.696 0.006909 0.682 0.709 ## 7897 3085 1 0.696 0.006911 0.682 0.709 ## 7899 3084 1 0.695 0.006912 0.682 0.709 ## 7905 3083 1 0.695 0.006914 0.682 0.709 ## 7921 3082 1 0.695 0.006915 0.681 0.709 ## 7923 3081 1 0.695 0.006917 0.681 0.708 ## 7935 3080 1 0.694 0.006918 0.681 0.708 ## 7940 3079 1 0.694 0.006919 0.681 0.708 ## 7945 3078 1 0.694 0.006921 0.681 0.708 ## 7950 3077 1 0.694 0.006922 0.680 0.707 ## 7951 3076 1 0.694 0.006924 0.680 0.707 ## 7953 3075 1 0.693 0.006925 0.680 0.707 ## 7963 3074 1 0.693 0.006927 0.680 0.707 ## 7972 3073 1 0.693 0.006928 0.679 0.707 ## 7974 3072 1 0.693 0.006929 0.679 0.706 ## 7980 3071 1 0.692 0.006931 0.679 0.706 ## 7985 3070 1 0.692 0.006932 0.679 0.706 ## 7996 3069 1 0.692 0.006934 0.678 0.706 ## 7998 3068 1 0.692 0.006935 0.678 0.705 ## 8002 3067 1 0.691 0.006936 0.678 0.705 ## 8007 3066 1 0.691 0.006938 0.678 0.705 ## 8013 3065 1 0.691 0.006939 0.678 0.705 ## 8016 3064 2 0.691 0.006942 0.677 0.704 ## 8020 3062 2 0.690 0.006945 0.677 0.704 ## 8031 3060 1 0.690 0.006946 0.676 0.704 ## 8036 3059 1 0.690 0.006948 0.676 0.703 ## 8046 3058 1 0.689 0.006949 0.676 0.703 ## 8048 3057 1 0.689 0.006950 0.676 0.703 ## 8050 3056 1 0.689 0.006952 0.676 0.703 ## 8052 3055 1 0.689 0.006953 0.675 0.703 ## 8054 3054 1 0.689 0.006955 0.675 0.702 ## 8060 3053 2 0.688 0.006957 0.675 0.702 ## 8064 3051 1 0.688 0.006959 0.674 0.702 ## 8066 3050 1 0.688 0.006960 0.674 0.701 ## 8078 3049 1 0.687 0.006961 0.674 0.701 ## 8084 3048 1 0.687 0.006963 0.674 0.701 ## 8092 3047 1 0.687 0.006964 0.673 0.701 ## 8098 3046 1 0.687 0.006965 0.673 0.701 ## 8101 3045 1 0.687 0.006967 0.673 0.700 ## 8111 3044 1 0.686 0.006968 0.673 0.700 ## 8119 3043 1 0.686 0.006970 0.673 0.700 ## 8122 3042 1 0.686 0.006971 0.672 0.700 ## 8128 3041 1 0.686 0.006972 0.672 0.699 ## 8132 3040 1 0.685 0.006974 0.672 0.699 ## 8133 3039 1 0.685 0.006975 0.672 0.699 ## 8137 3038 1 0.685 0.006976 0.671 0.699 ## 8141 3037 1 0.685 0.006978 0.671 0.699 ## 8157 3036 1 0.684 0.006979 0.671 0.698 ## 8158 3035 1 0.684 0.006980 0.671 0.698 ## 8163 3034 1 0.684 0.006982 0.670 0.698 ## 8164 3033 1 0.684 0.006983 0.670 0.698 ## 8172 3032 2 0.683 0.006986 0.670 0.697 ## 8173 3030 1 0.683 0.006987 0.670 0.697 ## 8177 3029 1 0.683 0.006988 0.669 0.697 ## 8187 3028 1 0.683 0.006990 0.669 0.697 ## 8195 3027 1 0.682 0.006991 0.669 0.696 ## 8212 3026 1 0.682 0.006992 0.669 0.696 ## 8214 3025 1 0.682 0.006994 0.668 0.696 ## 8215 3024 1 0.682 0.006995 0.668 0.696 ## 8218 3023 1 0.682 0.006996 0.668 0.695 ## 8220 3022 1 0.681 0.006998 0.668 0.695 ## 8223 3021 1 0.681 0.006999 0.668 0.695 ## 8233 3020 1 0.681 0.007000 0.667 0.695 ## 8234 3019 1 0.681 0.007002 0.667 0.695 ## 8238 3018 1 0.680 0.007003 0.667 0.694 ## 8240 3017 1 0.680 0.007004 0.667 0.694 ## 8241 3016 1 0.680 0.007006 0.666 0.694 ## 8242 3015 1 0.680 0.007007 0.666 0.694 ## 8244 3014 1 0.680 0.007008 0.666 0.693 ## 8246 3013 1 0.679 0.007009 0.666 0.693 ## 8263 3012 1 0.679 0.007011 0.665 0.693 ## 8264 3011 1 0.679 0.007012 0.665 0.693 ## 8266 3010 1 0.679 0.007013 0.665 0.693 ## 8271 3009 1 0.678 0.007015 0.665 0.692 ## 8273 3008 1 0.678 0.007016 0.665 0.692 ## 8274 3007 2 0.678 0.007019 0.664 0.692 ## 8275 3005 1 0.677 0.007020 0.664 0.691 ## 8277 3004 1 0.677 0.007021 0.664 0.691 ## 8281 3003 1 0.677 0.007022 0.663 0.691 ## 8285 3002 3 0.676 0.007026 0.663 0.690 ## 8291 2999 1 0.676 0.007027 0.663 0.690 ## 8293 2998 1 0.676 0.007029 0.662 0.690 ## 8295 2997 1 0.676 0.007030 0.662 0.690 ## 8298 2996 1 0.675 0.007031 0.662 0.689 ## 8305 2995 1 0.675 0.007033 0.662 0.689 ## 8307 2994 1 0.675 0.007034 0.661 0.689 ## 8310 2993 1 0.675 0.007035 0.661 0.689 ## 8312 2992 1 0.675 0.007036 0.661 0.688 ## 8317 2991 1 0.674 0.007038 0.661 0.688 ## 8319 2990 1 0.674 0.007039 0.660 0.688 ## 8321 2989 1 0.674 0.007040 0.660 0.688 ## 8332 2988 1 0.674 0.007041 0.660 0.688 ## 8335 2987 1 0.673 0.007043 0.660 0.687 ## 8337 2986 1 0.673 0.007044 0.660 0.687 ## 8344 2985 1 0.673 0.007045 0.659 0.687 ## 8349 2984 1 0.673 0.007046 0.659 0.687 ## 8357 2983 2 0.672 0.007049 0.659 0.686 ## 8361 2981 1 0.672 0.007050 0.658 0.686 ## 8370 2980 1 0.672 0.007051 0.658 0.686 ## 8371 2979 1 0.672 0.007053 0.658 0.686 ## 8374 2978 1 0.671 0.007054 0.658 0.685 ## 8386 2977 1 0.671 0.007055 0.657 0.685 ## 8392 2976 1 0.671 0.007056 0.657 0.685 ## 8401 2975 1 0.671 0.007058 0.657 0.685 ## 8402 2974 1 0.671 0.007059 0.657 0.684 ## 8404 2973 2 0.670 0.007061 0.656 0.684 ## 8406 2971 1 0.670 0.007062 0.656 0.684 ## 8411 2970 1 0.670 0.007064 0.656 0.684 ## 8413 2969 1 0.669 0.007065 0.656 0.683 ## 8415 2968 1 0.669 0.007066 0.655 0.683 ## 8417 2967 1 0.669 0.007067 0.655 0.683 ## 8423 2966 1 0.669 0.007069 0.655 0.683 ## 8436 2965 1 0.668 0.007070 0.655 0.682 ## 8451 2964 1 0.668 0.007071 0.655 0.682 ## 8452 2963 1 0.668 0.007072 0.654 0.682 ## 8454 2962 1 0.668 0.007073 0.654 0.682 ## 8457 2961 1 0.668 0.007075 0.654 0.682 ## 8469 2960 2 0.667 0.007077 0.653 0.681 ## 8470 2958 1 0.667 0.007078 0.653 0.681 ## 8471 2957 1 0.667 0.007079 0.653 0.681 ## 8490 2956 1 0.666 0.007081 0.653 0.680 ## 8494 2955 1 0.666 0.007082 0.652 0.680 ## 8498 2954 1 0.666 0.007083 0.652 0.680 ## 8501 2953 1 0.666 0.007084 0.652 0.680 ## 8505 2952 1 0.666 0.007085 0.652 0.680 ## 8507 2951 1 0.665 0.007087 0.652 0.679 ## 8511 2950 1 0.665 0.007088 0.651 0.679 ## 8513 2949 3 0.664 0.007091 0.651 0.678 ## 8516 2946 1 0.664 0.007092 0.650 0.678 ## 8518 2945 1 0.664 0.007094 0.650 0.678 ## 8519 2944 1 0.664 0.007095 0.650 0.678 ## 8525 2943 1 0.664 0.007096 0.650 0.678 ## 8529 2942 1 0.663 0.007097 0.650 0.677 ## 8530 2941 1 0.663 0.007098 0.649 0.677 ## 8538 2940 1 0.663 0.007099 0.649 0.677 ## 8543 2939 1 0.663 0.007101 0.649 0.677 ## 8546 2938 1 0.662 0.007102 0.649 0.676 ## 8549 2937 1 0.662 0.007103 0.648 0.676 ## 8550 2936 1 0.662 0.007104 0.648 0.676 ## 8556 2935 1 0.662 0.007105 0.648 0.676 ## 8576 2934 1 0.661 0.007106 0.648 0.676 ## 8577 2933 1 0.661 0.007108 0.647 0.675 ## 8589 2932 1 0.661 0.007109 0.647 0.675 ## 8595 2931 2 0.661 0.007111 0.647 0.675 ## 8608 2929 1 0.660 0.007112 0.647 0.674 ## 8610 2928 1 0.660 0.007113 0.646 0.674 ## 8613 2927 1 0.660 0.007114 0.646 0.674 ## 8618 2926 1 0.660 0.007116 0.646 0.674 ## 8622 2925 1 0.659 0.007117 0.646 0.674 ## 8625 2924 1 0.659 0.007118 0.645 0.673 ## 8627 2923 2 0.659 0.007120 0.645 0.673 ## 8630 2921 1 0.659 0.007121 0.645 0.673 ## 8631 2920 1 0.658 0.007122 0.645 0.672 ## 8640 2919 1 0.658 0.007124 0.644 0.672 ## 8648 2918 1 0.658 0.007125 0.644 0.672 ## 8650 2917 2 0.657 0.007127 0.644 0.672 ## 8656 2915 1 0.657 0.007128 0.643 0.671 ## 8657 2914 1 0.657 0.007129 0.643 0.671 ## 8664 2913 1 0.657 0.007130 0.643 0.671 ## 8669 2912 1 0.657 0.007131 0.643 0.671 ## 8671 2911 1 0.656 0.007133 0.642 0.670 ## 8674 2910 1 0.656 0.007134 0.642 0.670 ## 8676 2909 2 0.656 0.007136 0.642 0.670 ## 8683 2907 1 0.655 0.007137 0.642 0.670 ## 8690 2906 2 0.655 0.007139 0.641 0.669 ## 8697 2904 1 0.655 0.007140 0.641 0.669 ## 8700 2903 1 0.654 0.007141 0.641 0.669 ## 8702 2902 1 0.654 0.007143 0.640 0.668 ## 8705 2901 1 0.654 0.007144 0.640 0.668 ## 8707 2900 1 0.654 0.007145 0.640 0.668 ## 8708 2899 1 0.654 0.007146 0.640 0.668 ## 8717 2898 1 0.653 0.007147 0.640 0.668 ## 8722 2897 1 0.653 0.007148 0.639 0.667 ## 8723 2896 1 0.653 0.007149 0.639 0.667 ## 8726 2895 1 0.653 0.007150 0.639 0.667 ## 8727 2894 2 0.652 0.007152 0.638 0.666 ## 8734 2892 1 0.652 0.007153 0.638 0.666 ## 8738 2891 1 0.652 0.007154 0.638 0.666 ## 8741 2890 1 0.652 0.007156 0.638 0.666 ## 8744 2889 2 0.651 0.007158 0.637 0.665 ## 8747 2887 1 0.651 0.007159 0.637 0.665 ## 8753 2886 1 0.651 0.007160 0.637 0.665 ## 8759 2885 1 0.650 0.007161 0.637 0.665 fit1 %&gt;% ggsurvplot(xlab=&quot;Time to death (days)&quot;,ylab=expression(paste(&#39;Overall Survival Probablity &#39;, hat(S)*&quot;(t)&quot;))) We can see that as follow-up time increases survival decreases rather monotinacally over time, or in other words the number of people who have died increases. Survival \\(\\hat{S}(t)\\) drops to about 0.65 at the end of follow-up, or in other words about 35% of participants have died, which is what is expected as we already know that 1550 of 4434 participants have died. We can repeat this estimation with a different time-scale of interest. Other that follow-up times we may also be interested in Survival and failure (mortality) with respect to age. We repeat the same code only changing the first argument in the Surv function, substituting time of death with respect to follow-up time with age at death. fit2&lt;-survfit(Surv(agedth, death) ~ 1, data = fhs_first) summary(fit2) ## Call: survfit(formula = Surv(agedth, death) ~ 1, data = fhs_first) ## ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 38.4 4434 1 0.9998 0.000226 0.9993 1.000 ## 40.4 4433 1 0.9995 0.000319 0.9989 1.000 ## 41.6 4432 1 0.9993 0.000390 0.9986 1.000 ## 41.9 4431 1 0.9991 0.000451 0.9982 1.000 ## 42.6 4430 1 0.9989 0.000504 0.9979 1.000 ## 43.0 4429 1 0.9986 0.000552 0.9976 1.000 ## 43.2 4428 1 0.9984 0.000596 0.9973 1.000 ## 43.2 4427 1 0.9982 0.000637 0.9969 0.999 ## 43.7 4426 1 0.9980 0.000676 0.9966 0.999 ## 43.9 4425 1 0.9977 0.000712 0.9963 0.999 ## 44.2 4424 1 0.9975 0.000747 0.9961 0.999 ## 44.3 4423 1 0.9973 0.000780 0.9958 0.999 ## 44.9 4422 1 0.9971 0.000812 0.9955 0.999 ## 45.4 4421 1 0.9968 0.000843 0.9952 0.998 ## 45.5 4420 1 0.9966 0.000872 0.9949 0.998 ## 45.5 4419 1 0.9964 0.000900 0.9946 0.998 ## 45.6 4418 1 0.9962 0.000928 0.9943 0.998 ## 46.1 4417 1 0.9959 0.000955 0.9941 0.998 ## 46.6 4416 1 0.9957 0.000981 0.9938 0.998 ## 46.8 4415 1 0.9955 0.001006 0.9935 0.997 ## 47.0 4414 1 0.9953 0.001031 0.9932 0.997 ## 47.2 4413 1 0.9950 0.001055 0.9930 0.997 ## 47.4 4412 1 0.9948 0.001079 0.9927 0.997 ## 47.5 4411 1 0.9946 0.001102 0.9924 0.997 ## 47.5 4410 1 0.9944 0.001124 0.9922 0.997 ## 47.7 4409 1 0.9941 0.001147 0.9919 0.996 ## 47.9 4408 1 0.9939 0.001168 0.9916 0.996 ## 48.2 4407 1 0.9937 0.001190 0.9914 0.996 ## 48.3 4406 1 0.9935 0.001211 0.9911 0.996 ## 48.3 4405 1 0.9932 0.001231 0.9908 0.996 ## 48.6 4404 1 0.9930 0.001251 0.9906 0.995 ## 48.6 4403 1 0.9928 0.001271 0.9903 0.995 ## 48.7 4402 1 0.9926 0.001291 0.9900 0.995 ## 48.9 4401 1 0.9923 0.001310 0.9898 0.995 ## 49.0 4400 1 0.9921 0.001329 0.9895 0.995 ## 49.0 4399 1 0.9919 0.001348 0.9892 0.995 ## 49.1 4398 1 0.9917 0.001366 0.9890 0.994 ## 49.2 4397 1 0.9914 0.001384 0.9887 0.994 ## 49.3 4396 1 0.9912 0.001402 0.9885 0.994 ## 49.3 4395 1 0.9910 0.001420 0.9882 0.994 ## 49.4 4394 1 0.9908 0.001437 0.9879 0.994 ## 49.4 4393 1 0.9905 0.001455 0.9877 0.993 ## 49.4 4392 1 0.9903 0.001472 0.9874 0.993 ## 49.6 4391 1 0.9901 0.001489 0.9872 0.993 ## 49.6 4390 1 0.9899 0.001505 0.9869 0.993 ## 49.7 4389 1 0.9896 0.001522 0.9866 0.993 ## 49.8 4388 1 0.9894 0.001538 0.9864 0.992 ## 49.8 4387 1 0.9892 0.001554 0.9861 0.992 ## 49.8 4386 1 0.9889 0.001570 0.9859 0.992 ## 49.9 4385 1 0.9887 0.001586 0.9856 0.992 ## 49.9 4384 1 0.9885 0.001601 0.9854 0.992 ## 49.9 4383 1 0.9883 0.001617 0.9851 0.991 ## 49.9 4382 1 0.9880 0.001632 0.9849 0.991 ## 50.1 4381 1 0.9878 0.001647 0.9846 0.991 ## 50.1 4380 1 0.9876 0.001662 0.9843 0.991 ## 50.2 4379 1 0.9874 0.001677 0.9841 0.991 ## 50.3 4378 2 0.9869 0.001706 0.9836 0.990 ## 50.3 4376 1 0.9867 0.001721 0.9833 0.990 ## 50.4 4375 1 0.9865 0.001735 0.9831 0.990 ## 50.4 4374 1 0.9862 0.001749 0.9828 0.990 ## 50.7 4373 1 0.9860 0.001763 0.9826 0.989 ## 50.8 4372 1 0.9858 0.001777 0.9823 0.989 ## 50.9 4371 1 0.9856 0.001791 0.9821 0.989 ## 51.1 4370 1 0.9853 0.001805 0.9818 0.989 ## 51.2 4369 1 0.9851 0.001819 0.9816 0.989 ## 51.2 4368 1 0.9849 0.001832 0.9813 0.988 ## 51.2 4367 1 0.9847 0.001845 0.9811 0.988 ## 51.2 4366 1 0.9844 0.001859 0.9808 0.988 ## 51.3 4365 1 0.9842 0.001872 0.9806 0.988 ## 51.3 4364 1 0.9840 0.001885 0.9803 0.988 ## 51.3 4363 1 0.9838 0.001898 0.9800 0.987 ## 51.3 4362 1 0.9835 0.001911 0.9798 0.987 ## 51.4 4361 1 0.9833 0.001924 0.9795 0.987 ## 51.4 4360 1 0.9831 0.001937 0.9793 0.987 ## 51.4 4359 1 0.9829 0.001949 0.9790 0.987 ## 51.5 4358 1 0.9826 0.001962 0.9788 0.986 ## 51.5 4357 1 0.9824 0.001974 0.9785 0.986 ## 51.5 4356 1 0.9822 0.001987 0.9783 0.986 ## 51.7 4355 1 0.9820 0.001999 0.9780 0.986 ## 51.8 4354 1 0.9817 0.002011 0.9778 0.986 ## 51.8 4353 1 0.9815 0.002023 0.9775 0.985 ## 51.8 4352 1 0.9813 0.002035 0.9773 0.985 ## 51.9 4351 1 0.9811 0.002047 0.9771 0.985 ## 52.0 4350 1 0.9808 0.002059 0.9768 0.985 ## 52.1 4349 1 0.9806 0.002071 0.9766 0.985 ## 52.1 4348 1 0.9804 0.002083 0.9763 0.984 ## 52.2 4347 1 0.9802 0.002095 0.9761 0.984 ## 52.2 4346 1 0.9799 0.002106 0.9758 0.984 ## 52.3 4345 1 0.9797 0.002118 0.9756 0.984 ## 52.3 4344 1 0.9795 0.002129 0.9753 0.984 ## 52.3 4343 1 0.9793 0.002141 0.9751 0.983 ## 52.4 4342 1 0.9790 0.002152 0.9748 0.983 ## 52.4 4341 1 0.9788 0.002163 0.9746 0.983 ## 52.5 4340 1 0.9786 0.002175 0.9743 0.983 ## 52.7 4339 1 0.9783 0.002186 0.9741 0.983 ## 52.7 4338 1 0.9781 0.002197 0.9738 0.982 ## 52.8 4337 1 0.9779 0.002208 0.9736 0.982 ## 52.8 4336 1 0.9777 0.002219 0.9733 0.982 ## 52.9 4335 1 0.9774 0.002230 0.9731 0.982 ## 53.0 4334 1 0.9772 0.002241 0.9728 0.982 ## 53.1 4333 1 0.9770 0.002251 0.9726 0.981 ## 53.1 4332 1 0.9768 0.002262 0.9723 0.981 ## 53.2 4331 1 0.9765 0.002273 0.9721 0.981 ## 53.2 4330 1 0.9763 0.002283 0.9719 0.981 ## 53.2 4329 1 0.9761 0.002294 0.9716 0.981 ## 53.3 4328 1 0.9759 0.002305 0.9714 0.980 ## 53.3 4327 1 0.9756 0.002315 0.9711 0.980 ## 53.3 4326 1 0.9754 0.002325 0.9709 0.980 ## 53.4 4325 1 0.9752 0.002336 0.9706 0.980 ## 53.5 4324 1 0.9750 0.002346 0.9704 0.980 ## 53.5 4323 1 0.9747 0.002356 0.9701 0.979 ## 53.6 4322 1 0.9745 0.002367 0.9699 0.979 ## 53.6 4321 1 0.9743 0.002377 0.9696 0.979 ## 53.6 4320 1 0.9741 0.002387 0.9694 0.979 ## 53.7 4319 1 0.9738 0.002397 0.9692 0.979 ## 53.8 4318 1 0.9736 0.002407 0.9689 0.978 ## 53.8 4317 1 0.9734 0.002417 0.9687 0.978 ## 53.8 4316 1 0.9732 0.002427 0.9684 0.978 ## 53.8 4315 1 0.9729 0.002437 0.9682 0.978 ## 53.9 4314 1 0.9727 0.002447 0.9679 0.978 ## 53.9 4313 1 0.9725 0.002457 0.9677 0.977 ## 53.9 4312 1 0.9723 0.002466 0.9674 0.977 ## 54.0 4311 1 0.9720 0.002476 0.9672 0.977 ## 54.0 4310 1 0.9718 0.002486 0.9669 0.977 ## 54.1 4309 1 0.9716 0.002495 0.9667 0.976 ## 54.2 4308 1 0.9714 0.002505 0.9665 0.976 ## 54.2 4307 1 0.9711 0.002514 0.9662 0.976 ## 54.2 4306 1 0.9709 0.002524 0.9660 0.976 ## 54.2 4305 1 0.9707 0.002533 0.9657 0.976 ## 54.3 4304 1 0.9705 0.002543 0.9655 0.975 ## 54.3 4303 1 0.9702 0.002552 0.9652 0.975 ## 54.3 4302 1 0.9700 0.002562 0.9650 0.975 ## 54.4 4301 1 0.9698 0.002571 0.9648 0.975 ## 54.4 4300 1 0.9696 0.002580 0.9645 0.975 ## 54.4 4299 1 0.9693 0.002589 0.9643 0.974 ## 54.4 4298 1 0.9691 0.002599 0.9640 0.974 ## 54.4 4297 1 0.9689 0.002608 0.9638 0.974 ## 54.5 4296 1 0.9687 0.002617 0.9635 0.974 ## 54.5 4295 1 0.9684 0.002626 0.9633 0.974 ## 54.6 4294 1 0.9682 0.002635 0.9630 0.973 ## 54.6 4293 1 0.9680 0.002644 0.9628 0.973 ## 54.6 4292 1 0.9677 0.002653 0.9626 0.973 ## 54.6 4291 1 0.9675 0.002662 0.9623 0.973 ## 54.7 4290 1 0.9673 0.002671 0.9621 0.973 ## 54.7 4289 1 0.9671 0.002680 0.9618 0.972 ## 54.7 4288 1 0.9668 0.002689 0.9616 0.972 ## 54.8 4287 1 0.9666 0.002698 0.9613 0.972 ## 54.9 4286 1 0.9664 0.002706 0.9611 0.972 ## 54.9 4285 1 0.9662 0.002715 0.9609 0.972 ## 55.0 4284 1 0.9659 0.002724 0.9606 0.971 ## 55.1 4283 1 0.9657 0.002732 0.9604 0.971 ## 55.1 4282 1 0.9655 0.002741 0.9601 0.971 ## 55.1 4281 1 0.9653 0.002750 0.9599 0.971 ## 55.1 4280 1 0.9650 0.002758 0.9597 0.970 ## 55.1 4279 1 0.9648 0.002767 0.9594 0.970 ## 55.2 4278 1 0.9646 0.002775 0.9592 0.970 ## 55.2 4277 1 0.9644 0.002784 0.9589 0.970 ## 55.2 4276 2 0.9639 0.002801 0.9584 0.969 ## 55.3 4274 1 0.9637 0.002809 0.9582 0.969 ## 55.3 4273 1 0.9635 0.002818 0.9580 0.969 ## 55.3 4272 1 0.9632 0.002826 0.9577 0.969 ## 55.3 4271 1 0.9630 0.002834 0.9575 0.969 ## 55.3 4270 1 0.9628 0.002843 0.9572 0.968 ## 55.4 4269 1 0.9626 0.002851 0.9570 0.968 ## 55.4 4268 1 0.9623 0.002859 0.9567 0.968 ## 55.4 4267 1 0.9621 0.002867 0.9565 0.968 ## 55.5 4266 1 0.9619 0.002875 0.9563 0.968 ## 55.5 4265 1 0.9617 0.002884 0.9560 0.967 ## 55.6 4264 1 0.9614 0.002892 0.9558 0.967 ## 55.7 4263 1 0.9612 0.002900 0.9555 0.967 ## 55.7 4262 1 0.9610 0.002908 0.9553 0.967 ## 55.7 4261 1 0.9608 0.002916 0.9551 0.966 ## 55.7 4260 1 0.9605 0.002924 0.9548 0.966 ## 55.8 4259 1 0.9603 0.002932 0.9546 0.966 ## 55.8 4258 1 0.9601 0.002940 0.9543 0.966 ## 55.8 4257 1 0.9599 0.002948 0.9541 0.966 ## 55.8 4256 1 0.9596 0.002956 0.9539 0.965 ## 55.9 4255 1 0.9594 0.002964 0.9536 0.965 ## 55.9 4254 1 0.9592 0.002972 0.9534 0.965 ## 56.0 4253 1 0.9590 0.002979 0.9531 0.965 ## 56.0 4252 1 0.9587 0.002987 0.9529 0.965 ## 56.0 4251 1 0.9585 0.002995 0.9527 0.964 ## 56.0 4250 1 0.9583 0.003003 0.9524 0.964 ## 56.0 4248 1 0.9581 0.003011 0.9522 0.964 ## 56.1 4247 1 0.9578 0.003018 0.9519 0.964 ## 56.1 4246 1 0.9576 0.003026 0.9517 0.964 ## 56.1 4245 1 0.9574 0.003034 0.9514 0.963 ## 56.1 4244 1 0.9571 0.003041 0.9512 0.963 ## 56.2 4243 1 0.9569 0.003049 0.9510 0.963 ## 56.3 4242 1 0.9567 0.003057 0.9507 0.963 ## 56.3 4241 1 0.9565 0.003064 0.9505 0.962 ## 56.4 4240 1 0.9562 0.003072 0.9502 0.962 ## 56.4 4239 1 0.9560 0.003079 0.9500 0.962 ## 56.6 4238 1 0.9558 0.003087 0.9498 0.962 ## 56.6 4237 1 0.9556 0.003094 0.9495 0.962 ## 56.6 4236 1 0.9553 0.003102 0.9493 0.961 ## 56.6 4235 1 0.9551 0.003109 0.9490 0.961 ## 56.6 4234 1 0.9549 0.003117 0.9488 0.961 ## 56.7 4233 1 0.9547 0.003124 0.9486 0.961 ## 56.7 4232 1 0.9544 0.003132 0.9483 0.961 ## 56.8 4231 1 0.9542 0.003139 0.9481 0.960 ## 56.8 4230 1 0.9540 0.003146 0.9478 0.960 ## 56.9 4229 1 0.9538 0.003154 0.9476 0.960 ## 56.9 4228 1 0.9535 0.003161 0.9474 0.960 ## 56.9 4227 1 0.9533 0.003168 0.9471 0.960 ## 56.9 4226 1 0.9531 0.003176 0.9469 0.959 ## 56.9 4225 1 0.9529 0.003183 0.9466 0.959 ## 57.0 4224 1 0.9526 0.003190 0.9464 0.959 ## 57.0 4223 1 0.9524 0.003197 0.9462 0.959 ## 57.1 4217 1 0.9522 0.003204 0.9459 0.958 ## 57.1 4216 1 0.9520 0.003212 0.9457 0.958 ## 57.2 4215 1 0.9517 0.003219 0.9454 0.958 ## 57.2 4214 1 0.9515 0.003226 0.9452 0.958 ## 57.3 4213 1 0.9513 0.003233 0.9450 0.958 ## 57.3 4212 1 0.9511 0.003240 0.9447 0.957 ## 57.4 4211 1 0.9508 0.003247 0.9445 0.957 ## 57.4 4210 1 0.9506 0.003254 0.9442 0.957 ## 57.4 4209 1 0.9504 0.003261 0.9440 0.957 ## 57.5 4208 1 0.9502 0.003268 0.9438 0.957 ## 57.5 4207 1 0.9499 0.003275 0.9435 0.956 ## 57.6 4206 1 0.9497 0.003282 0.9433 0.956 ## 57.6 4205 1 0.9495 0.003289 0.9431 0.956 ## 57.6 4204 1 0.9492 0.003296 0.9428 0.956 ## 57.6 4203 1 0.9490 0.003303 0.9426 0.956 ## 57.6 4202 1 0.9488 0.003310 0.9423 0.955 ## 57.6 4201 1 0.9486 0.003317 0.9421 0.955 ## 57.6 4200 1 0.9483 0.003324 0.9419 0.955 ## 57.7 4199 1 0.9481 0.003331 0.9416 0.955 ## 57.7 4198 1 0.9479 0.003338 0.9414 0.954 ## 57.7 4197 1 0.9477 0.003345 0.9411 0.954 ## 57.7 4196 1 0.9474 0.003351 0.9409 0.954 ## 57.8 4195 1 0.9472 0.003358 0.9407 0.954 ## 57.9 4194 1 0.9470 0.003365 0.9404 0.954 ## 57.9 4193 1 0.9468 0.003372 0.9402 0.953 ## 57.9 4192 1 0.9465 0.003379 0.9399 0.953 ## 58.0 4175 1 0.9463 0.003385 0.9397 0.953 ## 58.1 4174 1 0.9461 0.003392 0.9395 0.953 ## 58.2 4173 1 0.9459 0.003399 0.9392 0.953 ## 58.2 4172 1 0.9456 0.003406 0.9390 0.952 ## 58.3 4171 1 0.9454 0.003412 0.9387 0.952 ## 58.3 4170 1 0.9452 0.003419 0.9385 0.952 ## 58.3 4169 2 0.9447 0.003432 0.9380 0.951 ## 58.3 4167 1 0.9445 0.003439 0.9378 0.951 ## 58.4 4166 1 0.9443 0.003446 0.9375 0.951 ## 58.5 4165 1 0.9440 0.003452 0.9373 0.951 ## 58.5 4164 1 0.9438 0.003459 0.9371 0.951 ## 58.5 4163 1 0.9436 0.003465 0.9368 0.950 ## 58.5 4162 1 0.9434 0.003472 0.9366 0.950 ## 58.6 4161 1 0.9431 0.003479 0.9363 0.950 ## 58.6 4160 1 0.9429 0.003485 0.9361 0.950 ## 58.6 4159 1 0.9427 0.003492 0.9359 0.950 ## 58.6 4158 1 0.9425 0.003498 0.9356 0.949 ## 58.6 4157 1 0.9422 0.003505 0.9354 0.949 ## 58.7 4156 1 0.9420 0.003511 0.9351 0.949 ## 58.7 4155 1 0.9418 0.003518 0.9349 0.949 ## 58.8 4154 1 0.9416 0.003524 0.9347 0.948 ## 58.8 4153 1 0.9413 0.003531 0.9344 0.948 ## 58.8 4152 1 0.9411 0.003537 0.9342 0.948 ## 58.8 4151 1 0.9409 0.003543 0.9340 0.948 ## 58.8 4150 1 0.9406 0.003550 0.9337 0.948 ## 58.8 4149 1 0.9404 0.003556 0.9335 0.947 ## 58.8 4148 1 0.9402 0.003563 0.9332 0.947 ## 58.8 4147 1 0.9400 0.003569 0.9330 0.947 ## 59.0 4146 1 0.9397 0.003575 0.9328 0.947 ## 59.0 4106 1 0.9395 0.003582 0.9325 0.947 ## 59.1 4105 1 0.9393 0.003588 0.9323 0.946 ## 59.1 4104 1 0.9391 0.003594 0.9320 0.946 ## 59.1 4103 1 0.9388 0.003601 0.9318 0.946 ## 59.1 4102 1 0.9386 0.003607 0.9316 0.946 ## 59.1 4101 1 0.9384 0.003614 0.9313 0.945 ## 59.1 4100 1 0.9381 0.003620 0.9311 0.945 ## 59.2 4099 1 0.9379 0.003626 0.9308 0.945 ## 59.2 4098 1 0.9377 0.003633 0.9306 0.945 ## 59.2 4097 1 0.9374 0.003639 0.9303 0.945 ## 59.3 4096 1 0.9372 0.003645 0.9301 0.944 ## 59.3 4095 1 0.9370 0.003652 0.9299 0.944 ## 59.3 4094 1 0.9368 0.003658 0.9296 0.944 ## 59.3 4093 1 0.9365 0.003664 0.9294 0.944 ## 59.3 4092 1 0.9363 0.003670 0.9291 0.944 ## 59.4 4091 1 0.9361 0.003677 0.9289 0.943 ## 59.4 4090 1 0.9358 0.003683 0.9287 0.943 ## 59.4 4089 1 0.9356 0.003689 0.9284 0.943 ## 59.5 4088 1 0.9354 0.003695 0.9282 0.943 ## 59.5 4087 1 0.9352 0.003701 0.9279 0.942 ## 59.5 4086 1 0.9349 0.003708 0.9277 0.942 ## 59.6 4085 1 0.9347 0.003714 0.9275 0.942 ## 59.7 4084 1 0.9345 0.003720 0.9272 0.942 ## 59.7 4083 1 0.9342 0.003726 0.9270 0.942 ## 59.7 4082 1 0.9340 0.003732 0.9267 0.941 ## 59.7 4081 1 0.9338 0.003738 0.9265 0.941 ## 59.8 4080 1 0.9336 0.003744 0.9262 0.941 ## 59.8 4079 1 0.9333 0.003750 0.9260 0.941 ## 59.8 4078 1 0.9331 0.003756 0.9258 0.940 ## 59.9 4077 1 0.9329 0.003762 0.9255 0.940 ## 60.0 4000 1 0.9326 0.003769 0.9253 0.940 ## 60.1 3999 1 0.9324 0.003775 0.9250 0.940 ## 60.1 3998 1 0.9322 0.003781 0.9248 0.940 ## 60.2 3997 1 0.9319 0.003787 0.9245 0.939 ## 60.2 3996 1 0.9317 0.003794 0.9243 0.939 ## 60.2 3995 1 0.9315 0.003800 0.9241 0.939 ## 60.2 3994 1 0.9312 0.003806 0.9238 0.939 ## 60.3 3993 1 0.9310 0.003812 0.9236 0.939 ## 60.3 3992 1 0.9308 0.003818 0.9233 0.938 ## 60.3 3991 1 0.9305 0.003825 0.9231 0.938 ## 60.3 3990 1 0.9303 0.003831 0.9228 0.938 ## 60.4 3989 1 0.9301 0.003837 0.9226 0.938 ## 60.4 3988 1 0.9298 0.003843 0.9223 0.937 ## 60.4 3987 1 0.9296 0.003849 0.9221 0.937 ## 60.5 3986 2 0.9291 0.003861 0.9216 0.937 ## 60.5 3984 1 0.9289 0.003867 0.9214 0.937 ## 60.5 3983 1 0.9287 0.003873 0.9211 0.936 ## 60.5 3982 1 0.9284 0.003879 0.9209 0.936 ## 60.6 3981 1 0.9282 0.003885 0.9206 0.936 ## 60.6 3980 1 0.9280 0.003892 0.9204 0.936 ## 60.7 3979 1 0.9277 0.003898 0.9201 0.935 ## 60.7 3978 1 0.9275 0.003904 0.9199 0.935 ## 60.7 3977 1 0.9273 0.003909 0.9196 0.935 ## 60.7 3976 1 0.9270 0.003915 0.9194 0.935 ## 60.8 3975 1 0.9268 0.003921 0.9192 0.935 ## 60.8 3974 1 0.9266 0.003927 0.9189 0.934 ## 60.8 3973 1 0.9263 0.003933 0.9187 0.934 ## 60.8 3972 1 0.9261 0.003939 0.9184 0.934 ## 60.8 3971 1 0.9259 0.003945 0.9182 0.934 ## 60.8 3970 1 0.9256 0.003951 0.9179 0.933 ## 60.9 3969 1 0.9254 0.003957 0.9177 0.933 ## 60.9 3968 1 0.9252 0.003963 0.9174 0.933 ## 60.9 3967 1 0.9249 0.003969 0.9172 0.933 ## 60.9 3966 1 0.9247 0.003974 0.9170 0.933 ## 60.9 3965 1 0.9245 0.003980 0.9167 0.932 ## 61.0 3964 1 0.9242 0.003986 0.9165 0.932 ## 61.0 3963 1 0.9240 0.003992 0.9162 0.932 ## 61.0 3962 1 0.9238 0.003998 0.9160 0.932 ## 61.0 3874 1 0.9235 0.004004 0.9157 0.931 ## 61.0 3873 1 0.9233 0.004010 0.9155 0.931 ## 61.0 3872 1 0.9231 0.004016 0.9152 0.931 ## 61.1 3871 1 0.9228 0.004022 0.9150 0.931 ## 61.1 3870 1 0.9226 0.004028 0.9147 0.931 ## 61.1 3869 1 0.9223 0.004034 0.9145 0.930 ## 61.1 3868 1 0.9221 0.004040 0.9142 0.930 ## 61.2 3867 1 0.9219 0.004046 0.9140 0.930 ## 61.2 3866 1 0.9216 0.004052 0.9137 0.930 ## 61.2 3865 1 0.9214 0.004058 0.9135 0.929 ## 61.2 3864 1 0.9212 0.004064 0.9132 0.929 ## 61.2 3863 1 0.9209 0.004070 0.9130 0.929 ## 61.2 3862 1 0.9207 0.004076 0.9127 0.929 ## 61.2 3861 1 0.9204 0.004082 0.9125 0.928 ## 61.3 3860 1 0.9202 0.004088 0.9122 0.928 ## 61.4 3859 1 0.9200 0.004093 0.9120 0.928 ## 61.4 3858 1 0.9197 0.004099 0.9117 0.928 ## 61.4 3857 1 0.9195 0.004105 0.9115 0.928 ## 61.5 3856 1 0.9192 0.004111 0.9112 0.927 ## 61.5 3855 1 0.9190 0.004117 0.9110 0.927 ## 61.5 3854 1 0.9188 0.004123 0.9107 0.927 ## 61.6 3853 1 0.9185 0.004129 0.9105 0.927 ## 61.6 3852 1 0.9183 0.004134 0.9102 0.926 ## 61.6 3851 1 0.9181 0.004140 0.9100 0.926 ## 61.6 3850 1 0.9178 0.004146 0.9097 0.926 ## 61.6 3849 1 0.9176 0.004152 0.9095 0.926 ## 61.6 3848 1 0.9173 0.004157 0.9092 0.926 ## 61.6 3847 1 0.9171 0.004163 0.9090 0.925 ## 61.6 3846 1 0.9169 0.004169 0.9087 0.925 ## 61.6 3845 1 0.9166 0.004175 0.9085 0.925 ## 61.7 3844 1 0.9164 0.004180 0.9082 0.925 ## 61.7 3843 1 0.9161 0.004186 0.9080 0.924 ## 61.7 3842 1 0.9159 0.004192 0.9077 0.924 ## 61.7 3841 1 0.9157 0.004197 0.9075 0.924 ## 61.8 3840 1 0.9154 0.004203 0.9072 0.924 ## 61.8 3839 1 0.9152 0.004209 0.9070 0.923 ## 61.8 3838 1 0.9150 0.004214 0.9067 0.923 ## 61.8 3837 1 0.9147 0.004220 0.9065 0.923 ## 61.8 3836 1 0.9145 0.004226 0.9062 0.923 ## 61.8 3835 1 0.9142 0.004231 0.9060 0.923 ## 61.9 3834 1 0.9140 0.004237 0.9057 0.922 ## 61.9 3833 1 0.9138 0.004243 0.9055 0.922 ## 61.9 3832 1 0.9135 0.004248 0.9052 0.922 ## 61.9 3831 1 0.9133 0.004254 0.9050 0.922 ## 61.9 3830 1 0.9130 0.004259 0.9047 0.921 ## 61.9 3829 1 0.9128 0.004265 0.9045 0.921 ## 62.0 3828 1 0.9126 0.004270 0.9042 0.921 ## 62.0 3827 1 0.9123 0.004276 0.9040 0.921 ## 62.0 3699 1 0.9121 0.004282 0.9037 0.921 ## 62.0 3698 1 0.9118 0.004288 0.9035 0.920 ## 62.1 3697 1 0.9116 0.004294 0.9032 0.920 ## 62.1 3696 1 0.9113 0.004300 0.9030 0.920 ## 62.1 3695 2 0.9109 0.004311 0.9024 0.919 ## 62.1 3693 1 0.9106 0.004317 0.9022 0.919 ## 62.2 3692 1 0.9104 0.004323 0.9019 0.919 ## 62.3 3691 1 0.9101 0.004329 0.9017 0.919 ## 62.3 3690 1 0.9099 0.004335 0.9014 0.918 ## 62.3 3689 1 0.9096 0.004341 0.9011 0.918 ## 62.3 3688 1 0.9094 0.004347 0.9009 0.918 ## 62.3 3687 1 0.9091 0.004352 0.9006 0.918 ## 62.3 3686 1 0.9089 0.004358 0.9004 0.917 ## 62.3 3685 1 0.9086 0.004364 0.9001 0.917 ## 62.3 3684 1 0.9084 0.004370 0.8999 0.917 ## 62.3 3683 1 0.9081 0.004376 0.8996 0.917 ## 62.3 3682 1 0.9079 0.004381 0.8993 0.917 ## 62.4 3681 1 0.9076 0.004387 0.8991 0.916 ## 62.4 3680 1 0.9074 0.004393 0.8988 0.916 ## 62.4 3679 2 0.9069 0.004404 0.8983 0.916 ## 62.4 3677 1 0.9067 0.004410 0.8981 0.915 ## 62.4 3676 1 0.9064 0.004416 0.8978 0.915 ## 62.4 3675 1 0.9062 0.004421 0.8975 0.915 ## 62.5 3674 1 0.9059 0.004427 0.8973 0.915 ## 62.5 3673 1 0.9057 0.004433 0.8970 0.914 ## 62.5 3672 1 0.9054 0.004438 0.8968 0.914 ## 62.5 3671 1 0.9052 0.004444 0.8965 0.914 ## 62.5 3670 1 0.9049 0.004450 0.8963 0.914 ## 62.5 3669 1 0.9047 0.004455 0.8960 0.913 ## 62.5 3668 1 0.9044 0.004461 0.8957 0.913 ## 62.6 3667 1 0.9042 0.004466 0.8955 0.913 ## 62.6 3666 1 0.9039 0.004472 0.8952 0.913 ## 62.6 3665 1 0.9037 0.004478 0.8950 0.913 ## 62.6 3664 1 0.9035 0.004483 0.8947 0.912 ## 62.6 3663 1 0.9032 0.004489 0.8945 0.912 ## 62.6 3662 1 0.9030 0.004494 0.8942 0.912 ## 62.6 3661 1 0.9027 0.004500 0.8939 0.912 ## 62.7 3660 1 0.9025 0.004505 0.8937 0.911 ## 62.7 3659 1 0.9022 0.004511 0.8934 0.911 ## 62.7 3658 1 0.9020 0.004516 0.8932 0.911 ## 62.8 3657 1 0.9017 0.004522 0.8929 0.911 ## 62.8 3656 1 0.9015 0.004527 0.8926 0.910 ## 62.8 3655 1 0.9012 0.004533 0.8924 0.910 ## 62.8 3654 1 0.9010 0.004538 0.8921 0.910 ## 62.8 3653 1 0.9007 0.004544 0.8919 0.910 ## 62.8 3652 1 0.9005 0.004549 0.8916 0.909 ## 62.8 3651 1 0.9002 0.004555 0.8914 0.909 ## 62.9 3650 1 0.9000 0.004560 0.8911 0.909 ## 62.9 3649 1 0.8998 0.004565 0.8908 0.909 ## 62.9 3648 1 0.8995 0.004571 0.8906 0.909 ## 62.9 3647 1 0.8993 0.004576 0.8903 0.908 ## 62.9 3646 1 0.8990 0.004582 0.8901 0.908 ## 62.9 3645 1 0.8988 0.004587 0.8898 0.908 ## 63.0 3644 1 0.8985 0.004592 0.8896 0.908 ## 63.0 3643 1 0.8983 0.004598 0.8893 0.907 ## 63.0 3493 1 0.8980 0.004604 0.8890 0.907 ## 63.1 3492 1 0.8978 0.004609 0.8888 0.907 ## 63.1 3491 1 0.8975 0.004615 0.8885 0.907 ## 63.1 3490 1 0.8972 0.004621 0.8882 0.906 ## 63.1 3489 1 0.8970 0.004627 0.8880 0.906 ## 63.2 3488 1 0.8967 0.004633 0.8877 0.906 ## 63.2 3487 1 0.8965 0.004639 0.8874 0.906 ## 63.2 3486 1 0.8962 0.004644 0.8872 0.905 ## 63.2 3485 1 0.8960 0.004650 0.8869 0.905 ## 63.3 3484 1 0.8957 0.004656 0.8866 0.905 ## 63.3 3483 1 0.8954 0.004662 0.8864 0.905 ## 63.3 3482 1 0.8952 0.004667 0.8861 0.904 ## 63.3 3481 1 0.8949 0.004673 0.8858 0.904 ## 63.4 3480 1 0.8947 0.004679 0.8855 0.904 ## 63.4 3479 1 0.8944 0.004685 0.8853 0.904 ## 63.4 3478 1 0.8942 0.004690 0.8850 0.903 ## 63.4 3477 1 0.8939 0.004696 0.8847 0.903 ## 63.4 3476 1 0.8936 0.004702 0.8845 0.903 ## 63.4 3475 1 0.8934 0.004707 0.8842 0.903 ## 63.5 3474 1 0.8931 0.004713 0.8839 0.902 ## 63.5 3473 1 0.8929 0.004719 0.8837 0.902 ## 63.5 3472 1 0.8926 0.004724 0.8834 0.902 ## 63.5 3471 1 0.8924 0.004730 0.8831 0.902 ## 63.5 3470 1 0.8921 0.004736 0.8829 0.901 ## 63.5 3469 1 0.8918 0.004741 0.8826 0.901 ## 63.6 3468 1 0.8916 0.004747 0.8823 0.901 ## 63.6 3467 1 0.8913 0.004752 0.8821 0.901 ## 63.6 3466 1 0.8911 0.004758 0.8818 0.900 ## 63.7 3465 1 0.8908 0.004763 0.8815 0.900 ## 63.7 3464 1 0.8906 0.004769 0.8813 0.900 ## 63.7 3463 1 0.8903 0.004775 0.8810 0.900 ## 63.8 3462 1 0.8900 0.004780 0.8807 0.899 ## 63.8 3461 1 0.8898 0.004786 0.8805 0.899 ## 63.9 3460 1 0.8895 0.004791 0.8802 0.899 ## 63.9 3459 1 0.8893 0.004797 0.8799 0.899 ## 63.9 3458 1 0.8890 0.004802 0.8797 0.898 ## 64.0 3457 1 0.8888 0.004808 0.8794 0.898 ## 64.0 3294 1 0.8885 0.004814 0.8791 0.898 ## 64.0 3293 1 0.8882 0.004820 0.8788 0.898 ## 64.0 3292 1 0.8879 0.004826 0.8785 0.897 ## 64.0 3291 1 0.8877 0.004832 0.8783 0.897 ## 64.1 3290 1 0.8874 0.004838 0.8780 0.897 ## 64.1 3289 1 0.8871 0.004844 0.8777 0.897 ## 64.2 3288 1 0.8869 0.004850 0.8774 0.896 ## 64.2 3287 1 0.8866 0.004856 0.8771 0.896 ## 64.2 3286 1 0.8863 0.004862 0.8769 0.896 ## 64.2 3285 1 0.8861 0.004868 0.8766 0.896 ## 64.3 3284 1 0.8858 0.004874 0.8763 0.895 ## 64.3 3283 1 0.8855 0.004880 0.8760 0.895 ## 64.3 3282 1 0.8852 0.004886 0.8757 0.895 ## 64.3 3281 1 0.8850 0.004892 0.8754 0.895 ## 64.3 3280 1 0.8847 0.004898 0.8752 0.894 ## 64.3 3279 1 0.8844 0.004904 0.8749 0.894 ## 64.3 3278 1 0.8842 0.004910 0.8746 0.894 ## 64.3 3277 1 0.8839 0.004916 0.8743 0.894 ## 64.3 3276 1 0.8836 0.004922 0.8740 0.893 ## 64.4 3275 2 0.8831 0.004933 0.8735 0.893 ## 64.4 3273 2 0.8826 0.004945 0.8729 0.892 ## 64.4 3271 1 0.8823 0.004951 0.8726 0.892 ## 64.4 3270 1 0.8820 0.004957 0.8723 0.892 ## 64.4 3269 1 0.8817 0.004963 0.8721 0.892 ## 64.5 3268 1 0.8815 0.004969 0.8718 0.891 ## 64.5 3267 1 0.8812 0.004974 0.8715 0.891 ## 64.5 3266 1 0.8809 0.004980 0.8712 0.891 ## 64.6 3265 1 0.8807 0.004986 0.8709 0.890 ## 64.6 3264 1 0.8804 0.004992 0.8707 0.890 ## 64.6 3263 1 0.8801 0.004997 0.8704 0.890 ## 64.6 3262 1 0.8799 0.005003 0.8701 0.890 ## 64.6 3261 1 0.8796 0.005009 0.8698 0.889 ## 64.7 3260 1 0.8793 0.005015 0.8695 0.889 ## 64.7 3259 1 0.8790 0.005020 0.8693 0.889 ## 64.7 3258 1 0.8788 0.005026 0.8690 0.889 ## 64.7 3257 1 0.8785 0.005032 0.8687 0.888 ## 64.7 3256 1 0.8782 0.005037 0.8684 0.888 ## 64.7 3255 1 0.8780 0.005043 0.8681 0.888 ## 64.7 3254 1 0.8777 0.005049 0.8679 0.888 ## 64.8 3253 1 0.8774 0.005054 0.8676 0.887 ## 64.8 3252 1 0.8772 0.005060 0.8673 0.887 ## 64.8 3251 1 0.8769 0.005066 0.8670 0.887 ## 64.8 3250 1 0.8766 0.005071 0.8667 0.887 ## 64.9 3249 1 0.8763 0.005077 0.8665 0.886 ## 65.0 3248 1 0.8761 0.005083 0.8662 0.886 ## 65.0 3095 1 0.8758 0.005089 0.8659 0.886 ## 65.0 3094 1 0.8755 0.005095 0.8656 0.886 ## 65.0 3093 1 0.8752 0.005101 0.8653 0.885 ## 65.1 3092 1 0.8749 0.005107 0.8650 0.885 ## 65.1 3091 1 0.8747 0.005114 0.8647 0.885 ## 65.1 3090 1 0.8744 0.005120 0.8644 0.884 ## 65.2 3089 1 0.8741 0.005126 0.8641 0.884 ## 65.2 3088 1 0.8738 0.005132 0.8638 0.884 ## 65.2 3087 1 0.8735 0.005138 0.8635 0.884 ## 65.2 3086 1 0.8732 0.005144 0.8632 0.883 ## 65.2 3085 1 0.8730 0.005150 0.8629 0.883 ## 65.2 3084 1 0.8727 0.005157 0.8626 0.883 ## 65.3 3083 1 0.8724 0.005163 0.8623 0.883 ## 65.3 3082 1 0.8721 0.005169 0.8620 0.882 ## 65.3 3081 1 0.8718 0.005175 0.8617 0.882 ## 65.3 3080 1 0.8715 0.005181 0.8615 0.882 ## 65.3 3079 1 0.8713 0.005187 0.8612 0.881 ## 65.3 3078 1 0.8710 0.005193 0.8609 0.881 ## 65.4 3077 1 0.8707 0.005199 0.8606 0.881 ## 65.4 3076 1 0.8704 0.005205 0.8603 0.881 ## 65.4 3075 1 0.8701 0.005211 0.8600 0.880 ## 65.4 3074 2 0.8696 0.005223 0.8594 0.880 ## 65.4 3072 1 0.8693 0.005229 0.8591 0.880 ## 65.4 3071 1 0.8690 0.005235 0.8588 0.879 ## 65.4 3070 1 0.8687 0.005241 0.8585 0.879 ## 65.4 3069 1 0.8684 0.005247 0.8582 0.879 ## 65.5 3068 1 0.8682 0.005253 0.8579 0.879 ## 65.5 3067 1 0.8679 0.005259 0.8576 0.878 ## 65.5 3066 1 0.8676 0.005264 0.8573 0.878 ## 65.6 3065 1 0.8673 0.005270 0.8570 0.878 ## 65.6 3064 1 0.8670 0.005276 0.8567 0.877 ## 65.6 3063 1 0.8667 0.005282 0.8564 0.877 ## 65.6 3062 1 0.8665 0.005288 0.8561 0.877 ## 65.6 3061 1 0.8662 0.005294 0.8559 0.877 ## 65.6 3060 1 0.8659 0.005300 0.8556 0.876 ## 65.6 3059 1 0.8656 0.005305 0.8553 0.876 ## 65.6 3058 1 0.8653 0.005311 0.8550 0.876 ## 65.6 3057 1 0.8650 0.005317 0.8547 0.876 ## 65.6 3056 1 0.8648 0.005323 0.8544 0.875 ## 65.6 3055 1 0.8645 0.005329 0.8541 0.875 ## 65.7 3054 1 0.8642 0.005334 0.8538 0.875 ## 65.7 3053 1 0.8639 0.005340 0.8535 0.874 ## 65.7 3052 1 0.8636 0.005346 0.8532 0.874 ## 65.7 3051 1 0.8633 0.005352 0.8529 0.874 ## 65.7 3050 1 0.8631 0.005357 0.8526 0.874 ## 65.8 3049 1 0.8628 0.005363 0.8523 0.873 ## 65.8 3048 1 0.8625 0.005369 0.8520 0.873 ## 65.8 3047 1 0.8622 0.005374 0.8517 0.873 ## 65.8 3046 1 0.8619 0.005380 0.8514 0.873 ## 65.8 3045 1 0.8616 0.005386 0.8511 0.872 ## 65.8 3044 1 0.8614 0.005391 0.8509 0.872 ## 65.8 3043 1 0.8611 0.005397 0.8506 0.872 ## 65.9 3042 1 0.8608 0.005403 0.8503 0.871 ## 65.9 3041 1 0.8605 0.005408 0.8500 0.871 ## 65.9 3040 1 0.8602 0.005414 0.8497 0.871 ## 65.9 3039 1 0.8599 0.005420 0.8494 0.871 ## 65.9 3038 1 0.8597 0.005425 0.8491 0.870 ## 65.9 3037 1 0.8594 0.005431 0.8488 0.870 ## 65.9 3036 1 0.8591 0.005436 0.8485 0.870 ## 66.0 3035 1 0.8588 0.005442 0.8482 0.870 ## 66.0 3034 1 0.8585 0.005448 0.8479 0.869 ## 66.0 3033 1 0.8582 0.005453 0.8476 0.869 ## 66.0 3032 1 0.8580 0.005459 0.8473 0.869 ## 66.0 3031 1 0.8577 0.005464 0.8470 0.868 ## 66.0 2877 1 0.8574 0.005470 0.8467 0.868 ## 66.0 2876 1 0.8571 0.005477 0.8464 0.868 ## 66.0 2875 1 0.8568 0.005483 0.8461 0.868 ## 66.0 2874 1 0.8565 0.005489 0.8458 0.867 ## 66.0 2873 1 0.8562 0.005495 0.8455 0.867 ## 66.0 2872 1 0.8559 0.005501 0.8452 0.867 ## 66.1 2871 1 0.8556 0.005507 0.8449 0.866 ## 66.1 2870 1 0.8553 0.005514 0.8446 0.866 ## 66.2 2869 1 0.8550 0.005520 0.8442 0.866 ## 66.2 2868 1 0.8547 0.005526 0.8439 0.866 ## 66.2 2867 1 0.8544 0.005532 0.8436 0.865 ## 66.2 2866 1 0.8541 0.005538 0.8433 0.865 ## 66.2 2865 1 0.8538 0.005544 0.8430 0.865 ## 66.3 2864 1 0.8535 0.005550 0.8427 0.864 ## 66.4 2863 1 0.8532 0.005556 0.8424 0.864 ## 66.4 2862 1 0.8529 0.005562 0.8421 0.864 ## 66.4 2861 1 0.8526 0.005568 0.8418 0.864 ## 66.5 2860 1 0.8523 0.005574 0.8415 0.863 ## 66.5 2859 1 0.8520 0.005580 0.8411 0.863 ## 66.5 2858 1 0.8517 0.005586 0.8408 0.863 ## 66.5 2857 1 0.8514 0.005592 0.8405 0.862 ## 66.5 2856 1 0.8511 0.005598 0.8402 0.862 ## 66.6 2855 1 0.8508 0.005604 0.8399 0.862 ## 66.6 2854 1 0.8505 0.005610 0.8396 0.862 ## 66.6 2853 1 0.8502 0.005616 0.8393 0.861 ## 66.6 2852 1 0.8499 0.005622 0.8390 0.861 ## 66.6 2851 1 0.8496 0.005628 0.8387 0.861 ## 66.6 2850 1 0.8493 0.005634 0.8384 0.860 ## 66.7 2849 1 0.8490 0.005640 0.8380 0.860 ## 66.7 2848 1 0.8487 0.005646 0.8377 0.860 ## 66.7 2847 1 0.8484 0.005652 0.8374 0.860 ## 66.7 2846 1 0.8481 0.005658 0.8371 0.859 ## 66.8 2845 1 0.8478 0.005663 0.8368 0.859 ## 66.8 2844 1 0.8475 0.005669 0.8365 0.859 ## 66.8 2843 1 0.8472 0.005675 0.8362 0.858 ## 66.8 2842 1 0.8469 0.005681 0.8359 0.858 ## 66.9 2841 1 0.8466 0.005687 0.8356 0.858 ## 66.9 2840 1 0.8463 0.005693 0.8353 0.858 ## 67.0 2839 1 0.8461 0.005698 0.8350 0.857 ## 67.0 2838 1 0.8458 0.005704 0.8346 0.857 ## 67.0 2705 1 0.8454 0.005711 0.8343 0.857 ## 67.0 2704 1 0.8451 0.005717 0.8340 0.856 ## 67.0 2703 1 0.8448 0.005724 0.8337 0.856 ## 67.1 2702 1 0.8445 0.005730 0.8333 0.856 ## 67.1 2701 1 0.8442 0.005736 0.8330 0.856 ## 67.1 2700 1 0.8439 0.005743 0.8327 0.855 ## 67.2 2699 1 0.8436 0.005749 0.8324 0.855 ## 67.2 2698 1 0.8433 0.005755 0.8320 0.855 ## 67.2 2697 1 0.8429 0.005762 0.8317 0.854 ## 67.2 2696 1 0.8426 0.005768 0.8314 0.854 ## 67.3 2695 1 0.8423 0.005774 0.8311 0.854 ## 67.3 2694 1 0.8420 0.005781 0.8307 0.853 ## 67.3 2693 1 0.8417 0.005787 0.8304 0.853 ## 67.4 2692 1 0.8414 0.005793 0.8301 0.853 ## 67.4 2691 1 0.8411 0.005800 0.8298 0.853 ## 67.4 2690 1 0.8407 0.005806 0.8294 0.852 ## 67.4 2689 1 0.8404 0.005812 0.8291 0.852 ## 67.4 2688 1 0.8401 0.005818 0.8288 0.852 ## 67.4 2687 1 0.8398 0.005825 0.8285 0.851 ## 67.4 2686 1 0.8395 0.005831 0.8281 0.851 ## 67.5 2685 1 0.8392 0.005837 0.8278 0.851 ## 67.5 2684 1 0.8389 0.005843 0.8275 0.850 ## 67.5 2683 1 0.8386 0.005849 0.8272 0.850 ## 67.5 2682 1 0.8382 0.005856 0.8268 0.850 ## 67.5 2681 1 0.8379 0.005862 0.8265 0.850 ## 67.6 2680 1 0.8376 0.005868 0.8262 0.849 ## 67.6 2679 1 0.8373 0.005874 0.8259 0.849 ## 67.6 2678 1 0.8370 0.005880 0.8256 0.849 ## 67.6 2677 1 0.8367 0.005886 0.8252 0.848 ## 67.6 2676 1 0.8364 0.005892 0.8249 0.848 ## 67.6 2675 1 0.8361 0.005898 0.8246 0.848 ## 67.6 2674 1 0.8357 0.005905 0.8243 0.847 ## 67.6 2673 1 0.8354 0.005911 0.8239 0.847 ## 67.7 2672 1 0.8351 0.005917 0.8236 0.847 ## 67.7 2671 1 0.8348 0.005923 0.8233 0.846 ## 67.7 2670 1 0.8345 0.005929 0.8230 0.846 ## 67.7 2669 1 0.8342 0.005935 0.8226 0.846 ## 67.7 2668 1 0.8339 0.005941 0.8223 0.846 ## 67.7 2667 1 0.8336 0.005947 0.8220 0.845 ## 67.8 2666 1 0.8332 0.005953 0.8217 0.845 ## 67.8 2665 1 0.8329 0.005959 0.8213 0.845 ## 67.9 2664 1 0.8326 0.005965 0.8210 0.844 ## 67.9 2663 1 0.8323 0.005971 0.8207 0.844 ## 67.9 2662 1 0.8320 0.005977 0.8204 0.844 ## 67.9 2661 1 0.8317 0.005982 0.8200 0.843 ## 67.9 2660 1 0.8314 0.005988 0.8197 0.843 ## 67.9 2659 1 0.8311 0.005994 0.8194 0.843 ## 67.9 2658 1 0.8307 0.006000 0.8191 0.843 ## 67.9 2657 1 0.8304 0.006006 0.8187 0.842 ## 68.0 2656 1 0.8301 0.006012 0.8184 0.842 ## 68.0 2655 1 0.8298 0.006018 0.8181 0.842 ## 68.0 2530 1 0.8295 0.006024 0.8178 0.841 ## 68.0 2529 1 0.8292 0.006031 0.8174 0.841 ## 68.0 2528 1 0.8288 0.006037 0.8171 0.841 ## 68.1 2527 1 0.8285 0.006044 0.8167 0.840 ## 68.1 2526 1 0.8282 0.006050 0.8164 0.840 ## 68.1 2525 1 0.8278 0.006057 0.8161 0.840 ## 68.1 2524 1 0.8275 0.006063 0.8157 0.839 ## 68.1 2523 1 0.8272 0.006070 0.8154 0.839 ## 68.1 2522 1 0.8269 0.006076 0.8150 0.839 ## 68.1 2521 1 0.8265 0.006083 0.8147 0.839 ## 68.1 2520 1 0.8262 0.006089 0.8143 0.838 ## 68.1 2519 1 0.8259 0.006096 0.8140 0.838 ## 68.1 2518 1 0.8255 0.006102 0.8137 0.838 ## 68.2 2517 1 0.8252 0.006108 0.8133 0.837 ## 68.2 2516 1 0.8249 0.006115 0.8130 0.837 ## 68.2 2515 1 0.8246 0.006121 0.8126 0.837 ## 68.2 2514 1 0.8242 0.006127 0.8123 0.836 ## 68.2 2513 1 0.8239 0.006134 0.8120 0.836 ## 68.2 2512 1 0.8236 0.006140 0.8116 0.836 ## 68.3 2511 1 0.8232 0.006146 0.8113 0.835 ## 68.3 2510 1 0.8229 0.006153 0.8109 0.835 ## 68.4 2509 1 0.8226 0.006159 0.8106 0.835 ## 68.4 2508 1 0.8223 0.006165 0.8103 0.834 ## 68.4 2507 1 0.8219 0.006172 0.8099 0.834 ## 68.4 2506 1 0.8216 0.006178 0.8096 0.834 ## 68.4 2505 1 0.8213 0.006184 0.8092 0.833 ## 68.5 2504 1 0.8210 0.006190 0.8089 0.833 ## 68.6 2503 1 0.8206 0.006196 0.8086 0.833 ## 68.6 2502 1 0.8203 0.006203 0.8082 0.833 ## 68.6 2501 1 0.8200 0.006209 0.8079 0.832 ## 68.6 2500 1 0.8196 0.006215 0.8075 0.832 ## 68.6 2499 1 0.8193 0.006221 0.8072 0.832 ## 68.6 2498 1 0.8190 0.006227 0.8069 0.831 ## 68.7 2497 1 0.8187 0.006233 0.8065 0.831 ## 68.7 2496 1 0.8183 0.006240 0.8062 0.831 ## 68.7 2495 1 0.8180 0.006246 0.8058 0.830 ## 68.7 2494 1 0.8177 0.006252 0.8055 0.830 ## 68.8 2493 1 0.8173 0.006258 0.8052 0.830 ## 68.8 2492 1 0.8170 0.006264 0.8048 0.829 ## 68.8 2491 1 0.8167 0.006270 0.8045 0.829 ## 68.8 2490 1 0.8164 0.006276 0.8042 0.829 ## 68.9 2489 1 0.8160 0.006282 0.8038 0.828 ## 68.9 2488 1 0.8157 0.006288 0.8035 0.828 ## 68.9 2487 1 0.8154 0.006294 0.8031 0.828 ## 68.9 2486 2 0.8147 0.006306 0.8025 0.827 ## 69.0 2484 1 0.8144 0.006312 0.8021 0.827 ## 69.0 2483 1 0.8141 0.006318 0.8018 0.827 ## 69.0 2358 1 0.8137 0.006325 0.8014 0.826 ## 69.0 2357 1 0.8134 0.006332 0.8011 0.826 ## 69.0 2356 1 0.8130 0.006338 0.8007 0.826 ## 69.0 2355 1 0.8127 0.006345 0.8003 0.825 ## 69.1 2354 1 0.8123 0.006352 0.8000 0.825 ## 69.1 2353 1 0.8120 0.006358 0.7996 0.825 ## 69.1 2352 1 0.8116 0.006365 0.7993 0.824 ## 69.1 2351 1 0.8113 0.006372 0.7989 0.824 ## 69.1 2350 1 0.8110 0.006378 0.7986 0.824 ## 69.2 2349 1 0.8106 0.006385 0.7982 0.823 ## 69.2 2348 1 0.8103 0.006392 0.7978 0.823 ## 69.2 2347 1 0.8099 0.006398 0.7975 0.823 ## 69.2 2346 1 0.8096 0.006405 0.7971 0.822 ## 69.2 2345 1 0.8092 0.006411 0.7968 0.822 ## 69.2 2344 1 0.8089 0.006418 0.7964 0.822 ## 69.2 2343 1 0.8085 0.006424 0.7960 0.821 ## 69.2 2342 1 0.8082 0.006431 0.7957 0.821 ## 69.2 2341 1 0.8078 0.006437 0.7953 0.821 ## 69.3 2340 1 0.8075 0.006444 0.7950 0.820 ## 69.3 2339 1 0.8072 0.006450 0.7946 0.820 ## 69.3 2338 1 0.8068 0.006457 0.7943 0.820 ## 69.3 2337 1 0.8065 0.006463 0.7939 0.819 ## 69.3 2336 1 0.8061 0.006470 0.7935 0.819 ## 69.3 2335 1 0.8058 0.006476 0.7932 0.819 ## 69.3 2334 1 0.8054 0.006483 0.7928 0.818 ## 69.3 2333 1 0.8051 0.006489 0.7925 0.818 ## 69.3 2332 1 0.8047 0.006495 0.7921 0.818 ## 69.4 2331 1 0.8044 0.006502 0.7918 0.817 ## 69.4 2330 1 0.8041 0.006508 0.7914 0.817 ## 69.4 2329 1 0.8037 0.006515 0.7910 0.817 ## 69.5 2328 1 0.8034 0.006521 0.7907 0.816 ## 69.5 2327 1 0.8030 0.006527 0.7903 0.816 ## 69.5 2326 1 0.8027 0.006534 0.7900 0.816 ## 69.5 2325 1 0.8023 0.006540 0.7896 0.815 ## 69.5 2324 1 0.8020 0.006546 0.7893 0.815 ## 69.5 2323 1 0.8016 0.006552 0.7889 0.815 ## 69.5 2322 1 0.8013 0.006559 0.7885 0.814 ## 69.5 2321 1 0.8009 0.006565 0.7882 0.814 ## 69.5 2320 1 0.8006 0.006571 0.7878 0.814 ## 69.5 2319 1 0.8003 0.006577 0.7875 0.813 ## 69.6 2318 1 0.7999 0.006584 0.7871 0.813 ## 69.6 2317 1 0.7996 0.006590 0.7868 0.813 ## 69.7 2316 1 0.7992 0.006596 0.7864 0.812 ## 69.7 2315 1 0.7989 0.006602 0.7860 0.812 ## 69.7 2314 1 0.7985 0.006608 0.7857 0.812 ## 69.7 2313 1 0.7982 0.006615 0.7853 0.811 ## 69.7 2312 1 0.7978 0.006621 0.7850 0.811 ## 69.8 2311 1 0.7975 0.006627 0.7846 0.811 ## 69.8 2310 1 0.7971 0.006633 0.7843 0.810 ## 69.8 2309 1 0.7968 0.006639 0.7839 0.810 ## 69.8 2308 1 0.7965 0.006645 0.7835 0.810 ## 69.8 2307 1 0.7961 0.006651 0.7832 0.809 ## 69.8 2306 1 0.7958 0.006657 0.7828 0.809 ## 69.8 2305 1 0.7954 0.006663 0.7825 0.809 ## 69.8 2304 1 0.7951 0.006669 0.7821 0.808 ## 69.8 2303 1 0.7947 0.006675 0.7818 0.808 ## 69.8 2302 1 0.7944 0.006681 0.7814 0.808 ## 69.8 2301 1 0.7940 0.006687 0.7810 0.807 ## 69.8 2300 1 0.7937 0.006693 0.7807 0.807 ## 69.9 2299 1 0.7933 0.006699 0.7803 0.807 ## 69.9 2298 1 0.7930 0.006705 0.7800 0.806 ## 69.9 2297 1 0.7927 0.006711 0.7796 0.806 ## 70.0 2296 2 0.7920 0.006723 0.7789 0.805 ## 70.0 2145 1 0.7916 0.006730 0.7785 0.805 ## 70.0 2144 1 0.7912 0.006737 0.7781 0.805 ## 70.1 2143 1 0.7909 0.006744 0.7778 0.804 ## 70.1 2142 1 0.7905 0.006751 0.7774 0.804 ## 70.1 2141 1 0.7901 0.006758 0.7770 0.803 ## 70.1 2140 1 0.7898 0.006765 0.7766 0.803 ## 70.2 2139 1 0.7894 0.006772 0.7762 0.803 ## 70.2 2138 1 0.7890 0.006779 0.7758 0.802 ## 70.2 2137 1 0.7886 0.006786 0.7755 0.802 ## 70.2 2136 1 0.7883 0.006793 0.7751 0.802 ## 70.3 2135 1 0.7879 0.006799 0.7747 0.801 ## 70.3 2134 1 0.7875 0.006806 0.7743 0.801 ## 70.4 2133 1 0.7872 0.006813 0.7739 0.801 ## 70.4 2132 1 0.7868 0.006820 0.7735 0.800 ## 70.4 2131 1 0.7864 0.006827 0.7732 0.800 ## 70.4 2130 1 0.7861 0.006833 0.7728 0.800 ## 70.5 2129 1 0.7857 0.006840 0.7724 0.799 ## 70.5 2128 1 0.7853 0.006847 0.7720 0.799 ## 70.5 2127 1 0.7850 0.006854 0.7716 0.799 ## 70.5 2126 1 0.7846 0.006860 0.7713 0.798 ## 70.5 2125 1 0.7842 0.006867 0.7709 0.798 ## 70.5 2124 1 0.7838 0.006874 0.7705 0.797 ## 70.5 2123 1 0.7835 0.006880 0.7701 0.797 ## 70.5 2122 1 0.7831 0.006887 0.7697 0.797 ## 70.6 2121 1 0.7827 0.006894 0.7693 0.796 ## 70.7 2120 1 0.7824 0.006900 0.7690 0.796 ## 70.7 2119 1 0.7820 0.006907 0.7686 0.796 ## 70.7 2118 1 0.7816 0.006914 0.7682 0.795 ## 70.7 2117 1 0.7813 0.006920 0.7678 0.795 ## 70.7 2116 1 0.7809 0.006927 0.7674 0.795 ## 70.7 2115 1 0.7805 0.006933 0.7671 0.794 ## 70.7 2114 1 0.7802 0.006940 0.7667 0.794 ## 70.8 2113 1 0.7798 0.006946 0.7663 0.794 ## 70.8 2112 1 0.7794 0.006953 0.7659 0.793 ## 70.8 2111 1 0.7790 0.006959 0.7655 0.793 ## 70.8 2110 1 0.7787 0.006966 0.7651 0.792 ## 70.8 2109 1 0.7783 0.006972 0.7648 0.792 ## 70.8 2108 1 0.7779 0.006979 0.7644 0.792 ## 70.9 2107 1 0.7776 0.006985 0.7640 0.791 ## 70.9 2106 1 0.7772 0.006992 0.7636 0.791 ## 70.9 2105 1 0.7768 0.006998 0.7632 0.791 ## 70.9 2104 1 0.7765 0.007004 0.7629 0.790 ## 70.9 2103 1 0.7761 0.007011 0.7625 0.790 ## 71.0 2004 1 0.7757 0.007018 0.7621 0.790 ## 71.0 2003 1 0.7753 0.007025 0.7617 0.789 ## 71.0 2002 1 0.7749 0.007032 0.7613 0.789 ## 71.1 2001 1 0.7745 0.007040 0.7609 0.788 ## 71.1 2000 1 0.7742 0.007047 0.7605 0.788 ## 71.1 1999 1 0.7738 0.007054 0.7601 0.788 ## 71.1 1998 2 0.7730 0.007068 0.7593 0.787 ## 71.1 1996 1 0.7726 0.007075 0.7589 0.787 ## 71.2 1995 1 0.7722 0.007082 0.7585 0.786 ## 71.2 1994 2 0.7714 0.007096 0.7577 0.785 ## 71.3 1992 1 0.7711 0.007103 0.7573 0.785 ## 71.3 1991 1 0.7707 0.007110 0.7569 0.785 ## 71.3 1990 1 0.7703 0.007117 0.7565 0.784 ## 71.3 1989 1 0.7699 0.007124 0.7561 0.784 ## 71.3 1988 1 0.7695 0.007131 0.7557 0.784 ## 71.3 1987 1 0.7691 0.007138 0.7553 0.783 ## 71.4 1986 1 0.7687 0.007145 0.7549 0.783 ## 71.4 1985 1 0.7683 0.007152 0.7545 0.782 ## 71.4 1984 1 0.7680 0.007159 0.7541 0.782 ## 71.4 1983 1 0.7676 0.007165 0.7537 0.782 ## 71.4 1982 1 0.7672 0.007172 0.7533 0.781 ## 71.4 1981 1 0.7668 0.007179 0.7529 0.781 ## 71.5 1980 1 0.7664 0.007186 0.7525 0.781 ## 71.5 1979 1 0.7660 0.007193 0.7521 0.780 ## 71.5 1978 1 0.7656 0.007199 0.7517 0.780 ## 71.5 1977 1 0.7652 0.007206 0.7513 0.780 ## 71.5 1976 1 0.7649 0.007213 0.7509 0.779 ## 71.5 1975 1 0.7645 0.007220 0.7505 0.779 ## 71.6 1974 1 0.7641 0.007226 0.7501 0.778 ## 71.6 1973 1 0.7637 0.007233 0.7497 0.778 ## 71.7 1972 1 0.7633 0.007240 0.7493 0.778 ## 71.7 1971 1 0.7629 0.007246 0.7489 0.777 ## 71.7 1970 1 0.7625 0.007253 0.7485 0.777 ## 71.7 1969 1 0.7622 0.007260 0.7481 0.777 ## 71.7 1968 1 0.7618 0.007266 0.7477 0.776 ## 71.7 1967 1 0.7614 0.007273 0.7473 0.776 ## 71.7 1966 1 0.7610 0.007280 0.7469 0.775 ## 71.7 1965 1 0.7606 0.007286 0.7465 0.775 ## 71.7 1964 1 0.7602 0.007293 0.7461 0.775 ## 71.8 1963 1 0.7598 0.007299 0.7457 0.774 ## 71.8 1962 1 0.7594 0.007306 0.7453 0.774 ## 71.8 1961 1 0.7591 0.007312 0.7449 0.774 ## 71.8 1960 1 0.7587 0.007319 0.7445 0.773 ## 71.8 1959 1 0.7583 0.007325 0.7441 0.773 ## 71.8 1958 1 0.7579 0.007332 0.7437 0.772 ## 71.9 1957 1 0.7575 0.007338 0.7433 0.772 ## 71.9 1956 1 0.7571 0.007345 0.7429 0.772 ## 71.9 1955 1 0.7567 0.007351 0.7425 0.771 ## 71.9 1954 1 0.7563 0.007358 0.7421 0.771 ## 71.9 1953 2 0.7556 0.007371 0.7413 0.770 ## 71.9 1951 1 0.7552 0.007377 0.7409 0.770 ## 71.9 1950 1 0.7548 0.007383 0.7405 0.769 ## 71.9 1949 1 0.7544 0.007390 0.7401 0.769 ## 71.9 1948 1 0.7540 0.007396 0.7397 0.769 ## 72.0 1947 1 0.7536 0.007402 0.7393 0.768 ## 72.0 1946 1 0.7532 0.007409 0.7389 0.768 ## 72.0 1945 1 0.7529 0.007415 0.7385 0.768 ## 72.0 1811 1 0.7524 0.007423 0.7380 0.767 ## 72.0 1810 1 0.7520 0.007430 0.7376 0.767 ## 72.0 1809 1 0.7516 0.007438 0.7372 0.766 ## 72.0 1808 1 0.7512 0.007445 0.7367 0.766 ## 72.0 1807 1 0.7508 0.007453 0.7363 0.766 ## 72.1 1806 1 0.7504 0.007460 0.7359 0.765 ## 72.1 1805 2 0.7495 0.007475 0.7350 0.764 ## 72.1 1803 1 0.7491 0.007482 0.7346 0.764 ## 72.1 1802 1 0.7487 0.007490 0.7342 0.764 ## 72.1 1801 1 0.7483 0.007497 0.7337 0.763 ## 72.1 1800 1 0.7479 0.007504 0.7333 0.763 ## 72.2 1799 1 0.7475 0.007512 0.7329 0.762 ## 72.2 1798 1 0.7470 0.007519 0.7324 0.762 ## 72.3 1797 1 0.7466 0.007526 0.7320 0.762 ## 72.3 1796 1 0.7462 0.007534 0.7316 0.761 ## 72.3 1795 1 0.7458 0.007541 0.7312 0.761 ## 72.3 1794 1 0.7454 0.007548 0.7307 0.760 ## 72.3 1793 1 0.7450 0.007555 0.7303 0.760 ## 72.3 1792 1 0.7445 0.007563 0.7299 0.760 ## 72.3 1791 1 0.7441 0.007570 0.7294 0.759 ## 72.4 1790 1 0.7437 0.007577 0.7290 0.759 ## 72.4 1789 1 0.7433 0.007584 0.7286 0.758 ## 72.4 1788 1 0.7429 0.007591 0.7281 0.758 ## 72.4 1787 1 0.7425 0.007598 0.7277 0.758 ## 72.4 1786 1 0.7420 0.007606 0.7273 0.757 ## 72.4 1785 1 0.7416 0.007613 0.7269 0.757 ## 72.4 1784 1 0.7412 0.007620 0.7264 0.756 ## 72.5 1783 1 0.7408 0.007627 0.7260 0.756 ## 72.5 1782 1 0.7404 0.007634 0.7256 0.755 ## 72.6 1781 1 0.7400 0.007641 0.7251 0.755 ## 72.6 1780 1 0.7396 0.007648 0.7247 0.755 ## 72.6 1779 1 0.7391 0.007655 0.7243 0.754 ## 72.6 1778 1 0.7387 0.007662 0.7239 0.754 ## 72.6 1777 1 0.7383 0.007669 0.7234 0.753 ## 72.6 1776 1 0.7379 0.007676 0.7230 0.753 ## 72.6 1775 1 0.7375 0.007683 0.7226 0.753 ## 72.6 1774 1 0.7371 0.007690 0.7221 0.752 ## 72.7 1773 1 0.7366 0.007696 0.7217 0.752 ## 72.7 1772 1 0.7362 0.007703 0.7213 0.751 ## 72.7 1771 1 0.7358 0.007710 0.7209 0.751 ## 72.7 1770 1 0.7354 0.007717 0.7204 0.751 ## 72.7 1769 2 0.7346 0.007731 0.7196 0.750 ## 72.7 1767 1 0.7341 0.007737 0.7191 0.749 ## 72.7 1766 1 0.7337 0.007744 0.7187 0.749 ## 72.7 1765 1 0.7333 0.007751 0.7183 0.749 ## 72.8 1764 1 0.7329 0.007758 0.7179 0.748 ## 72.8 1763 1 0.7325 0.007764 0.7174 0.748 ## 72.8 1762 1 0.7321 0.007771 0.7170 0.747 ## 72.8 1761 1 0.7317 0.007778 0.7166 0.747 ## 72.8 1760 1 0.7312 0.007785 0.7161 0.747 ## 72.9 1759 1 0.7308 0.007791 0.7157 0.746 ## 72.9 1758 2 0.7300 0.007804 0.7149 0.745 ## 72.9 1756 1 0.7296 0.007811 0.7144 0.745 ## 72.9 1755 1 0.7292 0.007818 0.7140 0.745 ## 72.9 1754 1 0.7287 0.007824 0.7136 0.744 ## 72.9 1753 1 0.7283 0.007831 0.7131 0.744 ## 73.0 1752 1 0.7279 0.007837 0.7127 0.743 ## 73.0 1751 2 0.7271 0.007850 0.7119 0.743 ## 73.0 1648 1 0.7266 0.007858 0.7114 0.742 ## 73.0 1647 1 0.7262 0.007866 0.7109 0.742 ## 73.1 1646 1 0.7258 0.007873 0.7105 0.741 ## 73.1 1645 1 0.7253 0.007881 0.7100 0.741 ## 73.1 1644 1 0.7249 0.007888 0.7096 0.741 ## 73.1 1643 1 0.7244 0.007896 0.7091 0.740 ## 73.1 1642 1 0.7240 0.007903 0.7087 0.740 ## 73.1 1641 1 0.7236 0.007911 0.7082 0.739 ## 73.1 1640 1 0.7231 0.007918 0.7078 0.739 ## 73.2 1639 1 0.7227 0.007926 0.7073 0.738 ## 73.2 1638 2 0.7218 0.007941 0.7064 0.738 ## 73.2 1636 1 0.7213 0.007948 0.7059 0.737 ## 73.2 1635 1 0.7209 0.007955 0.7055 0.737 ## 73.2 1634 1 0.7205 0.007963 0.7050 0.736 ## 73.3 1633 1 0.7200 0.007970 0.7046 0.736 ## 73.3 1632 1 0.7196 0.007977 0.7041 0.735 ## 73.3 1631 1 0.7191 0.007985 0.7037 0.735 ## 73.3 1630 2 0.7183 0.007999 0.7027 0.734 ## 73.3 1628 1 0.7178 0.008007 0.7023 0.734 ## 73.3 1627 1 0.7174 0.008014 0.7018 0.733 ## 73.3 1626 1 0.7169 0.008021 0.7014 0.733 ## 73.3 1625 1 0.7165 0.008028 0.7009 0.732 ## 73.3 1624 1 0.7161 0.008035 0.7005 0.732 ## 73.3 1623 1 0.7156 0.008042 0.7000 0.732 ## 73.4 1622 1 0.7152 0.008050 0.6996 0.731 ## 73.4 1621 2 0.7143 0.008064 0.6987 0.730 ## 73.4 1619 1 0.7138 0.008071 0.6982 0.730 ## 73.4 1618 1 0.7134 0.008078 0.6977 0.729 ## 73.4 1617 1 0.7130 0.008085 0.6973 0.729 ## 73.4 1616 1 0.7125 0.008092 0.6968 0.729 ## 73.5 1615 1 0.7121 0.008099 0.6964 0.728 ## 73.5 1614 1 0.7116 0.008106 0.6959 0.728 ## 73.5 1613 1 0.7112 0.008113 0.6955 0.727 ## 73.5 1612 1 0.7108 0.008120 0.6950 0.727 ## 73.6 1611 1 0.7103 0.008127 0.6946 0.726 ## 73.6 1610 2 0.7094 0.008141 0.6937 0.726 ## 73.6 1608 1 0.7090 0.008148 0.6932 0.725 ## 73.6 1607 1 0.7086 0.008154 0.6927 0.725 ## 73.6 1606 1 0.7081 0.008161 0.6923 0.724 ## 73.6 1605 1 0.7077 0.008168 0.6918 0.724 ## 73.7 1604 1 0.7072 0.008175 0.6914 0.723 ## 73.7 1603 1 0.7068 0.008182 0.6909 0.723 ## 73.7 1602 1 0.7063 0.008189 0.6905 0.723 ## 73.7 1601 1 0.7059 0.008195 0.6900 0.722 ## 73.7 1600 1 0.7055 0.008202 0.6896 0.722 ## 73.7 1599 1 0.7050 0.008209 0.6891 0.721 ## 73.7 1598 1 0.7046 0.008216 0.6887 0.721 ## 73.7 1597 1 0.7041 0.008222 0.6882 0.720 ## 73.8 1596 1 0.7037 0.008229 0.6878 0.720 ## 73.8 1595 1 0.7033 0.008236 0.6873 0.720 ## 73.8 1594 1 0.7028 0.008242 0.6868 0.719 ## 73.8 1593 1 0.7024 0.008249 0.6864 0.719 ## 73.8 1592 1 0.7019 0.008255 0.6859 0.718 ## 73.8 1591 1 0.7015 0.008262 0.6855 0.718 ## 73.9 1590 1 0.7011 0.008269 0.6850 0.717 ## 73.9 1589 1 0.7006 0.008275 0.6846 0.717 ## 73.9 1588 1 0.7002 0.008282 0.6841 0.717 ## 74.0 1587 1 0.6997 0.008288 0.6837 0.716 ## 74.0 1586 1 0.6993 0.008295 0.6832 0.716 ## 74.0 1488 1 0.6988 0.008302 0.6827 0.715 ## 74.0 1487 1 0.6983 0.008310 0.6822 0.715 ## 74.1 1486 1 0.6979 0.008318 0.6818 0.714 ## 74.1 1485 1 0.6974 0.008325 0.6813 0.714 ## 74.1 1484 1 0.6969 0.008333 0.6808 0.713 ## 74.1 1483 1 0.6965 0.008341 0.6803 0.713 ## 74.1 1482 1 0.6960 0.008348 0.6798 0.713 ## 74.2 1481 1 0.6955 0.008356 0.6793 0.712 ## 74.2 1480 1 0.6951 0.008363 0.6789 0.712 ## 74.2 1479 1 0.6946 0.008371 0.6784 0.711 ## 74.2 1478 2 0.6936 0.008386 0.6774 0.710 ## 74.2 1476 1 0.6932 0.008394 0.6769 0.710 ## 74.2 1475 1 0.6927 0.008401 0.6764 0.709 ## 74.2 1474 1 0.6922 0.008408 0.6760 0.709 ## 74.3 1473 1 0.6918 0.008416 0.6755 0.708 ## 74.3 1472 1 0.6913 0.008423 0.6750 0.708 ## 74.3 1471 1 0.6908 0.008431 0.6745 0.708 ## 74.4 1470 1 0.6904 0.008438 0.6740 0.707 ## 74.4 1469 1 0.6899 0.008445 0.6735 0.707 ## 74.4 1468 1 0.6894 0.008453 0.6730 0.706 ## 74.4 1467 1 0.6889 0.008460 0.6726 0.706 ## 74.4 1466 1 0.6885 0.008467 0.6721 0.705 ## 74.5 1465 1 0.6880 0.008474 0.6716 0.705 ## 74.5 1464 1 0.6875 0.008482 0.6711 0.704 ## 74.5 1463 1 0.6871 0.008489 0.6706 0.704 ## 74.5 1462 1 0.6866 0.008496 0.6701 0.703 ## 74.5 1461 1 0.6861 0.008503 0.6697 0.703 ## 74.5 1460 1 0.6857 0.008510 0.6692 0.703 ## 74.6 1459 1 0.6852 0.008518 0.6687 0.702 ## 74.6 1458 1 0.6847 0.008525 0.6682 0.702 ## 74.7 1457 1 0.6842 0.008532 0.6677 0.701 ## 74.7 1456 1 0.6838 0.008539 0.6672 0.701 ## 74.7 1455 1 0.6833 0.008546 0.6668 0.700 ## 74.8 1454 1 0.6828 0.008553 0.6663 0.700 ## 74.8 1453 1 0.6824 0.008560 0.6658 0.699 ## 74.8 1452 1 0.6819 0.008567 0.6653 0.699 ## 74.8 1451 1 0.6814 0.008574 0.6648 0.698 ## 74.8 1450 1 0.6810 0.008581 0.6643 0.698 ## 74.8 1449 1 0.6805 0.008588 0.6639 0.698 ## 74.8 1448 1 0.6800 0.008595 0.6634 0.697 ## 74.8 1447 1 0.6795 0.008602 0.6629 0.697 ## 74.8 1446 1 0.6791 0.008608 0.6624 0.696 ## 74.9 1445 1 0.6786 0.008615 0.6619 0.696 ## 74.9 1444 1 0.6781 0.008622 0.6614 0.695 ## 74.9 1443 1 0.6777 0.008629 0.6610 0.695 ## 74.9 1442 1 0.6772 0.008636 0.6605 0.694 ## 74.9 1441 1 0.6767 0.008643 0.6600 0.694 ## 75.0 1440 1 0.6763 0.008649 0.6595 0.693 ## 75.0 1439 1 0.6758 0.008656 0.6590 0.693 ## 75.0 1333 1 0.6753 0.008664 0.6585 0.692 ## 75.0 1332 1 0.6748 0.008673 0.6580 0.692 ## 75.0 1331 1 0.6743 0.008681 0.6575 0.691 ## 75.1 1330 1 0.6738 0.008689 0.6569 0.691 ## 75.1 1329 1 0.6733 0.008698 0.6564 0.691 ## 75.1 1328 1 0.6727 0.008706 0.6559 0.690 ## 75.1 1327 1 0.6722 0.008714 0.6554 0.690 ## 75.1 1326 1 0.6717 0.008722 0.6549 0.689 ## 75.1 1325 1 0.6712 0.008730 0.6543 0.689 ## 75.2 1324 1 0.6707 0.008738 0.6538 0.688 ## 75.2 1323 1 0.6702 0.008746 0.6533 0.688 ## 75.3 1322 1 0.6697 0.008754 0.6528 0.687 ## 75.4 1321 1 0.6692 0.008763 0.6522 0.687 ## 75.4 1320 1 0.6687 0.008771 0.6517 0.686 ## 75.5 1319 1 0.6682 0.008779 0.6512 0.686 ## 75.5 1318 1 0.6677 0.008787 0.6507 0.685 ## 75.5 1317 1 0.6672 0.008794 0.6502 0.685 ## 75.5 1316 1 0.6667 0.008802 0.6496 0.684 ## 75.5 1315 1 0.6662 0.008810 0.6491 0.684 ## 75.5 1314 1 0.6656 0.008818 0.6486 0.683 ## 75.5 1313 1 0.6651 0.008826 0.6481 0.683 ## 75.5 1312 1 0.6646 0.008834 0.6475 0.682 ## 75.6 1311 1 0.6641 0.008842 0.6470 0.682 ## 75.6 1310 1 0.6636 0.008849 0.6465 0.681 ## 75.6 1309 1 0.6631 0.008857 0.6460 0.681 ## 75.7 1308 1 0.6626 0.008865 0.6455 0.680 ## 75.7 1307 1 0.6621 0.008873 0.6449 0.680 ## 75.7 1306 1 0.6616 0.008880 0.6444 0.679 ## 75.7 1305 1 0.6611 0.008888 0.6439 0.679 ## 75.7 1304 1 0.6606 0.008896 0.6434 0.678 ## 75.7 1303 1 0.6601 0.008903 0.6429 0.678 ## 75.7 1302 1 0.6596 0.008911 0.6423 0.677 ## 75.7 1301 1 0.6591 0.008918 0.6418 0.677 ## 75.7 1300 1 0.6586 0.008926 0.6413 0.676 ## 75.8 1299 1 0.6580 0.008933 0.6408 0.676 ## 75.8 1298 1 0.6575 0.008941 0.6402 0.675 ## 75.9 1297 1 0.6570 0.008948 0.6397 0.675 ## 75.9 1296 2 0.6560 0.008963 0.6387 0.674 ## 75.9 1294 2 0.6550 0.008978 0.6376 0.673 ## 75.9 1292 1 0.6545 0.008985 0.6371 0.672 ## 75.9 1291 1 0.6540 0.008993 0.6366 0.672 ## 75.9 1290 1 0.6535 0.009000 0.6361 0.671 ## 75.9 1289 1 0.6530 0.009007 0.6356 0.671 ## 75.9 1288 1 0.6525 0.009014 0.6350 0.670 ## 75.9 1287 1 0.6520 0.009022 0.6345 0.670 ## 76.0 1286 1 0.6515 0.009029 0.6340 0.669 ## 76.0 1285 1 0.6509 0.009036 0.6335 0.669 ## 76.0 1284 1 0.6504 0.009043 0.6330 0.668 ## 76.0 1187 2 0.6493 0.009061 0.6318 0.667 ## 76.0 1185 1 0.6488 0.009070 0.6313 0.667 ## 76.0 1184 1 0.6482 0.009079 0.6307 0.666 ## 76.0 1183 1 0.6477 0.009088 0.6301 0.666 ## 76.1 1182 1 0.6472 0.009097 0.6296 0.665 ## 76.1 1181 1 0.6466 0.009105 0.6290 0.665 ## 76.1 1180 1 0.6461 0.009114 0.6284 0.664 ## 76.1 1179 1 0.6455 0.009123 0.6279 0.664 ## 76.2 1178 1 0.6450 0.009132 0.6273 0.663 ## 76.2 1177 1 0.6444 0.009140 0.6267 0.663 ## 76.2 1176 1 0.6439 0.009149 0.6262 0.662 ## 76.2 1175 1 0.6433 0.009158 0.6256 0.662 ## 76.2 1174 1 0.6428 0.009166 0.6251 0.661 ## 76.2 1173 1 0.6422 0.009175 0.6245 0.660 ## 76.3 1172 1 0.6417 0.009183 0.6239 0.660 ## 76.3 1171 1 0.6411 0.009192 0.6234 0.659 ## 76.3 1170 1 0.6406 0.009200 0.6228 0.659 ## 76.3 1169 1 0.6400 0.009209 0.6222 0.658 ## 76.3 1168 1 0.6395 0.009217 0.6217 0.658 ## 76.3 1167 1 0.6389 0.009225 0.6211 0.657 ## 76.3 1166 1 0.6384 0.009234 0.6205 0.657 ## 76.4 1165 1 0.6378 0.009242 0.6200 0.656 ## 76.4 1164 1 0.6373 0.009250 0.6194 0.656 ## 76.4 1163 1 0.6367 0.009259 0.6189 0.655 ## 76.5 1162 1 0.6362 0.009267 0.6183 0.655 ## 76.5 1161 1 0.6356 0.009275 0.6177 0.654 ## 76.5 1160 1 0.6351 0.009283 0.6172 0.654 ## 76.5 1159 1 0.6345 0.009291 0.6166 0.653 ## 76.6 1158 1 0.6340 0.009299 0.6160 0.652 ## 76.6 1157 1 0.6335 0.009308 0.6155 0.652 ## 76.6 1156 1 0.6329 0.009316 0.6149 0.651 ## 76.6 1155 1 0.6324 0.009324 0.6143 0.651 ## 76.6 1154 1 0.6318 0.009332 0.6138 0.650 ## 76.6 1153 1 0.6313 0.009340 0.6132 0.650 ## 76.6 1152 1 0.6307 0.009348 0.6127 0.649 ## 76.7 1151 1 0.6302 0.009356 0.6121 0.649 ## 76.7 1150 1 0.6296 0.009363 0.6115 0.648 ## 76.7 1149 1 0.6291 0.009371 0.6110 0.648 ## 76.7 1148 1 0.6285 0.009379 0.6104 0.647 ## 76.7 1147 1 0.6280 0.009387 0.6098 0.647 ## 76.8 1146 1 0.6274 0.009395 0.6093 0.646 ## 76.8 1145 1 0.6269 0.009403 0.6087 0.646 ## 76.8 1144 1 0.6263 0.009410 0.6082 0.645 ## 76.8 1143 1 0.6258 0.009418 0.6076 0.645 ## 76.8 1142 1 0.6252 0.009426 0.6070 0.644 ## 76.9 1141 1 0.6247 0.009433 0.6065 0.643 ## 76.9 1140 1 0.6241 0.009441 0.6059 0.643 ## 76.9 1139 1 0.6236 0.009449 0.6053 0.642 ## 76.9 1138 1 0.6230 0.009456 0.6048 0.642 ## 76.9 1137 1 0.6225 0.009464 0.6042 0.641 ## 76.9 1136 1 0.6219 0.009471 0.6037 0.641 ## 76.9 1135 1 0.6214 0.009479 0.6031 0.640 ## 77.0 1134 1 0.6209 0.009486 0.6025 0.640 ## 77.0 1133 1 0.6203 0.009494 0.6020 0.639 ## 77.0 1036 1 0.6197 0.009503 0.6014 0.639 ## 77.0 1035 1 0.6191 0.009513 0.6007 0.638 ## 77.0 1034 1 0.6185 0.009523 0.6001 0.637 ## 77.0 1033 1 0.6179 0.009532 0.5995 0.637 ## 77.0 1032 1 0.6173 0.009542 0.5989 0.636 ## 77.0 1031 2 0.6161 0.009561 0.5977 0.635 ## 77.1 1029 1 0.6155 0.009570 0.5970 0.635 ## 77.1 1028 2 0.6143 0.009589 0.5958 0.633 ## 77.1 1026 1 0.6137 0.009598 0.5952 0.633 ## 77.1 1025 1 0.6131 0.009607 0.5946 0.632 ## 77.1 1024 1 0.6125 0.009617 0.5940 0.632 ## 77.2 1023 1 0.6119 0.009626 0.5933 0.631 ## 77.2 1022 1 0.6113 0.009635 0.5927 0.631 ## 77.2 1021 2 0.6101 0.009653 0.5915 0.629 ## 77.2 1019 1 0.6095 0.009662 0.5909 0.629 ## 77.2 1018 1 0.6089 0.009672 0.5903 0.628 ## 77.3 1017 1 0.6083 0.009681 0.5896 0.628 ## 77.3 1016 1 0.6077 0.009690 0.5890 0.627 ## 77.3 1015 1 0.6071 0.009698 0.5884 0.626 ## 77.4 1014 1 0.6065 0.009707 0.5878 0.626 ## 77.4 1013 1 0.6059 0.009716 0.5872 0.625 ## 77.4 1012 1 0.6053 0.009725 0.5866 0.625 ## 77.4 1011 1 0.6047 0.009734 0.5860 0.624 ## 77.4 1010 1 0.6041 0.009743 0.5853 0.624 ## 77.4 1009 1 0.6035 0.009751 0.5847 0.623 ## 77.5 1008 1 0.6029 0.009760 0.5841 0.622 ## 77.6 1007 1 0.6023 0.009769 0.5835 0.622 ## 77.6 1006 1 0.6017 0.009777 0.5829 0.621 ## 77.6 1005 1 0.6011 0.009786 0.5823 0.621 ## 77.6 1004 1 0.6005 0.009794 0.5817 0.620 ## 77.6 1003 1 0.5999 0.009803 0.5810 0.619 ## 77.6 1002 1 0.5993 0.009811 0.5804 0.619 ## 77.6 1001 1 0.5987 0.009820 0.5798 0.618 ## 77.7 1000 1 0.5981 0.009828 0.5792 0.618 ## 77.7 999 1 0.5975 0.009837 0.5786 0.617 ## 77.7 998 1 0.5970 0.009845 0.5780 0.617 ## 77.7 997 1 0.5964 0.009853 0.5773 0.616 ## 77.7 996 1 0.5958 0.009862 0.5767 0.615 ## 77.8 995 1 0.5952 0.009870 0.5761 0.615 ## 77.8 994 1 0.5946 0.009878 0.5755 0.614 ## 77.8 993 2 0.5934 0.009894 0.5743 0.613 ## 77.8 991 1 0.5928 0.009903 0.5737 0.612 ## 77.8 990 1 0.5922 0.009911 0.5731 0.612 ## 77.8 989 1 0.5916 0.009919 0.5724 0.611 ## 77.8 988 1 0.5910 0.009927 0.5718 0.611 ## 77.9 987 1 0.5904 0.009935 0.5712 0.610 ## 77.9 986 1 0.5898 0.009943 0.5706 0.610 ## 77.9 985 1 0.5892 0.009951 0.5700 0.609 ## 77.9 984 1 0.5886 0.009958 0.5694 0.608 ## 77.9 983 1 0.5880 0.009966 0.5688 0.608 ## 77.9 982 1 0.5874 0.009974 0.5681 0.607 ## 78.0 981 1 0.5868 0.009982 0.5675 0.607 ## 78.0 893 1 0.5861 0.009992 0.5669 0.606 ## 78.0 892 1 0.5855 0.010003 0.5662 0.605 ## 78.0 891 1 0.5848 0.010013 0.5655 0.605 ## 78.0 890 1 0.5841 0.010023 0.5648 0.604 ## 78.1 889 1 0.5835 0.010034 0.5641 0.603 ## 78.1 888 1 0.5828 0.010044 0.5635 0.603 ## 78.1 887 1 0.5822 0.010054 0.5628 0.602 ## 78.1 886 1 0.5815 0.010064 0.5621 0.602 ## 78.1 885 1 0.5809 0.010074 0.5614 0.601 ## 78.1 884 1 0.5802 0.010084 0.5608 0.600 ## 78.2 883 1 0.5795 0.010094 0.5601 0.600 ## 78.2 882 1 0.5789 0.010104 0.5594 0.599 ## 78.2 881 1 0.5782 0.010114 0.5587 0.598 ## 78.2 880 1 0.5776 0.010124 0.5581 0.598 ## 78.2 879 1 0.5769 0.010133 0.5574 0.597 ## 78.2 878 1 0.5763 0.010143 0.5567 0.596 ## 78.3 877 1 0.5756 0.010153 0.5560 0.596 ## 78.3 876 1 0.5749 0.010163 0.5554 0.595 ## 78.3 875 1 0.5743 0.010172 0.5547 0.595 ## 78.3 874 1 0.5736 0.010182 0.5540 0.594 ## 78.3 873 1 0.5730 0.010191 0.5533 0.593 ## 78.3 872 1 0.5723 0.010201 0.5527 0.593 ## 78.3 871 1 0.5717 0.010210 0.5520 0.592 ## 78.4 870 1 0.5710 0.010219 0.5513 0.591 ## 78.4 869 1 0.5703 0.010229 0.5506 0.591 ## 78.4 868 1 0.5697 0.010238 0.5500 0.590 ## 78.4 867 1 0.5690 0.010247 0.5493 0.589 ## 78.4 866 1 0.5684 0.010257 0.5486 0.589 ## 78.4 865 1 0.5677 0.010266 0.5479 0.588 ## 78.4 864 1 0.5671 0.010275 0.5473 0.588 ## 78.4 863 1 0.5664 0.010284 0.5466 0.587 ## 78.4 862 1 0.5657 0.010293 0.5459 0.586 ## 78.4 861 1 0.5651 0.010302 0.5453 0.586 ## 78.4 860 1 0.5644 0.010311 0.5446 0.585 ## 78.4 859 1 0.5638 0.010320 0.5439 0.584 ## 78.4 858 1 0.5631 0.010329 0.5432 0.584 ## 78.5 857 1 0.5625 0.010338 0.5426 0.583 ## 78.5 856 1 0.5618 0.010346 0.5419 0.582 ## 78.5 855 1 0.5611 0.010355 0.5412 0.582 ## 78.5 854 1 0.5605 0.010364 0.5405 0.581 ## 78.5 853 1 0.5598 0.010372 0.5399 0.581 ## 78.5 852 1 0.5592 0.010381 0.5392 0.580 ## 78.6 851 1 0.5585 0.010390 0.5385 0.579 ## 78.6 850 1 0.5579 0.010398 0.5378 0.579 ## 78.6 849 1 0.5572 0.010407 0.5372 0.578 ## 78.6 848 1 0.5565 0.010415 0.5365 0.577 ## 78.7 847 1 0.5559 0.010424 0.5358 0.577 ## 78.7 846 1 0.5552 0.010432 0.5352 0.576 ## 78.7 845 1 0.5546 0.010440 0.5345 0.575 ## 78.7 844 1 0.5539 0.010449 0.5338 0.575 ## 78.7 843 1 0.5533 0.010457 0.5331 0.574 ## 78.8 842 1 0.5526 0.010465 0.5325 0.574 ## 78.8 841 1 0.5519 0.010473 0.5318 0.573 ## 78.8 840 1 0.5513 0.010481 0.5311 0.572 ## 78.9 839 1 0.5506 0.010489 0.5305 0.572 ## 78.9 838 1 0.5500 0.010497 0.5298 0.571 ## 78.9 837 1 0.5493 0.010505 0.5291 0.570 ## 78.9 836 1 0.5487 0.010513 0.5284 0.570 ## 78.9 835 1 0.5480 0.010521 0.5278 0.569 ## 78.9 834 1 0.5473 0.010529 0.5271 0.568 ## 78.9 833 1 0.5467 0.010537 0.5264 0.568 ## 79.0 832 1 0.5460 0.010545 0.5258 0.567 ## 79.0 741 1 0.5453 0.010556 0.5250 0.566 ## 79.0 740 1 0.5446 0.010568 0.5242 0.566 ## 79.0 739 1 0.5438 0.010579 0.5235 0.565 ## 79.1 738 1 0.5431 0.010590 0.5227 0.564 ## 79.1 737 1 0.5423 0.010602 0.5220 0.564 ## 79.1 736 1 0.5416 0.010613 0.5212 0.563 ## 79.1 735 1 0.5409 0.010624 0.5204 0.562 ## 79.1 734 1 0.5401 0.010635 0.5197 0.561 ## 79.1 733 1 0.5394 0.010646 0.5189 0.561 ## 79.2 732 1 0.5387 0.010657 0.5182 0.560 ## 79.3 731 1 0.5379 0.010668 0.5174 0.559 ## 79.3 730 1 0.5372 0.010679 0.5167 0.559 ## 79.3 729 1 0.5365 0.010689 0.5159 0.558 ## 79.3 728 1 0.5357 0.010700 0.5152 0.557 ## 79.3 727 1 0.5350 0.010711 0.5144 0.556 ## 79.4 726 1 0.5342 0.010721 0.5136 0.556 ## 79.4 725 1 0.5335 0.010732 0.5129 0.555 ## 79.5 724 1 0.5328 0.010742 0.5121 0.554 ## 79.5 723 1 0.5320 0.010753 0.5114 0.554 ## 79.5 722 1 0.5313 0.010763 0.5106 0.553 ## 79.6 721 1 0.5306 0.010773 0.5099 0.552 ## 79.6 720 1 0.5298 0.010783 0.5091 0.551 ## 79.6 719 1 0.5291 0.010793 0.5083 0.551 ## 79.6 718 1 0.5283 0.010804 0.5076 0.550 ## 79.7 717 1 0.5276 0.010814 0.5068 0.549 ## 79.7 716 1 0.5269 0.010824 0.5061 0.549 ## 79.8 715 1 0.5261 0.010833 0.5053 0.548 ## 79.8 714 1 0.5254 0.010843 0.5046 0.547 ## 79.8 713 1 0.5247 0.010853 0.5038 0.546 ## 79.9 712 1 0.5239 0.010863 0.5031 0.546 ## 79.9 711 1 0.5232 0.010873 0.5023 0.545 ## 79.9 710 1 0.5225 0.010882 0.5016 0.544 ## 80.0 709 1 0.5217 0.010892 0.5008 0.544 ## 80.0 639 1 0.5209 0.010905 0.5000 0.543 ## 80.0 638 1 0.5201 0.010919 0.4991 0.542 ## 80.0 637 1 0.5193 0.010932 0.4983 0.541 ## 80.1 636 1 0.5185 0.010945 0.4974 0.540 ## 80.1 635 1 0.5176 0.010958 0.4966 0.540 ## 80.1 634 1 0.5168 0.010972 0.4958 0.539 ## 80.2 633 1 0.5160 0.010985 0.4949 0.538 ## 80.2 632 1 0.5152 0.010998 0.4941 0.537 ## 80.3 631 1 0.5144 0.011010 0.4932 0.536 ## 80.3 630 1 0.5136 0.011023 0.4924 0.536 ## 80.3 629 1 0.5127 0.011036 0.4916 0.535 ## 80.4 628 1 0.5119 0.011048 0.4907 0.534 ## 80.4 627 1 0.5111 0.011061 0.4899 0.533 ## 80.4 626 1 0.5103 0.011073 0.4890 0.532 ## 80.4 625 1 0.5095 0.011086 0.4882 0.532 ## 80.4 624 1 0.5087 0.011098 0.4874 0.531 ## 80.5 623 1 0.5078 0.011110 0.4865 0.530 ## 80.5 622 1 0.5070 0.011122 0.4857 0.529 ## 80.5 621 1 0.5062 0.011134 0.4848 0.529 ## 80.5 620 1 0.5054 0.011146 0.4840 0.528 ## 80.6 619 1 0.5046 0.011158 0.4832 0.527 ## 80.7 618 1 0.5038 0.011170 0.4823 0.526 ## 80.7 617 1 0.5029 0.011181 0.4815 0.525 ## 80.7 616 2 0.5013 0.011205 0.4798 0.524 ## 80.7 614 1 0.5005 0.011216 0.4790 0.523 ## 80.7 613 1 0.4997 0.011227 0.4781 0.522 ## 80.8 612 1 0.4989 0.011239 0.4773 0.521 ## 80.8 611 1 0.4980 0.011250 0.4765 0.521 ## 80.8 610 1 0.4972 0.011261 0.4756 0.520 ## 80.9 609 1 0.4964 0.011272 0.4748 0.519 ## 80.9 608 1 0.4956 0.011283 0.4740 0.518 ## 80.9 607 1 0.4948 0.011294 0.4731 0.517 ## 81.0 606 1 0.4940 0.011305 0.4723 0.517 ## 81.0 605 1 0.4931 0.011316 0.4715 0.516 ## 81.1 540 1 0.4922 0.011332 0.4705 0.515 ## 81.1 539 1 0.4913 0.011347 0.4696 0.514 ## 81.2 538 1 0.4904 0.011363 0.4686 0.513 ## 81.2 537 1 0.4895 0.011378 0.4677 0.512 ## 81.2 536 1 0.4886 0.011394 0.4667 0.511 ## 81.2 535 1 0.4877 0.011409 0.4658 0.511 ## 81.2 534 1 0.4867 0.011424 0.4649 0.510 ## 81.2 533 1 0.4858 0.011439 0.4639 0.509 ## 81.3 532 1 0.4849 0.011454 0.4630 0.508 ## 81.3 531 1 0.4840 0.011469 0.4620 0.507 ## 81.3 530 1 0.4831 0.011483 0.4611 0.506 ## 81.4 529 1 0.4822 0.011498 0.4602 0.505 ## 81.4 528 1 0.4813 0.011512 0.4592 0.504 ## 81.4 527 1 0.4804 0.011527 0.4583 0.503 ## 81.4 526 1 0.4794 0.011541 0.4573 0.503 ## 81.5 525 1 0.4785 0.011555 0.4564 0.502 ## 81.5 524 1 0.4776 0.011569 0.4555 0.501 ## 81.5 523 1 0.4767 0.011583 0.4545 0.500 ## 81.5 522 1 0.4758 0.011597 0.4536 0.499 ## 81.6 521 1 0.4749 0.011610 0.4527 0.498 ## 81.6 520 1 0.4740 0.011624 0.4517 0.497 ## 81.6 519 1 0.4730 0.011637 0.4508 0.496 ## 81.7 518 1 0.4721 0.011650 0.4498 0.496 ## 81.7 517 1 0.4712 0.011664 0.4489 0.495 ## 81.7 516 1 0.4703 0.011677 0.4480 0.494 ## 81.8 515 1 0.4694 0.011690 0.4470 0.493 ## 81.8 514 1 0.4685 0.011703 0.4461 0.492 ## 81.8 513 1 0.4676 0.011715 0.4452 0.491 ## 81.8 512 1 0.4667 0.011728 0.4442 0.490 ## 81.8 511 1 0.4657 0.011741 0.4433 0.489 ## 81.8 510 1 0.4648 0.011753 0.4424 0.488 ## 81.9 509 1 0.4639 0.011765 0.4414 0.488 ## 81.9 508 1 0.4630 0.011778 0.4405 0.487 ## 81.9 507 1 0.4621 0.011790 0.4396 0.486 ## 81.9 506 1 0.4612 0.011802 0.4386 0.485 ## 82.0 505 1 0.4603 0.011814 0.4377 0.484 ## 82.0 504 1 0.4594 0.011825 0.4367 0.483 ## 82.0 447 1 0.4583 0.011844 0.4357 0.482 ## 82.1 446 1 0.4573 0.011862 0.4346 0.481 ## 82.1 445 1 0.4563 0.011879 0.4336 0.480 ## 82.1 444 1 0.4552 0.011897 0.4325 0.479 ## 82.1 443 1 0.4542 0.011914 0.4315 0.478 ## 82.1 442 1 0.4532 0.011932 0.4304 0.477 ## 82.2 441 1 0.4522 0.011949 0.4293 0.476 ## 82.2 440 1 0.4511 0.011966 0.4283 0.475 ## 82.2 439 1 0.4501 0.011982 0.4272 0.474 ## 82.2 438 1 0.4491 0.011999 0.4262 0.473 ## 82.2 437 1 0.4480 0.012016 0.4251 0.472 ## 82.3 436 1 0.4470 0.012032 0.4240 0.471 ## 82.3 435 1 0.4460 0.012048 0.4230 0.470 ## 82.3 434 1 0.4450 0.012064 0.4219 0.469 ## 82.3 433 1 0.4439 0.012080 0.4209 0.468 ## 82.3 432 1 0.4429 0.012096 0.4198 0.467 ## 82.3 431 1 0.4419 0.012111 0.4188 0.466 ## 82.4 430 1 0.4409 0.012126 0.4177 0.465 ## 82.4 429 1 0.4398 0.012142 0.4167 0.464 ## 82.4 428 1 0.4388 0.012157 0.4156 0.463 ## 82.4 427 1 0.4378 0.012171 0.4146 0.462 ## 82.4 426 1 0.4367 0.012186 0.4135 0.461 ## 82.4 425 1 0.4357 0.012201 0.4124 0.460 ## 82.5 424 1 0.4347 0.012215 0.4114 0.459 ## 82.5 423 1 0.4337 0.012230 0.4103 0.458 ## 82.5 422 1 0.4326 0.012244 0.4093 0.457 ## 82.5 421 1 0.4316 0.012258 0.4082 0.456 ## 82.6 420 1 0.4306 0.012271 0.4072 0.455 ## 82.6 419 1 0.4295 0.012285 0.4061 0.454 ## 82.7 418 1 0.4285 0.012299 0.4051 0.453 ## 82.7 417 1 0.4275 0.012312 0.4040 0.452 ## 82.7 416 1 0.4265 0.012325 0.4030 0.451 ## 82.7 415 2 0.4244 0.012351 0.4009 0.449 ## 82.7 413 1 0.4234 0.012364 0.3998 0.448 ## 82.8 412 1 0.4224 0.012377 0.3988 0.447 ## 82.8 411 1 0.4213 0.012389 0.3977 0.446 ## 82.8 410 1 0.4203 0.012401 0.3967 0.445 ## 82.9 409 1 0.4193 0.012414 0.3956 0.444 ## 82.9 408 1 0.4182 0.012426 0.3946 0.443 ## 82.9 407 1 0.4172 0.012437 0.3935 0.442 ## 83.0 342 1 0.4160 0.012461 0.3923 0.441 ## 83.0 341 1 0.4148 0.012484 0.3910 0.440 ## 83.1 340 1 0.4136 0.012507 0.3898 0.439 ## 83.1 339 1 0.4123 0.012529 0.3885 0.438 ## 83.1 338 1 0.4111 0.012551 0.3872 0.436 ## 83.2 337 1 0.4099 0.012573 0.3860 0.435 ## 83.2 336 1 0.4087 0.012595 0.3847 0.434 ## 83.2 335 1 0.4075 0.012616 0.3835 0.433 ## 83.2 334 1 0.4062 0.012637 0.3822 0.432 ## 83.3 333 1 0.4050 0.012658 0.3810 0.431 ## 83.3 332 1 0.4038 0.012678 0.3797 0.429 ## 83.4 331 1 0.4026 0.012699 0.3784 0.428 ## 83.4 330 1 0.4014 0.012719 0.3772 0.427 ## 83.4 329 1 0.4001 0.012738 0.3759 0.426 ## 83.4 328 1 0.3989 0.012758 0.3747 0.425 ## 83.4 327 1 0.3977 0.012777 0.3734 0.424 ## 83.5 326 1 0.3965 0.012796 0.3722 0.422 ## 83.6 325 1 0.3953 0.012815 0.3709 0.421 ## 83.6 324 1 0.3940 0.012833 0.3697 0.420 ## 83.6 323 1 0.3928 0.012851 0.3684 0.419 ## 83.6 322 1 0.3916 0.012869 0.3672 0.418 ## 83.7 321 1 0.3904 0.012887 0.3659 0.416 ## 83.7 320 1 0.3892 0.012904 0.3647 0.415 ## 83.8 319 1 0.3879 0.012921 0.3634 0.414 ## 83.9 318 1 0.3867 0.012938 0.3622 0.413 ## 83.9 317 1 0.3855 0.012954 0.3609 0.412 ## 83.9 316 1 0.3843 0.012971 0.3597 0.411 ## 84.0 315 1 0.3831 0.012987 0.3584 0.409 ## 84.0 260 1 0.3816 0.013020 0.3569 0.408 ## 84.0 259 1 0.3801 0.013053 0.3554 0.407 ## 84.1 258 1 0.3786 0.013085 0.3538 0.405 ## 84.1 257 1 0.3772 0.013117 0.3523 0.404 ## 84.2 256 1 0.3757 0.013148 0.3508 0.402 ## 84.3 255 1 0.3742 0.013179 0.3493 0.401 ## 84.3 254 1 0.3727 0.013209 0.3477 0.400 ## 84.3 253 1 0.3713 0.013239 0.3462 0.398 ## 84.4 252 1 0.3698 0.013268 0.3447 0.397 ## 84.4 251 1 0.3683 0.013297 0.3432 0.395 ## 84.5 250 1 0.3669 0.013325 0.3416 0.394 ## 84.5 249 1 0.3654 0.013353 0.3401 0.393 ## 84.5 248 1 0.3639 0.013380 0.3386 0.391 ## 84.6 247 1 0.3624 0.013407 0.3371 0.390 ## 84.7 246 1 0.3610 0.013433 0.3356 0.388 ## 84.7 245 1 0.3595 0.013458 0.3341 0.387 ## 84.8 244 1 0.3580 0.013484 0.3325 0.385 ## 84.8 243 1 0.3565 0.013508 0.3310 0.384 ## 84.9 242 1 0.3551 0.013533 0.3295 0.383 ## 84.9 241 1 0.3536 0.013557 0.3280 0.381 ## 84.9 240 1 0.3521 0.013580 0.3265 0.380 ## 85.0 239 1 0.3506 0.013603 0.3250 0.378 ## 85.0 194 1 0.3488 0.013652 0.3231 0.377 ## 85.0 193 1 0.3470 0.013701 0.3212 0.375 ## 85.0 192 1 0.3452 0.013748 0.3193 0.373 ## 85.1 191 1 0.3434 0.013794 0.3174 0.372 ## 85.2 190 1 0.3416 0.013840 0.3155 0.370 ## 85.3 189 1 0.3398 0.013884 0.3137 0.368 ## 85.3 188 1 0.3380 0.013927 0.3118 0.366 ## 85.3 187 1 0.3362 0.013970 0.3099 0.365 ## 85.4 186 1 0.3344 0.014011 0.3080 0.363 ## 85.7 185 1 0.3326 0.014051 0.3061 0.361 ## 85.7 184 1 0.3308 0.014091 0.3043 0.360 ## 85.8 183 1 0.3290 0.014129 0.3024 0.358 ## 85.8 182 1 0.3272 0.014167 0.3005 0.356 ## 85.9 181 1 0.3253 0.014203 0.2987 0.354 ## 85.9 180 1 0.3235 0.014239 0.2968 0.353 ## 85.9 179 1 0.3217 0.014273 0.2949 0.351 ## 85.9 178 1 0.3199 0.014307 0.2931 0.349 ## 86.0 177 1 0.3181 0.014340 0.2912 0.347 ## 86.0 176 1 0.3163 0.014372 0.2894 0.346 ## 86.0 175 1 0.3145 0.014403 0.2875 0.344 ## 86.2 134 1 0.3122 0.014486 0.2850 0.342 ## 86.2 133 1 0.3098 0.014566 0.2825 0.340 ## 86.3 132 1 0.3075 0.014643 0.2801 0.338 ## 86.3 131 2 0.3028 0.014791 0.2751 0.333 ## 86.4 129 1 0.3004 0.014861 0.2727 0.331 ## 86.4 128 1 0.2981 0.014929 0.2702 0.329 ## 86.6 127 1 0.2957 0.014995 0.2677 0.327 ## 86.6 126 1 0.2934 0.015059 0.2653 0.324 ## 86.7 125 1 0.2910 0.015120 0.2629 0.322 ## 86.8 124 1 0.2887 0.015179 0.2604 0.320 ## 86.8 123 1 0.2863 0.015236 0.2580 0.318 ## 86.9 122 1 0.2840 0.015291 0.2555 0.316 ## 86.9 121 1 0.2816 0.015344 0.2531 0.313 ## 87.0 120 1 0.2793 0.015394 0.2507 0.311 ## 87.1 79 1 0.2758 0.015600 0.2468 0.308 ## 87.1 78 1 0.2722 0.015796 0.2430 0.305 ## 87.2 77 1 0.2687 0.015981 0.2391 0.302 ## 87.4 76 1 0.2652 0.016157 0.2353 0.299 ## 87.5 75 1 0.2616 0.016324 0.2315 0.296 ## 87.6 74 1 0.2581 0.016482 0.2277 0.292 ## 87.8 73 1 0.2545 0.016631 0.2240 0.289 ## 87.9 72 1 0.2510 0.016771 0.2202 0.286 ## 87.9 71 1 0.2475 0.016904 0.2165 0.283 ## 88.1 45 1 0.2420 0.017400 0.2102 0.279 ## 88.3 44 1 0.2365 0.017852 0.2040 0.274 ## 88.3 43 1 0.2310 0.018264 0.1978 0.270 ## 88.4 42 1 0.2255 0.018639 0.1918 0.265 ## 88.9 41 1 0.2200 0.018979 0.1858 0.261 ## 88.9 40 1 0.2145 0.019284 0.1798 0.256 ## 89.0 39 1 0.2090 0.019558 0.1740 0.251 ## 89.0 22 1 0.1995 0.020849 0.1625 0.245 ## 89.1 21 1 0.1900 0.021913 0.1515 0.238 ## 89.6 20 1 0.1805 0.022784 0.1409 0.231 ## 89.8 19 1 0.1710 0.023481 0.1306 0.224 ## 89.8 18 1 0.1615 0.024022 0.1206 0.216 ## 89.8 17 1 0.1520 0.024414 0.1109 0.208 ## 89.9 16 1 0.1425 0.024667 0.1015 0.200 ## 90.0 15 1 0.1330 0.024784 0.0923 0.192 ## 91.1 3 1 0.0887 0.039787 0.0368 0.214 fit2 %&gt;% ggsurvplot(xlab=&quot;Age (years)&quot;,ylab=expression(paste(&#39;Overall Survival Probablity &#39;, hat(S)*&quot;(t)&quot;))) We see that the shape of this survival curve is different, with virtually no one dying until they reach their 40s, and then a sharper drop in survival as age increases. The Cox proportional hazards model in a simple form has this form \\(log(\\lambda(t|X))=log(\\lambda_{0}(t))+\\beta_{1}\\times X\\) where \\(\\lambda(t)\\) represent the hazard at time \\(t\\), \\(\\lambda_{0}(t)\\) is the baseline hazard at time \\(t\\), and \\(\\beta_{1}\\) is the log hazard for those with \\(X=1\\) compared to \\(X=0\\). The baseline hazard \\(\\lambda_{0}(t)\\) is similar to the intercept term in a linear model or glm and is the value of the hazard when all covariates equal 0. However, unlike the intercept term in a linear model or glm, \\(\\lambda_{0}(t)\\) is not estimated by the model. The above model can also be writen as \\(\\lambda(t|X)=\\lambda_{0}(t)\\times e^{\\beta_{1}\\times X}\\) \\(e^{\\beta_{1}}\\) is the hazard ratio comparing those hose with \\(X=1\\) and \\(X=0\\) Using the fhs data we will fit a simple Cox proportianal hazard for the effect of smoking on the hazard for MI. Note: Variables of interest to continue with: for mixed models, sysbp, diabp, totchol compared to cigpday, bmi smoking or not for long. analysis, timemi and timestrk and hyperten, exposure: cigpday, sysbp, diabp, bmi 7.3 Handling complexity 7.3.1 Multi-level exposure 7.3.2 Recurrent outcome 7.3.3 Time-varying coeffificents 7.3.4 Using survey results [e.g., NHANES] References "],["some-approaches-for-confounding.html", "Chapter 8 Some approaches for confounding 8.1 Inverse probability weighting 8.2 Propensity scores", " Chapter 8 Some approaches for confounding 8.1 Inverse probability weighting 8.2 Propensity scores [Modeling for weights/propensity scores, involves machine learning] "],["mixed-models.html", "Chapter 9 Mixed models", " Chapter 9 Mixed models [Using a mixed modeling framework to help analyze repeated measures] "],["instrumental-variables.html", "Chapter 10 Instrumental variables", " Chapter 10 Instrumental variables "],["causal-inference.html", "Chapter 11 Causal inference", " Chapter 11 Causal inference "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
